
\chapter{Random Matrices}

%=========================================================================================
\epigraph{Unfortunately, no one can be told what The Matrix is. You'll have to see it for yourself.}{\textit{Morpheus \\ The Matrix}}
%=========================================================================================

As discussed in the introduction, this thesis will be an exploration of spectral statistics of random matrices. This means that we must first be able to understand what random matrices are.
At a fundamental level, random matrices are simply matrices whose entries are random variables that are distributed in accordance to either some explicit distribution or algorithm.
So, to define a random matrix, our approach will be to do so by formalizing and defining what it mean for one to be $\D$-distributed.
This way, the notation encapsulates and completely characterizes the random matrix.

Prior to beginning the discussion on $\D$-distributions, the reader should be familiar or at least accquainted with the notion of random variables and what they are.
For a review, see \textbf{Appendix A.2.1}.

%=========================================================================================
%=========================================================================================

\section{$\D$-Distributions}

As a general rule, when it comes to random simulation, there is usually a rule or constrain to which our randomness must conform.
For example, sampling a vector from a distribution is a rudimentary example of this.
For random matrices, there will be a few methods of generating their entries that are not just sampling from theoretical distributions.
As such, we motivate the $\D$-distribution.

\begin{definition}[$\D$-distribution]
Suppose $P$ is a $\D$-distributed random matrix. Then, we notate this $P \sim \D$. In the simplest of terms, $\D$ is essentially the algorithm that generates the entries of $P$.
We define two primary methods of distribution: \textbf{explicit} distribution, and \textbf{implicit} distribution.
If $\D$ is an explicit distribution, then some or all the entries of $P$ are independent random variables with a given distribution.
Otherwise, if $\D$ is implicit, then the matrix has dependent entries imposed by the algorithm that generates it.
\end{definition}

%=========================================================================================

\subsection{Explicit Distributions}

\minititle{Homogenous Explicit $\D$-distributions}

The simplest type of $\D$-distribution is one that is explicitly and homogenously distributed. From thereonafter, this will be shortened as e.h.d.
Suppose $\D$ is a probability distribution for random variables in the classical sense. The simplest way to think of e.h.d distributions is to use the concept of notational overload.
For example, it is unambiguous to say that an r.v. $X \sim \Normal(0,1)$. However, the same cannot be said if we said a matrix $P \sim \Normal(0,1)$.

That being said, we can define e.h $\D$-distributions as an notational extension that means \textbf{every} entry of the matrix is an i.i.d random variable with that same distribution!
In otherwords, we simply perform entry-wise sampling from the corresponding r.v. distribution.

Note that this means by construction, $\D$ can only be e.h.d. if it has a corresponding probability distribution for random variables.

\begin{definition}[Explicit $\D$-distribution]
Suppose $P \sim \D$ where $\D$ is a homogenous and explicit distribution. Additionally, let $\D^*$ denote the corresponding random variable analogue of $\D$.
Then, every single entry of $P$ is an i.i.d random variable with the corresponding distribution. That is,
$$ P \sim \D \iff \forall i,j \mid p_{ij} \sim \D^* $$
\end{definition}

\begin{example}
Suppose $P \sim \Normal(0,1)$ and that $P$ is a $2 \times 2$ matrix.
Then, $p_{11}, p_{12}, p_{21}, p_{22}$ are independent, identically distributed random variables with the standard normal distribution.
\end{example}

\ALGexplicit

\begin{formalization}
Explicit homogenous distributions can be formalized as overloading the standard notation of random variable distribution as seen in probability theory.
This way, our random matrix has a representation as a random vector, which we commonly encounter in probability theory as a (i.i.d) sequence of random variables!
Suppose $P$ is an $N \times N$ random matrix that is explicity and homogenously $\D$-distributed. Then, this would mean that $P$ is an array of $N^2$ i.i.d random variables sampled from $\D$.
\end{formalization}

\minititle{Non-Homogenous Explicit $\D$-distributions}

There are a few instances where we encounter the need to only initialize a subset of a matrix's entries as random variables. In this case, we say that a matrix has a non-homogenous explicit $\D$ distribution.
This type is distribution is similar to e.h. $\D$-distributions, but we no longer have the ability to overload random variable notation and indicate that only some entries have such distribution. This would be
confusing because entry doesn't necessarily have the same distribution anymore. As such, we have to define new notation to do so.

To keep things simple, we characterize a non-homogenous explicit $D$-distribution by listing the distribution of every entry. However, in this thesis, there are only one application of this that imposes a different
distribution, and it does so by its diagonals --- this is the Hermite $\b$-ensemble. Since its definition is characterized by distributions on the diagonals, we will avoid full entry-wise generality to be more concise.
As such, we will only describe Non-Homogenous Explicit $\D$-distributions as schemes where we assign diagonal bands to a specific vector of i.i.d random variables.

\begin{definition}[Diagonal Bands]
Suppose $P = (p_ij)$ is an $N \times N$ matrix. Then, $P$ may be partitioned into $2n - 1$ rows called diagonal bands. Each band is denoted $[\rho]$ where $\rho = \{p_{ij} \mid \rho = i - j\}$. We have
$\rho \in \{ -(N-1), \dots, -1, 0, 1, \dots, N-1 \}$.
\end{definition}

Heres an example using the diagonal bands constructor notation.

\begin{example}
Suppose $P$ is an $N \times N$ matrix. Then, $[0]_P$ is the main diagonal of $P$ since $p_{ii} \Ra i = j \Ra i - j = 0 \Ra p_{ii} \in [0]_P$.
Similarly, the main off-diagonal in the upper triangle is $[-1]_P$ since $p_{12} \in [-1]_P$.
Likewise, the main off-digonal in the lower triangle is $[1]_P$. The entry in the top-right corner of the matrix, $p_{1N}$ comprises $[1 - N]_P$.
\end{example}

\begin{code}
Here's how to use diagonal bands idea in R.
\end{code}

\begin{lstlisting}[language=R]
# Set the dimension of the matrix
N <- 5
# Generate an example matrix of zeros
P <- matrix(rep(0, N^2), nrow = N)
# Assign the upper main off-diagonal band as a vector
rho <- -1
P[row(P) - col(P) == rho] <- rnorm(n = 4, mean = 0, sd = 1)
# Assign the main diagonal as a vector
rho <- 0
P[row(P) - col(P) == rho] <- rep(10, N)
# Returns the following
P
     [,1]       [,2]       [,3]       [,4]      [,5]
[1,]   10  0.1932123  0.0000000  0.0000000  0.000000
[2,]    0 10.0000000 -0.4346821  0.0000000  0.000000
[3,]    0  0.0000000 10.0000000  0.9132671  0.000000
[4,]    0  0.0000000  0.0000000 10.0000000  1.793388
[5,]    0  0.0000000  0.0000000  0.0000000 10.000000
\end{lstlisting}

%The general defintion is below.

% \begin{definition}[Distribution Rule Set]
% Suppose $\D$ is a non-homogenous explicit $D$-distribution. Then, for an $N \times N$ matrix, $A_\D = \{a_{ij} = \D_{ij}\}$ is its rule set.
% \end{definition}
%
% The general defintion using purely diagonal bands is below.
%
% \begin{definition}[Diagonal Rule Set]
% Suppose $\D$ is a non-homogenous explicit $D$-distribution. Then, for an $N \times N$ matrix, $A_\D = \{a_{ij} = \D_{ij}\}$ is its rule set.
% \end{definition}

For the distribution using diagonal bands, consider the definiton of the beta matrix below. %refer to the definition of a beta matrix in \textbf{Section 1.2.2}.

\ALGbeta

\begin{definition}[Beta Matrix]
Suppose $P \sim \H(\b)$. Then, $[0]_P \sim \Normal(0,1)$. Additionally, $[-1]_{P} = \vec{X}$ where $X_k = \chi(\text{df} = \beta k)$ for $k = 1,\dots,N-1$. Then, set $[1]_P = [-1]_P$.
\end{definition}

% \begin{example}[Formalization]
% For example, suppose $P$ is a $2 \times 2$ random matrix with $\D = \Normal(0,1)$. Then, we have four random variables to initialize. In the random matrix representation, we need to initialize $P_{11}, P_{12}, P_{21}, \and P_{22}$ by sampling them from $\D$. In the vector representation, we just say we are sampling four i.i.d random variables from $\D$. The matrix indexing is intrinsic and preservable by using index hacking with some modular math.
% \end{example}

%=========================================================================================

\subsection{Implicit Distributions}

In the latter case, we are concerned less about the distribution of the matrix entries and moreso about its holisitic properties.

Consider for example, the following implicit distributions.

\begin{enumerate}
\item If $\D = \text{Stochastic}$, then the matrix is a row of random stochastic rows. (See Algorithm B.x)
\item If $\D = \text{ER(p)}$, then the matrix is a row of Erdos-Renyi $p$-stochastic rows. (See Algorithm B.x)
\end{enumerate}

\minititle{Stochastic Matrices}

Stochastic matrices will serve as our canonical implicitly distributed $\D$-distributed. One might ask, what are stochastic matrices? How are they distributions?

Stochastic matrices, in short, are matrices that represent Markov Chains (see A.x).
We can also think of them as the matrix representation of a specific setup of a random walk on a fixed graph with \textbf{randomized weights}. Consider the following generative algorithm.

\ALGstochrow

\ALGstoch

A stochastic row is a vector whose entries are all dependent. This is because we normalize by the row sum, making things very difficult. Since it is not the perview of this thesis, we won't try to find
the exact distribution.
As such, to avoid the process of finding the distribution of these dependent entries, we will encapsulate the distribution and abstract it away by calling these implicit distributions!


%=========================================================================================

\subsection{Random Matrices}

With the various types of $\D$-distributions defined, the definition of a random matrix is quite simple.

\begin{definition}[Random Matrix]
Let $P \sim \D$ be an $N \times N$ matrix over $\F$. Then, the entries of $P$ are elements in $\F$ completely determined by the $\D$-distribution, regardless of what type it is.
Also, if $\D$ is an explicit distribution, $\D^\dagger$ represents the symmetric/hermitian version $\D$.
\end{definition}

% \begin{definition}[Random Matrix]
% Assuming $\D$ is an explicit distribution, a random matrix is any matrix over the field $\F$ is a matrix $M \in \F^{N \times N}$ is a matrix whose entries are i.i.d random variables.
% So, if a random matrix $M = (m_{ij})$ is $\mathcal{D}$-distributed, then we say $m_{ij} \sim \mathcal{D}$.
% In the scope of this thesis, assume every random matrix to be homogenously distributed.
% Otherwise, if $\D$ is an implicit distribution, then $P$ is a matrix whose entries are determined by the algorithm imposed by $\D$.
% \end{definition}

Here is a clarification on symmetric/hermitian versions of distributions.

\begin{remark}[Symmetric/Hermitian Matrices]
As mentioned in the definition, we automatically have a class of derivative $\D$-distributions given that they are explicitly distributed denoted by $\D^\dagger$. To make a matrix symmetric
(or Hermitian if $\F = \C$), then we simply set the elements in the upper triangle to be equal to those in the lower triangle. There is also an alternative algorithm provided in the appendix. (See B.x)
\end{remark}

Sometimes, our distribtions may depend on the dimension of the matrix $N$.

\begin{remark}[Dependence on $N$]
Every implicit $\D$-distribution will have some sort of dependence on the dimension of the matrix $N$.
Non-homogenous explicit distributions may have a dependence, in the case of Hermite-$\beta$ matrices, it happens that they do.
It is only e.h. $\D$-distributions that never have a dependence on $N$, matrices with such distributions have entries which are i.i.d.
\end{remark}

As mentioned in the definition, a random matrix defined over a field $\F$ has entries from $\F$ that are determined by the $\D$-distribution.
Sometimes, specify that a matrix may have complex entires. We notate this by specifying $\F = \Cc$, and signify it as described below.

\begin{remark}[Complex Entries]
To say that a random matrix is explicitly $\D$-distributed over $\Cc$ would mean that its entries take the form $a + bi$ where $a,b \sim \D$ are random variables.
In other words, if we allow the matrix to have complex entries by setting $\F = \Cc$, then we must sample the real and imaginary component as $\D$-distributed i.i.d. random variables.
Note that this means we cannot set the field for implicit $\D$ distributions, as the field is automatically chosen by the generative algorithm.
\end{remark}

\medskip
\noindent Below, we can see code on how to generate a standard normal random matrix using the $\textbf{RMAT}$ package.
\begin{code}[Standard Normal Matrix]
Let $\mathcal{D} = \Normal(0,1)$. We can generate $P \sim \D$, a $4 \times 4$ standard normal matrix, as such:
\end{code}

\begin{lstlisting}[language=R]
library(RMAT)
P <- RM_norm(N = 4, mean = 0, sd = 1)
# Outputs the following
P
           [,1]       [,2]       [,3]        [,4]
[1,]  0.1058257 -1.0835598 -0.7031727  1.01608625
[2,] -0.2170453  1.8206070 -0.4539230  0.06828296
[3,]  1.3002145  0.1254992 -0.5214005 -0.61516174
[4,] -1.0398587  0.1975445 -0.8511950  0.86366082
\end{lstlisting}


\newpage

%=========================================================================================
%=========================================================================================

\section{The Crew: Ensembles}

With a random matrix well defined, we may now motivate one of the most important ideas - the random matrix ensemble. One common theme in this thesis will be that random matrices on their own provide little information. When we consider them at the ensemble level, we start to obtain more fruitful results. Without further ado, we motivate the random matrix ensemble.

\begin{definition}[Random Matrix Ensemble]
A $\D$-distributed random matrix ensemble $\Ens$ over $\F^{N \times N}$ of size $K$ is defined as a set of $\D$-distributed random matrices $\Ens = \{P_i \sim \mathcal{D} \mid P_i \in \F^{N \times N}\}_{i = 1}^K$. In simple words, it is simply a collection of $K$ iterations of a specified class of random matrix.
\end{definition}

\medskip
\noindent So, for example, we could compute a simple ensemble of matrices as follows.
\begin{code}[Standard Normal Hermitian Ensemble]
Let $\mathcal{D} = \Normal(0,1)^{\dagger}$. We can generate $\Ens \sim \D$ over $\Cc$, an ensemble of $4 \times 4$ complex Hermitian standard normal matrices of size 10 as such:
\end{code}

\begin{lstlisting}[language=R]
library(RMAT)
# By default, mean = 0 and sd = 1.
ensemble <- RME_norm(N = 4, cplx = TRUE, herm = TRUE, size = 10)
# Outputs the following
ensemble
...
[[10]]
                  [,1]              [,2]              [,3]
[1,] -0.59931+1.24286i  1.29457+0.66058i  0.83539-0.16662i
[2,]  1.29457-0.66058i  0.78841+0.09818i -1.16592+1.14666i
[3,]  0.83539+0.16662i -1.16592-1.14666i -0.51256+0.17750i
\end{lstlisting}

With this in mind, we will survey and characterize, and briefly discuss a few special recurring ensembles in this thesis.

%=========================================================================================

\subsection{Erdos-Renyi $p$-Ensembles}

The Erdos-Renyi transition matrices are a class of stochastic matrices. Any stochastic matrix represents a walk on a fixed graph, the particular class of random graphs that we will consider are the Erdos-Renyi random graphs. Essentially, these are graphs whose vertices are connected with a uniform probability $p$. We can interpret this as saying an Erdos-Renyi graph is a simple random walk on a graph with parameterized sparsity (given by $p$). Without further ado, we motivate the Erdos-Renyi graph:

\begin{definition}[Erdos-Renyi Graph]
An Erdos-Renyi graph is a graph $G = (V,E)$ with a set of vertices $V = \{\oneto[N]\}$ and edges $E = \mathds{1}_{i,j \in V} \sim \Bern(p_{ij})$. It is homogenous if $p_{ij} = p$ is fixed for all $i, j$.
\end{definition}

Essentially, an Erdos-Renyi graph is a graph whose 'connectedness' is parameterized by a probability $p$ (assuming it's homogenous, which this document will unless otherwise noted).
As $p \to 0$, we say that graph becomes more sparse; analogously, as $p \to 1$ the graph becomes more connected.

Recall from probability theory that a sum of i.i.d Bernoulli random variables is a Binomial variable.
As such, we may alternatively say that the degree of each vertex $v$ is distributed as $deg(v) \sim Bin(N,p)$ where $N$ is the number of vertices.
This makes simulating the graphs much easier. This is demonstrated in the algorithm used to generate the matrices.  (See B.x)

\ALGerdos

\begin{warning}
Note that we are not considering the adjacency matrix of an Erdos-Renyi graph. Rather, we are simulating a transition matrix that \textbf{represents a walk on one}.
\end{warning}

\begin{code}[Erdos-Renyi p = 0.5 Ensemble]
Let $\mathcal{D} = \text{ER}(p = 0.5)$. We can generate $\Ens \sim \D$, an ensemble of $4 \times 4$ Erdos-Renyi matrices ($p = 0.5$) of size 10 as such:
\end{code}

\begin{lstlisting}[language=R]
library(RMAT)
ensemble <- RME_erdos(N = 4, p = 0.5, size = 10)
# Outputs the following
ensemble
...
[[10]]
          [,1]      [,2]      [,3]     [,4]
[1,] 0.0000000 0.1729581 0.8270419 0.000000
[2,] 0.0000000 0.0000000 1.0000000 0.000000
[3,] 0.2557890 0.3766740 0.0000000 0.367537
[4,] 0.2151029 0.3929580 0.3919391 0.000000
\end{lstlisting}

%=========================================================================================

\subsection{Hermite $\beta$-Ensembles}

The Hermite $\beta$-Ensembles will be one of the primary ensembles discussed in this thesis. The Hermite $\beta$-ensembles are a normal-like class of random matrices. This ensemble will be characterized, motivated, and defined more thoroughly in $\textbf{Chapter 4}$. However, we will give a brief introduction to the ensemble.

At a practical level, all one would need to know is that the matrices are generated in accordance to an algorithm found in the appendix (Algorithm B.x) and in Chapter 4.

%=========================================================================================

%\section{Analytical Results}

%****************************************************************************************%
\minititle{Summary Table of $\D$-Distributions}
\begin{center}
  \Ddisttable
\end{center}
%****************************************************************************************%
