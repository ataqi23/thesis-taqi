
\chapter{Random Matrices}

%=========================================================================================
\epigraph{Unfortunately, no one can be told what The Matrix is. You'll have to see it for yourself.}{\textit{Morpheus \\ The Matrix}}
%=========================================================================================

As discussed in the introduction, this thesis will be an exploration of spectral statistics of random matrices. This means that we must first be able to understand what random matrices are.
At a fundamental level, random matrices are simply matrices which have \textit{some or all} entries as random variables that are distributed in accordance to either some explicit distribution or algorithm.
So, to define a random matrix, our approach will be to do so by formalizing and defining what it mean for one to be $\D$-distributed.
This way, the notation encapsulates and completely characterizes the random matrix.

Prior to beginning the discussion on $\D$-distributions, the reader should be familiar or at least accquainted with the notion of random variables and what they are.
For a review, see \textbf{Appendix A.2.1}.

%=========================================================================================
%=========================================================================================

\section{$\D$-Distributions}

As a general rule, when it comes to random simulation, there is usually a rule or constraint to which the randomness must conform.
For example, sampling a vector of random variables from a distribution is a rudimentary example of this.
For random matrices, there are various techniques for generating their entries that are not just sampling from theoretical distributions.
As such, we motivate the $\D$-distribution: a generalized matrix entry distribution framework.

\begin{definition}[$\D$-distribution]
Suppose $P$ is a $\D$-distributed random matrix. Then, we notate this $P \sim \D$. In the simplest of terms, $\D$ is essentially the algorithm that generates the entries of $P$.
We define two primary methods of distribution: \textbf{explicit} distribution, and \textbf{implicit} distribution.
If $\D$ is an explicit distribution, then some or all the entries of $P$ are independent random variables with a given distribution.
Otherwise, if $\D$ is implicit, then the matrix has dependent entries imposed by the algorithm that generates it.
\end{definition}

%=========================================================================================

\subsection{Explicit Distributions}

\minititle{Homogenous Explicit $\D$-distributions}

The simplest type of $\D$-distribution is one that is explicitly and homogenously distributed. From thereonafter, this will be shortened as e.h.d.
Suppose $\D$ is a probability distribution for random variables in the classical sense. The simplest way to think of e.h.d distributions is to use the concept of notational overload.
For example, it is unambiguous to say that an r.v. $X \sim \Normal(0,1)$. However, the same cannot be said if we said a matrix $P \sim \Normal(0,1)$.

That being said, we can define e.h $\D$-distributions as an notational extension that means \textbf{every} entry of the matrix is an i.i.d random variable with that same distribution!
In otherwords, we simply perform entry-wise sampling from the corresponding r.v. distribution.

Note that this means by construction, $\D$ can only be e.h.d. if it has a corresponding probability distribution for random variables.

\begin{definition}[Homogenous Explicit $\D$-distribution]
Suppose $P \sim \D$ where $\D$ is a homogenous and explicit distribution. Additionally, let $\D^*$ denote the corresponding random variable analogue of $\D$.
Then, every single entry of $P$ is an i.i.d random variable with the corresponding distribution. That is,
$$ P \sim \D \iff \forall i,j \mid p_{ij} \sim \D^* $$
\end{definition}

\begin{example}
Suppose $P \sim \Normal(0,1)$ and that $P$ is a $2 \times 2$ matrix.
Then, $p_{11}, p_{12}, p_{21}, p_{22}$ are independent, identically distributed random variables with the standard normal distribution.
\end{example}

\ALGexplicit

\begin{formalization}
Explicit homogenous distributions can be formalized as overloading the standard notation of random variable distribution as seen in probability theory.
This way, our random matrix has a representation as a random vector, which we commonly encounter in probability theory as a (i.i.d) sequence of random variables!
Suppose $P$ is an $N \times N$ random matrix that is explicitly and homogenously $\D$-distributed. Then, this would mean that $P$ is an array of $N^2$ i.i.d random variables sampled from $\D$.
\end{formalization}

\minititle{Non-Homogenous Explicit $\D$-distributions}

There are a few instances where we encounter the need to only initialize a subset of a matrix's entries as random variables. In this case, we say that a matrix has a non-homogenous explicit $\D$-distribution.
This type of distribution is similar to e.h. $\D$-distributions, but we no longer have the ability to overload random variable notation and indicate that only some entries have such distribution. This would be
confusing because not every entry has the same distribution anymore. As such, we have to define new notation to do so.

Simply put, a non-homogenous explicit $\D$-distribution can be completely characterized by listing the distributions of every entry.
In this thesis, there is only one application of this type of $\D$-distribution; however, it describes the entry disributions by its diagonals --- this is the Hermite $\b$-ensemble matrix.
Since its definition is characterized by distributions on the diagonals, we may avoid full entry-wise generality for the purpose of being succinct.
As such, we will only describe non-homogenous explicit $\D$-distributions as schemes where we assign diagonal bands a specific vector of i.i.d random variables.

\begin{definition}[Diagonal Bands]
Suppose $P = (p_{ij})$ is an $N \times N$ matrix. Then, $P$ may be partitioned into $2n - 1$ rows called diagonal bands. Each band is denoted $[\rho]_P$ where $[\rho]_P = \{p_{ij} \mid \rho = i - j\}$. We have
$\rho \in \{ -(N-1), \dots, -1, 0, 1, \dots, N-1 \}$.
\end{definition}

Heres an example of using the diagonal bands constructor notation.

\begin{example}
Suppose $P$ is an $N \times N$ matrix. Then, $[0]_P$ is the main diagonal of $P$ since $p_{ii} \Ra i = j \Ra i - j = 0 \Ra p_{ii} \in [0]_P$.
Similarly, the main off-diagonal in the upper triangle is $[-1]_P$ since $p_{12} \in [-1]_P$.
Likewise, the main off-digonal in the lower triangle is $[1]_P$. The entry in the top-right corner of the matrix, $p_{1N}$ solely comprises $[1 - N]_P$.
\end{example}

\begin{code}
Here is an example utilizing diagonal bands constructors in R.
\end{code}

\begin{lstlisting}[language=R]
# Set the dimension of the matrix
N <- 5
# Generate an example matrix of zeros
P <- matrix(rep(0, N^2), nrow = N)
# Assign the upper main off-diagonal band as a vector
rho <- -1
P[row(P) - col(P) == rho] <- rnorm(n = 4, mean = 0, sd = 1)
# Assign the main diagonal as a vector
rho <- 0
P[row(P) - col(P) == rho] <- rep(10, N)
# Returns the following
P
     [,1]       [,2]       [,3]       [,4]      [,5]
[1,]   10  0.1932123  0.0000000  0.0000000  0.000000
[2,]    0 10.0000000 -0.4346821  0.0000000  0.000000
[3,]    0  0.0000000 10.0000000  0.9132671  0.000000
[4,]    0  0.0000000  0.0000000 10.0000000  1.793388
[5,]    0  0.0000000  0.0000000  0.0000000 10.000000
\end{lstlisting}

%The general defintion is below.

% \begin{definition}[Distribution Rule Set]
% Suppose $\D$ is a non-homogenous explicit $D$-distribution. Then, for an $N \times N$ matrix, $A_\D = \{a_{ij} = \D_{ij}\}$ is its rule set.
% \end{definition}
%
% The general defintion using purely diagonal bands is below.
%
% \begin{definition}[Diagonal Rule Set]
% Suppose $\D$ is a non-homogenous explicit $D$-distribution. Then, for an $N \times N$ matrix, $A_\D = \{a_{ij} = \D_{ij}\}$ is its rule set.
% \end{definition}

For the distribution using diagonal bands, consider the definiton of the beta matrix below. %refer to the definition of a beta matrix in \textbf{Section 1.2.2}.

\ALGbeta

%for $k \in \N_{N-1}$.

\begin{definition}[Hermite-$\b$ Matrix]
Suppose $P \sim \H(\b)$ is an $N \times N$ matrix. Then, the main diagonal $[0]_P \sim \Normal(0,2)$.
Additionally, both the main off-digaonals are equal and they are given by $[1]_{P} = [-1]_{P} = \vec{X} = (X_k)_{k=1}^{N-1}$ where $X_k \sim \chi(\text{df} = \beta k)$.
As such, we obtain a Hermite-$\b$ distributed matrix. Note that this is a symmetric tridiagonal matrix.
\end{definition}

\begin{remark}[Off-Diagonal Distributions]
Note that the off-diagonal distributions are chi variables with varying degrees of freedom; in fact it is an increasing linear sequence that is a multiple of $\b$.
For this reason, we cannot say that the off-diagonal entries are identically distributed despite them being within the same diagonal band. However, they do remain independent.
On the other hand, the diagonal is a vector of $N$ i.i.d $\Normal(0,2)$ variables.
\end{remark}

\begin{warning}[$\b$-Notation]
Please note that the Hermite $\b$-ensemble matrices are \textbf{not} related to the $\Beta$ distribution.
Instead, $\b \in \N$ is a natural number that determines the nature of the matrix ensemble (more later). The $\Beta(a,b)$ distribution is a r.v. distribution that
takes in two parameters. For this reason, it is possible to have a homogenous explicit distribution $P \sim \Beta(a,b)$, but they do not mean the same thing.
\end{warning}


%=========================================================================================

\subsection{Implicit Distributions}

Now, we are done with defining the explicit $\D$-distribtions. The other type of $\D$-distribution is the implicit $\D$-distribution.
As a general rule, when it comes to implicit $\D$-distributions, we are concerned less about the distribution of actual the matrix entries and moreso about its holisitic properties.

In this thesis, we will cover one type of implicit $\D$-distributed matrix: the stochastic matrix. One might ask, what are stochastic matrices?
Stochastic matrices, in short, are matrices that represent random walks on Markov Chains (see \textbf{Appendix A.3}). So, for a fixed graph, there exists a matrix that represents a random walk on that graph.
We call that either the stochastic matrix or transition matrix of the graph.

We will consider two variations of stochastic matrices: fully connected stochastic matrices and sparse stochastic matrices.
The sparse stochastic matrices will be refered as Erdos-Renyi matrices, they are essentially the same as stochastic matrices, except they have a parameter $p$ that
determines how connected the graph it represents is. They will be discussed in more detail in \textbf{Section 1.2.1}.

\minititle{Stochastic Matrices}

Stochastic matrices will serve as our canonical implicitly distributed $\D$-distributed matrices. How are they distributions?

Conventionally, stochastic matrices represent random walks on fixed graphs - so what does it mean to sample a random stochastic matrix?
Well, the algorithm tells us that sampling a random stochastic matrix represents a random walk on a fixed random graph with \textbf{randomized weights}.
In other words, we sample a fully connected graph, and randomize the weights of each edge. As mentioned previously, in \textbf{Section 1.2.1}, we explore walks on Erdos-Renyi graphs,
which are graphs with introduced sparsity (have edges with weight 0) as opposed to fully connected graphs.

Consider the following generating algorithms below. We first start by generating a stochastic row - a row that sums to one. Afterwards, we simply take $N$ stochastic rows to generate a stochastic matrix.

%AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
\ALGstochrow
%AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA

As such, we define the implicit $\D$-distribution corresponding to a transition matrix on a complete graph with randomized weights as $\D = \text{Stochastic}$ as the distribution that
is characterized by the generating algorithm below.

%AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
\ALGstoch
%AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA

\begin{remark}[Dependent Entries]
A stochastic row is a vector whose entries are all dependent; this is due to the fact that we normalize by the row sum.
As such, this makes finding the distribution of entries very difficult. Since it is not the perview of this thesis, we won't try to find the exact distribution.
As such, to avoid the process of finding the distribution of these dependent entries, we will encapsulate the distribution and abstract it away by calling it an implicit distribution!
\end{remark}

Additionally, our distribtions depend on the dimension of the matrix $N$.

\begin{remark}[Dependence on $N$]
Every implicit $\D$-distribution will have some sort of dependence on the dimension of the matrix $N$.
Non-homogenous explicit distributions may have a dependence, in the case of Hermite-$\beta$ matrices, it happens that they do.
It is only e.h. $\D$-distributions that never have a dependence on $N$, matrices with such distributions have entries which are i.i.d.
\end{remark}

%=========================================================================================

\subsection{Random Matrices}

With the various types of $\D$-distributions defined, the definition of a random matrix is quite simple.

\begin{definition}[Random Matrix]
Let $P \sim \D$ be an $N \times N$ matrix over $\F$. Then, the entries of $P$ are elements in $\F$ completely determined by the $\D$-distribution, regardless of what type it is.
Also, if $\D$ is an explicit distribution, $\D^\dagger$ represents the symmetric/hermitian version of $\D$.
\end{definition}

Here is a clarification on symmetric/hermitian versions of distributions.

\begin{remark}[Symmetric/Hermitian Matrices]
As mentioned in the definition, we automatically have a class of derivative $\D$-distributions given that they are explicitly distributed denoted by $\D^\dagger$. To make a matrix symmetric
(or Hermitian if $\F = \C$), then we simply set the elements in the upper triangle to be equal to those in the lower triangle.
\end{remark}

As mentioned in the definition, a random matrix defined over a field $\F$ has entries from $\F$ that are determined by the $\D$-distribution.
Sometimes, we may specify a matrix to have complex entries. We notate this by specifying $\F = \Cc$, and interpret it as described below.

\begin{remark}[Complex Entries]
To say that a random matrix is explicitly $\D$-distributed over $\Cc$ would mean that its entries take the form $a + bi$ where $a,b \sim \D$ are random variables.
In other words, if we allow the matrix to have complex entries by setting $\F = \Cc$, then we must sample the real and imaginary component as $\D$-distributed i.i.d. random variables.
Note that this means we cannot set the field for implicit $\D$ distributions, as the field is automatically chosen by the generative algorithm.
\end{remark}

\medskip
 Below, we can see code on how to generate a standard normal random matrix using the $\textbf{RMAT}$ package.
\begin{code}[Standard Normal Matrix]
Let $P \sim \Normal(0,1)$ be a $4 \times 4$ random matrix. Then, we may generate $P$ as such:
\end{code}

\begin{lstlisting}[language=R]
library(RMAT)
P <- RM_norm(N = 4, mean = 0, sd = 1)
# Outputs the following
P
           [,1]       [,2]       [,3]        [,4]
[1,]  0.1058257 -1.0835598 -0.7031727  1.01608625
[2,] -0.2170453  1.8206070 -0.4539230  0.06828296
[3,]  1.3002145  0.1254992 -0.5214005 -0.61516174
[4,] -1.0398587  0.1975445 -0.8511950  0.86366082
\end{lstlisting}


\newpage

%=========================================================================================
%=========================================================================================

\section{The Crew: Ensembles}

With a random matrix well-defined, we may now motivate one of the most important ideas - the random matrix ensemble.
Simply put, a random matrix ensemble is essentially a collection of various matrices distributed the same way.
One common theme in this thesis is that we will find that on their own, random matrices provide little information. However, when we consider them at the ensemble level, we start to obtain more fruitful results.
Without further ado, we define the random matrix ensemble.

\begin{definition}[Random Matrix Ensemble]
A $\D$-distributed ensemble $\Ens$ of $N \times N$ random matrices over $\F$ of size $K$ is defined as a set of $K$ iterations of that class of random matrix, and it is denoted:
$$ \Ens = \bigcup_{i = 1}^K P_i \where P_i \sim \mathcal{D} \and P_i \in \F^{N \times N} $$
\end{definition}

\medskip
 So, for example, we could compute a simple ensemble of matrices as follows.
\begin{code}[Standard Normal Hermitian Ensemble]
Let $\mathcal{D} = \Normal(0,1)^{\dagger}$. We can generate $\Ens \sim \D$ over $\Cc$, an ensemble of $3 \times 3$ complex Hermitian standard normal matrices of size 10 as such:
\end{code}

\begin{lstlisting}[language=R]
library(RMAT)
# By default, mean = 0 and sd = 1.
ensemble <- RME_norm(N = 3, cplx = TRUE, herm = TRUE, size = 10)
# Outputs the following
ensemble
...
[[10]]
                  [,1]              [,2]              [,3]
[1,] -0.59931+1.24286i  1.29457+0.66058i  0.83539-0.16662i
[2,]  1.29457-0.66058i  0.78841+0.09818i -1.16592+1.14666i
[3,]  0.83539+0.16662i -1.16592-1.14666i -0.51256+0.17750i
\end{lstlisting}

With this in mind, we will survey and characterize, and briefly discuss a few special recurring ensembles in this thesis.

%=========================================================================================

\subsection{Erdos-Renyi $p$-Ensembles}

As mentioned in \textbf{Section 1.1.3}, Erdos-Renyi transition matrices are a specific class of stochastic matrices.
Again, a stochastic matrix represents a random walk on a fixed graph. For $\D = \text{Stochastic}$, we generate matrices that represent walks on fully connected graphs with randomized weights.
For the Erdos-Renyi matrices, we will alternatively be considering a different graph to represent --- the $p$-Erdos-Renyi graphs.

Essentially, these are graphs whose vertices are connected with a uniform probability $p$.
This way, we add more variation since the connectivity of a graph is one feature of interest.
Without further ado, we motivate the Erdos-Renyi graph:

\begin{definition}[Erdos-Renyi Graph]
An Erdos-Renyi graph is a graph $G = (V,E)$ with a set of vertices $V = \{\oneto[N]\}$ and edges $E = \mathds{1}_{i,j \in V} \sim \Bern(p_{ij})$. It is homogenous if $p_{ij} = p$ is fixed for all $i, j$.
\end{definition}

Essentially, an Erdos-Renyi graph is a graph whose ``connectedness'' is parameterized by a probability $p$.
We will assume every edge $(i,j)$ has a non-zero weight with probability $p$ (so $p_{ij} \neq 0$).
As $p \to 0$, we say that graph becomes more \textit{sparse}; analogously, as $p \to 1$ the graph becomes more \textit{connected}.

\begin{remark}[Homogeneity]
In this thesis, we will assume that every edge has a uniform probability $p$ of being connected.
That is, we will assume $p_{ij}$ is non-zero with probability $p$ for all $i,j$.
We say that the edge probabilities are homogenous - akin to our description of homogenous $\D$-distributions.
\end{remark}

Recall from probability theory that a sum of i.i.d Bernoulli random variables is a Binomial variable.
As such, we may alternatively say that the degree of each vertex $v$ (corrosponding to a row) is distributed as $deg(v) \sim Bin(N,p)$ where $N$ is the number of vertices.
This makes simulating the graphs much easier. To achieve this effect, we first sample the vertex degree, and then uniformly randomly sever edges by setting the weights of $N - deg(v)$ edges to $0$. \newline

All, that being said, we now motivate the defintion of an $p$-Erdos-Renyi transition matrix as an implicit $\D$-distribution by the generating algorithm below.

\ALGerdos

We can interpret this as saying an Erdos-Renyi graph is a simple random walk on a graph with parameterized sparsity (given by $p$).

\begin{warning}
Note that we are not considering the adjacency matrix of an Erdos-Renyi graph. Adjacency matrices are a commonly studied class of matrices that also represent graphs.
However, we are simulating a transition matrix, which \textbf{represents a random walk on one}.
\end{warning}

Consider the code example below, where generate an Erdos-Renyi ensemble of matrices.

\begin{code}[Erdos-Renyi p = 0.5 Ensemble]
Let $\mathcal{D} = \text{ER}(p = 0.5)$. We can generate $\Ens \sim \D$, an ensemble of $4 \times 4$ Erdos-Renyi matrices ($p = 0.5$) of size 10 as such below.
Notice how half of all the entries are $0$, which is what we expect since $p = 0.5$ implies that we expect half the edges to not be connected (have weight 0).
\end{code}

\begin{lstlisting}[language=R]
library(RMAT)
ensemble <- RME_erdos(N = 4, p = 0.5, size = 10)
# Outputs the following
ensemble
...
[[10]]
          [,1]      [,2]      [,3]     [,4]
[1,] 0.0000000 0.1729581 0.8270419 0.000000
[2,] 0.0000000 0.0000000 1.0000000 0.000000
[3,] 0.2557890 0.3766740 0.0000000 0.367537
[4,] 0.2151029 0.3929580 0.3919391 0.000000
\end{lstlisting}

%=========================================================================================

\subsection{Hermite $\beta$-Ensembles}

The Hermite $\beta$-Ensembles will be one of the primary ensembles discussed in this thesis. The Hermite $\beta$-ensembles are a normal-like class of random matrices.
This ensemble will be characterized, motivated, and defined more thoroughly in $\textbf{Chapter 4}$. However, we will give a brief introduction to the ensemble.

As mentioned briefly in \textbf{Section 1.1.1}, the $\b$-ensemble is an ensemble whose matrix model has a special unique generative algorithm. There is a reason we
care about this type the $\b$-matrix. This is because their original characterization is not in this matrix model with a non-homogenous $\D$-distribution. Rather, it is
a matrix model that is characterized by its joint density of eigenvalues!

That is, given a parameter $\b \in \N$, the Hermite $\b$-ensemble represents a class of matrices that have an explicit formula for the joint density of eigenvalues which
ultimately is what characterizes the ensemble. This leads in quite nicely to our discussion of our first spectral statistic: the spectrum of a matrix, which is all concerned
about the distribution of eigenvalues.

%=========================================================================================

%\section{Analytical Results}

%****************************************************************************************%
\minititle{Summary Table of $\D$-Distributions}
\begin{center}
  \Ddisttable
\end{center}
%****************************************************************************************%
