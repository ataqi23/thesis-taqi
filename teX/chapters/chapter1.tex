\chapter{Random Matrices}

As discussed in the introduction, this thesis will be an exploration of spectral statistics of random matrices. This means that we must first be able to understand what random matrices are. At a fundamental level, random matrices are simply matrices whose entries are randomly distributed in accordance to some distribution or method. To formalize all these notions, we will define what random matrices are and what it means for them to be $\D$-distributed.

\section{Introduction}
When it comes to random simulation, there must always be a rule to which our randomness must conform, regardless of complexity. For example, sampling a vector from a distribution is a rudimentary example of this. For random matrices, there will be a few methods of generating their entries that are not just sampling from theoretical distributions. As such, we motivate the $\D$-distribution.

\begin{definition}[$\D$-distribution]
When we define a random matrix that is $\D$-distributed, we say that $P \sim \D$. In the simplest of terms, $\D$ is essentially the algorithm that generates the random matrix $P$. There are two primary types of algorithms that will be covered: explicit distribution and implicit distribution. If $\D$ is an explicit distribution, then we simply sample every entry of $P$ to be from that distribution. Otherwise, if it is implicit, we utilize an algorithm that enforces an implicit distribution of the entries. 
\end{definition}


$\textbf{Explicit Distributions}$ \hfill

The simplest case is explicitly distributed random matrices. If $\D$ is an expxlicit distribution, then we overload the notation $\D$ to mean a probability distribution in the classical sense (see Appendix A.1). So, if $\D$ is a probability distribution, the matrix $P \sim \D$ when $p_{ij} \sim \D$. In otherwords, we simply perform entry-wise sampling from that distribution.

\begin{enumerate}
\item  If $\D = \Normal(0,1)$, then $p_{ij} \sim \Normal(0,1)$.
\item  If $\D = \Unif(0,1)$, then $p_{ij} \sim \Unif(0,1)$.
\end{enumerate}

\newpage

$\textbf{Implicit Distributions}$ \hfill

In the latter case, we are concerned less about the distribution of the matrix entries and moreso about its holisitic properties.

\begin{enumerate}
\item If $\D = \text{Stochastic}$, then the matrix is a row of random stochastic rows. (See Algorithm B.1.)
\item If $\D$ is any distribution (implicit or explicit), then $\D^{\dagger}$ is the Symmetric/Hermitian version of $\D$. (See Algorithm B.2.)
\end{enumerate}

\begin{definition}[Random Matrix]
Assuming $\D$ is an explicit distribution, a random matrix is any matrix over the field $\F$ is a matrix $M \in \F^{N \times N}$ is a matrix whose entries are i.i.d random variables. So, if a random matrix $M = (m_{ij})$ is $\mathcal{D}$-distributed, then we say $m_{ij} \sim \mathcal{D}$. In the scope of this thesis, assume every random matrix to be homogenously distributed. Otherwise, if $\D$ is an implicit distribution, then $P$ is a matrix whose entries are determined by the algorithm imposed by $\D$.
\end{definition}

\begin{code}[Standard Normal Matrix]
Let $\mathcal{D} = \Normal(0,1)$. We can generate $P \sim \D$, a $4 \times 4$ standard normal matrix, as such:
\end{code}

\begin{lstlisting}[language=R]
# Using the RMAT package
library(RMAT)
P <- RM_norm(N = 4, mean = 0, sd = 1)

# Outputs the following
P

           [,1]       [,2]       [,3]        [,4]
[1,]  0.1058257 -1.0835598 -0.7031727  1.01608625
[2,] -0.2170453  1.8206070 -0.4539230  0.06828296
[3,]  1.3002145  0.1254992 -0.5214005 -0.61516174
[4,] -1.0398587  0.1975445 -0.8511950  0.86366082
\end{lstlisting}

\newpage

\section{The Crew: Ensembles}

With a random matrix well defined, we may now motivate one of the most important ideas - the random matrix ensemble. One common theme in this thesis will be that random matrices on their own provide little information. When we consider them at the ensemble level, we start to obtain more fruitful results. Without further ado, we motivate the random matrix ensemble.

\begin{definition}[Random Matrix Ensemble]
A $\D$-distributed random matrix ensemble $\Ens$ over $\F^{N \times N}$ of size $K$ is defined as a set of $\D$-distributed random matrices $\Ens = \{P_i \sim \mathcal{D} \mid P_i \in \F^{N \times N}\}_{i = 1}^K$. In simple words, it is simply a collection of $K$ iterations of a specified class of random matrix.
\end{definition}

So, for example, we could compute a simple ensemble of matrices as follows.

\begin{code}[Standard Normal Hermitian Ensemble]
Let $\mathcal{D} = \Normal(0,1)^{\dagger}$. We can generate $\Ens \sim \D$ over $\Cc$, an ensemble of $4 \times 4$ complex Hermitian standard normal matrices of size 10 as such:
\end{code}

\begin{lstlisting}[language=R]
# Using the RMAT package
library(RMAT)
# Note that RM_norm takes mean = 0 and sd = 1 as default values.
ensemble <- RME_norm(N = 4, cplx = TRUE, herm = TRUE, size = 10)

# Outputs the following
ensemble

...
[[10]]
           [,1]       [,2]       [,3]        [,4]
[1,]  0.1058257 -1.0835598 -0.7031727  1.01608625
[2,] -0.2170453  1.8206070 -0.4539230  0.06828296
[3,]  1.3002145  0.1254992 -0.5214005 -0.61516174
[4,] -1.0398587  0.1975445 -0.8511950  0.86366082
\end{lstlisting}

With this in mind, we will gloss over, characterize, and briefly discuss a few special recurring ensembles in this thesis.

\subsection{Hermite $\beta$-Ensembles}

The Hermite $\beta$-ensembles, also called the Gaussian ensembles, are an important class of random matrix ensembles studied in engineering, statistical physics, and probability theory. Parameterized by $\beta \in \N$ through the Dyson index, this ensemble is charecterized by a few things.

\begin{itemize}
  \item The Dyson index $\beta$ corrosponds to the number of real number of components the subject matrices have.
  \item The subject matrices are classically defined for $\beta = 1,2,4$ and they corrospond to matrices with real, complex, and quaternionic entries. The corrosponding fields are $\R$, $\Cc$, and $\mathbb{H}$.
  \item The matrices in this ensemble, most importantly, have a feature called conjugation invariance. With respect to the conjugation by the respective group of matrices.
  \item Most importantly, the eigenvalues are determined by the joint probability density function given below.
\end{itemize}

\begin{definition}[Hermite $\beta$-ensembles]
The Hermite $\beta$-ensembles, commonly known as the Gaussian ensembles, are an ensemble of random matrices parameterized by $\beta$, and their eigenvalues have the joint probability density function:
\begin{align*}
f_\beta(\Lambda) = c_H^\beta \prod_{i < j} |\lambda_i - \lambda_j|^\beta e^{-1/2\sum_i \lambda_i^2}
\end{align*}
where the normalization constant $c_H^\beta$ is given by:
\begin{align*}
c_H^\beta = (2\pi)^{-n/2} \prod_{j = 1}^n \frac{\Gamma(1 + \beta/2)}{\Gamma(1 + \beta j/2)}
\end{align*}
\end{definition}

To simulate matrices from the $\beta$-ensemble, we will be using a recent result published in ``Matrix Models for Beta Ensembles'' \cite{dimitriu:2018}. This makes the $\beta$-ensemble a canonical example of an implicity distributed matrix; we do not care about the actual distribution of the entries, but rather the effect they have on the eigenvalues (trace) of the matrices. The algorithm used is directly cited from the results of Dumitriu's paper, and can be found in Algorithms B.1.3.

\begin{code}[Hermite Beta = 2 Ensemble]
Let $\mathcal{D} = \mathcal{H}(\beta = 2)$. We can generate $\Ens \sim \D$, an ensemble of $4 \times 4$ Hermite matrices ($\beta = 2$) of size 10 as such:
\end{code}

\begin{lstlisting}[language=R]
# Using the RMAT package
library(RMAT)
ensemble <- RME_beta(N = 4, beta = 2, size = 10)

# Outputs the following
ensemble

...
[[10]]
           [,1]       [,2]       [,3]        [,4]
[1,]  0.1058257 -1.0835598 -0.7031727  1.01608625
[2,] -0.2170453  1.8206070 -0.4539230  0.06828296
[3,]  1.3002145  0.1254992 -0.5214005 -0.61516174
[4,] -1.0398587  0.1975445 -0.8511950  0.86366082
\end{lstlisting}

\subsection{Erdos-Renyi $p$-Ensembles}

The Hermite $\beta$-ensembles are a normal-like class of random matrices. Now, we will veer away from the normal distribution as a whole and switch to a different class of matrices: stochastic matrices. Stochastic matrices, in short, are matrices that represent Markov Chains (see A.3.1). We can also think of them as the matrix representation of a specific setup of a walk on a random graph. 

A particular class of random graphs that we will consider are the Erdos-Renyi random graphs. Essentially, these are graphs whose vertices are connected with a uniform probability $p$. We can interpret this as saying an Erdos-Renyi graph is a simple random walk on a graph with parameterized sparsity (given by $p$). Without further ado, we motivate the Erdos-Renyi graph:

\begin{definition}[Erdos-Renyi Graph]
An Erdos-Renyi graph is a graph $G = (V,E)$ with a set of vertices $V = \{\oneto[N]\}$ and edges $E = \mathds{1}_{i,j \in V} \sim \Bern(p_{ij})$. It is homogenous if $p_{ij} = p$ is fixed for all $i, j$.
\end{definition}

Essentially, an Erdos-Renyi graph is a graph whose 'connectedness' is parameterized by a probability $p$ (assuming it's homogenous, which this document will unless otherwise noted). As $p \to 0$, we say that graph becomes more sparse; analogously, as $p \to 1$ the graph becomes more connected.\newline
\indent Recall from probability theory that a sum of i.i.d Bernoulli random variables is a Binomial variable. As such, we may alternatively say that the degree of each vertex $v$ is distributed as $deg(v) \sim Bin(N,p)$ where $N$ is the number of vertices. This makes simulating the graphs much easier.

\begin{code}[Erdos-Renyi p = 0.5 Ensemble]
Let $\mathcal{D} = \text{Erdos}(p = 0.5)$. We can generate $\Ens \sim \D$, an ensemble of $4 \times 4$ Erdos-Renyi matrices ($p = 0.5$) of size 10 as such:
\end{code}

\begin{lstlisting}[language=R]
# Using the RMAT package
library(RMAT)
ensemble <- RME_erdos(N = 4, p = 0.5, size = 10)

# Outputs the following
ensemble

...
[[10]]
           [,1]       [,2]       [,3]        [,4]
[1,]  0.1058257 -1.0835598 -0.7031727  1.01608625
[2,] -0.2170453  1.8206070 -0.4539230  0.06828296
[3,]  1.3002145  0.1254992 -0.5214005 -0.61516174
[4,] -1.0398587  0.1975445 -0.8511950  0.86366082
\end{lstlisting}

\section{Analytical Results}




