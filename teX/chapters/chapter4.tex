
\chapter{$\beta$-Ensembles}

%=========================================================================================
\epigraph{With thermodynamics, one can calculate almost everything crudely; with kinetic theory, one can calculate fewer things, but more accurately;
and with statistical mechanics, one can calculate almost nothing exactly.}{\textit{Eugene Wigner}}
%=========================================================================================

\section{Introduction}

In this chapter, we will talk about the Hermite $\beta$-ensembles, also commonly known as the \textit{Gaussian ensembles}, more in depth.
The $\b$-ensembles have wide applications in statistical physics, eningeering, and many other places.
Canonically, there are three ``standard'' $\b$-ensembles, corresponding to the values of $\b = 1, 2, \and 4$.
It was briefly mentioned in a previous section, but what makes these ensembles special is how they are characterized.
Fundamentally, the ensembles are defined by the joint density of their eigenvalues, dependent on the parameter $\beta$ of course.
While $\b$-ensemble model parameter $\b$ has a support of $\N$, the three standard values give us very special properties that we discuss in this chapter.
Extending the model beyond those standard values (which we do using the $\b$-matrix model) as we do in the previous sections in turn mean that the following special characterizations
that will be discussed do not apply anymore.

%=========================================================================================
\newpage
\subsection{Hermite $\beta$-Ensemble}

To begin, we will write down the joint eigenvalue probability density function of a Hermite $\b$-matrix.

\begin{definition}[$\beta$-ensemble]
A (Hermite) $\beta$-ensemble is an ensemble of random matrices parameterized by $\beta$, which determines the joint eigenvalue p.d.f that characterizes it.
So, given an observed set of eigenvalues $\L = (\seqo[N]{\lambda})$. Then, the joint p.d.f. of $\L$ is as follows:
\begin{align*}
\jepdf = C_\beta \prod_{i < j} |\lambda_i - \lambda_j|^\beta e^{-\frac{1}{2} \sum_{i=1}^N \lambda_i^2}
%f_\beta(\Lambda) = c_\beta \prod_{i < j} |\lambda_i - \lambda_j|^\beta \exp({-\frac{1}{2} \sum_{i=1}^N \lambda_i^2})
\end{align*}
where the normalization constant $C_\beta$ is given by:
\begin{align*}
C_{\beta} = (2\pi)^{-n/2} \prod_{j = 1}^n \frac{\Gamma(1 + \frac{\b}{2})}{\Gamma(1 + \frac{\b}{2}j)}
\end{align*}
\end{definition}

%=========================================================================================
\minititle{Breaking Down the Behemoth}

Now, if the reader's reaction to the statement of the $\b$-ensemble was shock or trepidation. Do not worry.
Everything will become clear soon as we start to understand the components of the density function and
focus on characterizing the components rather than try to parse through the dense notation.
Speaking of which, let us start to rewrite the density function with friendlier, more readable notation.
To begin, let us refer to the two primary components as the ``blue term'' and the ``red term'' as shown below.
\begin{center}
$\jepdf = \betaconst \; $\bbx{$\blueterm$} \; \rbx{$\redterm$}
\end{center}

\blocktitle{Normalization Constant} First things first, let us consider the normalization constant $\betaconst$.
This constant is simply a constant needed by $f_\b$ to normalize the p.d.f such that it sums to $1$.
The term is expressed as a product of gamma terms involving $\beta$. Additionally, this term is related to an integral called the \textit{Selberg integral}. (Ref)
While the constant is interesting in terms of its relationship with that integral, it is not under our perview. So, the important takeaway is that it just normalizes our p.d.f,
because every p.d.f needs to sum to $1$ in the end of the day.

\bigskip
%\newpage

\blocktitle{The Blue Term} The first big component, which we will call ``the blue term'' is the term $\blueterme$. Let us break this term down further.
  \begin{enumerate}
    \item The first thing to note is that this term is a product of terms in the form $|\l_j - \l_i|^\b$.
    \item Secondly, note that this product term runs over the indices $1 \leq i < j \leq n$.
  \end{enumerate}
    Here's the great part: the reader has already seen and is already accquianted with those two items! Realize,
  \begin{enumerate}
    \item The term $|\l_j - \l_i|^\b$ is simply the dispersion of $\piij$ with respect to the $\b$-norm dispersion metric $\d_\b$.
      So, we can rewrite $\d_\b(\piij) = |\l_j - \l_i|^\b$.
    \item The indices $\{i < j \mid i,j \in \N_N\}$ may look familiar. This is the lower pairing scheme!
  \end{enumerate}
  As such, when we consider two facts, the interpretation hopefully becomes a little simpler. We can say that the blue term is a product of the dispersion of all eigenvalue pairs in the lower pairing scheme w.r.t the $\b$-norm dispersion metric.
Now, we know that for all $\b \in \N$ that the $\b$-norm is a positive definite metric. As such, this means that the term $\d_\b(\pi)$ is a positive term that grows when the dispersion between the eigenvalues in the pair $\pi$ grow larger.
So, the important takeaway here is that the blue term tells us that the eigenvalues in the $\b$-ensemble tend to ``repel'' each other.
The farther all the eigenvalues in $\L$ are from each other, the more likely we are to observe such $\L$.

Additionally, this also means there is \textbf{zero probability} of observing any pair of identical eigenvalues.
This is because $\d_\b(z,z') = 0$ if $z = z'$, meaning that if we observed two identical eigenvalues, then the probability density would be zero.
This effectively places a bound on the how close our eigenvalues can be to each other.

\bigskip

\blocktitle{The Red Term} The second big component, which we will call ``the red term'' is the term $\redterm$. This term is slightly simpler to understand.
Fundamentally, this term is simply an exponential term with an input that is a \textbf{negative value}.

To see this, first label the term $S = \sum_{i = 1}^N \l_i^2$. This simplifies the red term to become $\exp(-\frac{1}{2} S)$.
We know the term $S$ to be positive since the eigenvalues are \textbf{real} and that the sum of squares of several real values is strictly positive.
As such, the term, multiplied by a negative number ($-\frac{1}{2}$) is guranteed to be negative. So, to summarize, we know that $-\frac{1}{2} S$ is a negative value.
Then, from what we know about the exponential function, we know that:
$$\lim_{x \to -\infty} \exp(x) = 0$$
So, this means that as $S \to \infty$, $\exp(-\frac{1}{2}S) \to 0$. Since $S$ is simply the sum of magnitudes of the eigenvalues, this implies that the larger our eigenvalues
are, the smaller is our red term. In turn, this implies that the joint p.d.f. of eigenvalues is smaller overall.
To summarize, this means that our red term tells us that (relatively speaking) the larger the eigenvalues in $\L$, the less likely we are to observe those eigenvalues in the $\b$-ensemble. This effectively places a bound on the sizes of our eigenvalues.

\bigskip

\blocktitle{Takeaways} Altogether, here is what we can say about the $\beta$-ensemble joint eigenvalue p.d.f just from observing the terms.
  \begin{enumerate}
    %\item When $\l_i \in \L$ is large, then $\P(\L)$ is small.
    %\item When $\d(\l_i, \l_j) \for \l_i,\l_j \in \L$ is small, then $\P(\L)$ is small.
    \item When $\l_i$ is large, then $\P(\L)$ is small.
    \item When $\d(\l_i, \l_j)$ is small, then $\P(\L)$ is small.
  \end{enumerate}

%=========================================================================================
%=========================================================================================
\newpage
\subsection{The Invariance Criterion}

\minititle{The Three Musketeers (Associative Algebras)}

We know from Abstract Algebra that there are only three associative algebras over the real numbers $\R$ (i.e. algebras with a robust multiplication structure) [\cite{dummit}].
They are the following:
% \begin{enumerate}
%   \item The real numbers: $$\R = \{ x \in \R \}$$
%   \item The complex numbers: $$\C = \{ z = x + yi \mid x,y \in \R \}$$
%   \item The quaternionic numbers: $$\Hh = \{ \alpha = a + bi + cj + dk \mid a,b,c,d \in \R \}$$
% \end{enumerate}
\begin{enumerate}
  \item The real numbers, $\R$
  \item The complex numbers, $\C = \{ z = x + yi \mid x,y \in \R \}$
  \item The quaternionic numbers, $\Hh = \{ \alpha = a + bi + cj + dk \mid a,b,c,d \in \R \}$
\end{enumerate}

\minititle{Diagonalization \& Conjugation Invariance}

One of the most elegant results in Random Matrix Theory is the equivalence between the $\b$-ensembles
and their normal matrix counterparts over the three associative algebras. Without futher ado, consider the following.

\medskip

\blocktitle{Real} Suppose $M \sim \Normal(\mu,\sigma^2)^\dagger$ over $\R$ is a real symmetric matrix.
Then, its eigenvalues have a joint p.d.f. of $f_{\b = 1}(\L)$, meaning it repersents the $\beta = 1$ ensemble.

Additionally, we know that symmetric matrices are diagonalizable under conjugation with a orthogonal matrix.
In other words, we could express $M$ as $P^{-1} D P$ where $D$ is a diagonal matrix and $P \in O(n)$ is an orthogonal matrix.
For this reason, we call the $\b = 1$ ensemble the \textit{Gaussian Orthogonal Ensemble}, abbreviated as GOE.

\bigskip

\blocktitle{Complex} Suppose $M \sim \Normal(\mu,\sigma^2)^\dagger$ over $\C$ is a complex hermitian matrix.
Then, its eigenvalues have a joint p.d.f. of $f_{\b = 2}(\L)$, meaning it repersents the $\beta = 2$ ensemble.

Additionally, we know that hermitian matrices are diagonalizable under conjugation with a unitary matrix.
In other words, we could express $M$ as $P^{-1} D P$ where $D$ is a diagonal matrix and $P \in U(n)$ is an unitary matrix.
For this reason, we call the $\b = 1$ ensemble the \textit{Gaussian Unitary Ensemble}, abbreviated as GUE.

\bigskip

\blocktitle{Quaternionic} Suppose $M \sim \Normal(\mu,\sigma^2)^*$ over $\Hh$ is a quaternionic self-dual matrix.
Then, its eigenvalues have a joint p.d.f. of $f_{\b = 4}(\L)$, meaning it repersents the $\beta = 4$ ensemble.

Additionally, we know that self-dual quaternionic matrices are diagonalizable under conjugation with a sympleptic matrix.
In other words, we could express $M$ as $P^{-1} D P$ where $D$ is a diagonal matrix and $P \in Sp(n)$ is a sympleptic matrix.
For this reason, we call the $\b = 1$ ensemble the \textit{Gaussian Sympleptic Ensemble}, abbreviated as GSE.

\bigskip

All that being said, we can say that with respect to each matrix group, each $\b$-ensemble matrix has a feature called
\textbf{conjugation invariance}. What this entails is that any of the matrices can be characterized by two things: their eigenvalues, and an arbitrary group element (from the respective algebra's matrix group) which determines its basis.

\newpage
%=========================================================================================
\minititle{Dyson Index}

So, all that being said, why do we use the value $\b$ and where do the numbers come from? The answer is the Dyson index [\cite{tao}].
The Dyson index $\beta$ corresponds to the number of real number of components the subject matrices have. Refer to the definitions
of the associative algebras above, and you will find that for $\b = 1,2,4$, you have the representation of a matrix over a field with $\b$ real dimensions.


%=========================================================================================
%=========================================================================================
\newpage
\subsection{A Physical Interpretation}

Alongside these great algebraic properies, the $\b$-ensembles also have a intepretation as a physical model.
This is because as mentioned previously, these ensembles show up frequently in statistical physics.
So, we will cover one physical model that the $\b$-ensemble represents.

Suppose that $P \sim \H(\beta)$ is an $N \times N$ matrix. Then, the eigenvalues of $P$ have a representation as a model of charged point particles.

\minititle{Charged Particle Model}

Suppose $N$ particles are in a line about the origin. Additionally, suppose there is a quadratic field potential $V(x) = x^2$.
Then, the eigenvalues of $P$, $\spec(P)$ represents a stochastic system of such particles. As discussed previously in the breakdown of the density function,
these particles tend to repel each other. Specifically, their repulsion factor is $\beta$ itself! By observing the red term and blue term at extreme values of $\b$,
we can interpret the physical model as follows:
\medskip

\blocktitle{Low Replsuion, High Temperature} As $\b \to 0$, the temperature of the system $T \to \infty$.
At these values, the model starts to behave like an ideal gas.
Additionally, there is no interaction between the paricles anymore as the power of the blue term implies that the dispersion has no effect anymore. This returns a \textbf{fully stochastic} model of particles trying to align themselves along the field potential. In this model, the field potential matters a lot, as it determines the positioning of the particles.

\bigskip

\blocktitle{High Repulsion, Low Temperature} As $\b \to \infty$, the temperature of the system $T \to 0$.
At these values, the model loses its stochastic properties and starts to become \textbf{deterministic}.
Additionally, there is maximal interaction between the paricles as the power of the blue term starts to become more prominent. This returns a fully deterministic*
model of particles that align themselves equidistantly. In this model, the field potential doesn't matter anymore.

\medskip

\begin{remark}[Deterministic System]
While the second model is effectively deterministic, there does remain the randomness in the indices of the particles which are uniform.
\end{remark}

%=========================================================================================
%=========================================================================================
\newpage
%=========================================================================================
\subsection{The Matrix Model}

Now, we have throughly charecterized, interpreted, and explained the $\b$-ensemble.
How do we simulate such an ensemble given that we only know the joint p.d.f. of the eigenvalues?
Luckily, we do not need to dwelve deep into this. To simulate matrices from the $\beta$-ensemble, we will be using the result published in ``Matrix Models for Beta Ensembles'' by Dr. Ioana Dumitriu [\cite{dumitriu}].
This gives us a matrix model that generates a matrix that would be observed in a $\beta$-ensemble given any $\beta \in \N$.

\minititle{Dumitriu's Matrix Model of $\b$-Ensembles}

This result was actually described previously in \textbf{Section 1.1.1}! We first described the $\b$-matrices as a non-homogenous $\D$-distribution that we denoted $\D = \H(\b)$. This is a symmetric, tridiagonal matrix model with entries sampling from the normal and chi distributions. What we get in turn, is a matrix model whose eigenvalues \textbf{implicitly} have the joint p.d.f. of the Hermite-$\b$ ensemble described in \textbf{Section 4.1.1}. The algorithm used is directly cited from the results of Dumitriu's paper, and can be found below.

\ALGbeta

In turn, we can now easily simulate $\b$-ensembles using $\RMAT$ as such.

\begin{code}[Hermite Beta = 2 Ensemble]
Let $\mathcal{D} = \mathcal{H}(\beta = 2)$. We can generate $\Ens \sim \D$, an ensemble of $4 \times 4$ Hermite matrices ($\beta = 2$) of size 10 as such:
\end{code}

\begin{lstlisting}[language=R]
library(RMAT)
ensemble <- RME_beta(N = 4, beta = 2, size = 10)
# Outputs the following
ensemble
...
[[10]]
          [,1]      [,2]        [,3]     [,4]
[1,] 0.7246302 1.8893868  0.00000000 0.000000
[2,] 1.8893868 1.5278221  0.68840045 0.000000
[3,] 0.0000000 0.6884004 -0.03876104 1.944495
[4,] 0.0000000 0.0000000  1.94449533 1.042741
\end{lstlisting}
%=========================================================================================

\newpage
\section{Spectra}

In this section, we will continue survey the spectra of $\b$-ensembles. The reason we are continuing is because we have already started this work in \textbf{Section 2.3.3}!
To remind the reader, in that section, we studied the symmetric normal ensemble over $\R$. As we now know, this means that we have already studied the GOE $(\b = 1)$ ensemble.
However, before we proceed, here is an important regarding $\textbf{identifiability}$ to take into consideration when simulating these random eigenvalues.

\begin{remark}[Implicit Distribution]
Note that the  $\b$-ensemble is an ensemble characterized by its joint eigenvalue p.d.f.
As such, there are some subtle but important consideration to take into account regarding identifiability.
Recall that we formalized the spectrum of a matrix as a function which takes in as an input a vector of random variables.
So, while Dumitriu's model provides an explicit formula for the matrix entries, for the eigenvalues, there is the issue of identifiability.
That is, given some eigenvalues, there is no injective function to the characteristic polynomial that produced it.
Rather, there are infinitly many equivalence classes of characteristic polynomials (and such, random matrices) that surjectively produce a given multiset of eigenvalues.
\end{remark}

Without further ado, we will now take a look at the properties of the spectra of $\b$-ensembles.

%=========================================================================================

\subsection{Standard Ensembles}

As mentioned previously, we have already analyzed the $\b = 1$. As it turns out, all three ensembles share the properties listed in \textbf{Section 2.3.3}. Namely, we find that all three ensembles have a semicircle distribution (with potentially differing radii). Additionally, we also observe that the variance is higher towards the edges where the larger eigenvalues are. That being said, consider the spectra of the $\b = 2$ and $\b = 4$ ensembles.
\trim
\plotwrapperNC{h}{0.525}{../graphics/chap4/beta24_spec}

\newpage

The most notable difference between the two ensembles is that the peaks of the $\b = 4$ ensemble seem to be more ``dull'' than those of the $\b = 2$ ensemble. Recall from previous sections in this chapter, we state that the higher the value of $\b$, the more likely is the ensemble to have equidistant eigenvalues. As such, this aligns with that fact since less peaked densities for the values implies that there is more intersection between them at a given order. As such, this plot second-handedly demonstrates this phenomenon.

%=========================================================================================
\subsection{Extending the $\b$-Ensembles}

Now, we may consider extending the $\b$-ensembles and observe their spectra. In this section, we observe various $\b$-ensembles for various values of $\b$ in the first eleven powers of $2$. Additionally, we are using the sign-ordered scheme on the eigenvalues.

In the plot below, we observe the statistic $\E(\l_i \mid i)$. Immediately, we start to notice that the larger the value of $\b$, the larger the eigenvalues are (fixing $N$). This is consistent with what we expect about the limiting behaviour of the eigenvalues as $\b \to \infty$.
\trim
%**********************************************************
\plotwrapperNC{h}{0.37}{../graphics/chap4/betaspec_Re_mean}
%**********************************************************
\trim
In this plot, we now observe the statistic $\Var(\l_i \mid i)$. At a first glance, there doesn't seem to be any resounding patterns. There is roughly a tendency for the larger value of $\b$ to have a higher variance towards the center, but this may as well be random noise. This in turn shows that the variance $\Var(\l_i \mid i)$ is roughly uniform for any $\b \in \N$.

\plotwrapperNC{h}{0.37}{../graphics/chap4/betaspec_Re_var}

%=========================================================================================
\newpage
%=========================================================================================
%=========================================================================================
\section{Dispersions}
%=========================================================================================

In this section, we will take a look at some dispersion statistics of the $\b$-ensembles.

%=========================================================================================
\subsection{Order Statistics}
%=========================================================================================

Firstly, we will consider the dispersion $\disp_{\text{abs}}(\Ens \mid \Pi_<)$ where $\Ens \sim \H(\b = 4)$ is an ensemble of $100 \times 100$ matrices. Since we are using $\Pi_<$, we can say that we are interested in the difference of absolutes for an arbitrary pair of eigenvalues. Additionally, assume that we are using the norm-ordering scheme.

%FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF
\plotwrapperNC{h}{0.3}{../graphics/chap4/4-2_lm_beta_big}%{}
%FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF


In the plot above, we are considering the absolute differences dispersion metric $\d = \d_{\text{abs}}$. Additionally, we are using the lower pairing scheme $\Pi_C$ so we can interpet these pairs as the pairs we would obtain from randomly picking two distinct eigenvalues.
Additionally, since we are using the norm-ordering scheme, this means that the ranking difference tells us the difference of sizes of two eigenvalues that are ranked a certain distance $\rho$ apart.

As such, this explains why the bulk about $\rho = 50$ is the widest. This is because it represents the dispersion between pairs of eigenvalues in the origin and those near the boundary. Analogously, this explains why both $\rho = 0$ and $\rho = 100$ exhibit the narrowest dispersions. This is because they are comparing the largest eigenvalues with themselves (since the Semicircle distribution is symmetric). Lastly, note that this graph is symmetric about the identity line.

%=========================================================================================
\newpage
%=========================================================================================
\subsection{Wigner's Surmise}
%=========================================================================================

\minititle{Extending the $\b$-Ensembles}

Below, we extend the simulations of the Wigner dispersions as discussed in \textbf{Section 3.3} beyond the standard values of $\beta = 1,2,4$ to any $\b \in \N$.

%FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF
\plotwrapperNC{h}{0.6}{../graphics/chap3/3-3_wigner_beta_extended}%{Wigner's Surmise for Non-Standard Beta Ensembles}
%FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF

\minititle{Takeaways}

As mentioned in \textbf{Section 3.3.2}, we notice that there is a general pattern of decreasing variance, indicating that the pattern is consistent for other values of $\b$! In other words, this seems to provide evidence that as $\b \to \infty$, the variance of the consecutive dispersions shrinks, $\Var(\d(\pit)) \to 0$. Recall from the physical interpretation of the model in \textbf{Section 4.1.3}, we discuss the limiting behaviour of the ensemble as $\beta \to \infty$. Specifically, it was mentioned that as $\beta$ grows, the ensemble represents a deterministic system of particles of low temperature and maximum interaction which tries to align itself equidistantly. This would confirm our prediction, because an equidistant system of particles is precisely a system of point charges where there is no variance in the distance between consecutive particles, or in other words --- where $\Var(\d(\pit)) = 0$!
