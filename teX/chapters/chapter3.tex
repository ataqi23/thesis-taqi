
\chapter{Dispersions}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

In this section, we define the final spectral statistic studied in this chapter: eigenvalue dispersions. As the name suggests, these statistics are concerned with the distirbution of the spacings between the eigenvalues. Interestingly, this is almost as literal as it gets when we use the word "spectral". In physics and chemistry, atomic spectra are essentially differences between energy levels or quanta, so the translation is close. 

In any case, we will begin this chapter by first motivating a few definitions and formalisms in this section. Then, once our setup is ready, we will motivate the definition of a matrix's eigenvalue dispersion and formalize it as a statistic. To outline the section, we will first define two things: the dispersion metric and the pairing scheme. In simple terms, we formalize $\textbf{what}$ ``eigenvalue spacings'' are and $\textbf{which}$ eigenvalue pairs' spacings to consider. Without further ado, we motivate the dispersion metric.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Dispersion Metrics}

% When we define the dispersion of a matrix, we will see that there is a free argument $d$ called the dispersion metric. This function $d$ is a general function whose domain is always two eigenvalues. In set notation, this is the set $\Cc \times \Cc$ - a pair of two complex numbers. Its range will often be the positive reals $\R^+$; this is because the dispersion metric often will be substitutable with distance metric. Sometimes, the range will be $\Cc$. So, the dispersion metric will take the following form:
Before we may even start to consider studying dispersions of eigenvalues, we must first formalize and make clear what ``metric'' of spacing we are using. To do so, we motivate the dispersion metric.

\begin{definition}[Dispersion Metric]
A dispersion metric $\delta: \Cc \times \Cc \to \R^+$ is defined as a function from the space of pairs of complex numbers to the positive reals. In simple terms, it is a way of measuring "space" between two complex numbers - our eigenvalues.
\end{definition}

Consider the following dispersion metrics below.

\begin{enumerate}
\item The standard norm: $\d(z,z') = |z' - z|$
\item The $\beta$-norm: $\d_\beta(z,z') = |z' - z|^\beta$
\item The difference of absolutes: $\d_{\text{abs}}(z,z') = |z'| - |z|$
\end{enumerate}

\begin{remark}[Symmetric Metrics]
Note that the standard norm is a $\textit{symmetric}$ operation. The $\beta$-norm can also be a symmetric operation when $\beta$ is even. Otherwise, the difference of absolutes metric is not symmetric.
\end{remark}

While we have defined disperison metrics to be functions from $\Cc^2$, there is one special case where we can make an exception so that the domain of $\delta$ is not $\R^+$. 

\begin{remark}[Identity Difference Heuristic]
Suppose we take the arithmetic difference of two complex numbers. Then, the range of $\delta$ is $\Cc$. For this reason, we won't consider the arithmetic difference a formal dispersion metric, but we will honor it as a dispersion huerestic. As such, we will denote this as the ``identity difference'' huerestic and call it $\delta_{\text{id}}$. So, we define $\delta_{\text{id}}: (z, z') \mapsto z' - z$
\end{remark}

\begin{center}
$\textbf{Common Dispersion Metrics}$
\end{center}

For the formula, assume the order of arguments is $\d(z, z')$. \newline

\begin{tabular}{ |p{4.5cm}||p{2cm}|p{2cm}|p{2cm}|p{2cm}|  }
 \hline
 \multicolumn{5}{|c|}{Table of Dispersion Metrics} \\
 \hline
 Metric* & Notation & Formula & Symmetric & Parameters\\
 \hline
 Standard Norm & $\d$ & $|z' - z|$ &  True & - \\
 $\beta$-Norm & $\d_\b$ & $|z' - z|^\b$ & $\b$ Even & $\b \in \N$ \\
 Difference of Absolutes & $\d_{\text{abs}}$ & $|z'| - |z|$ &  False  &  - \\
 Identity Difference &  $\d_{\text{id}}$ & $z' - z$ & False &  - \\
 %Angola & AO & AGO & 024\\
 \hline
\end{tabular}

\vspace{1em}

*Note that the identity difference is a heurestic, and not a formal metric.

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Pairing Schema}

% Next up, we introduce a new notation for a pairing scheme denoted $\Pi$. What are pairing schemes and why do they matter? Recall that our goal is to study the spacings between eigenvalues. If we are studying spacing, then a priori, we are concerned with pairs of eigenvalues! Spacing, after all, is a binary relationship. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% You are a studious student, earnest to learn about eigenvalue spacings, for some reason. So you begin, and the first question pops into your head.
% 
% $\textbf{Student:}$ For which eigenvalue pairs will we observe their dispersion? \hfill \newline
% $\textbf{Wigner's Ghost:}$ The ones specified by the pairing scheme.
% 
% Unphased by the fact you are meeting a ghost from the 1990s - a wild spirit you are - you find it a good idea to interrogate the ghost physicist.
% 
% The student, irritated about having to learn yet another snippet of loaded notation complains and asks:
% 
% $\textbf{Student:}$ Aha! Why don't we just look at all the eigenvalue pairs? Isn't more information always better? \hfill \newline
% $\textbf{Wigner's Ghost:}$ Actually, no. We will later find out a few reason taking all pairs isn't best. For instance, some spacings are linear combination of others. Or if we have a dispersion metric that is symmetric, half of the information is going to be repeated! 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% So, with the definitions of spectra well motiviated, a natural definition of pairing schema follows. Essentially, $\Pi$ is just a subset of the Cartesian product of a spectrum with itself. In other words, if we denote $\S := \sigma(P)$, then we say that a pairing scheme is simply a subset $\Pi \subseteq \S^2$. Here are some pairing schema that we will consider:

The next thing we need to motivate before talking about eigenvalue dispersions are pairing schema. In simple terms, pairing schema are templates (for some $N \in \N$) for which eigenvalue pairs to pick. There are many subtle reasons why this is important, which will be covered in detail later. Without further ado, we motivate the pairing schema. 

\begin{definition}[Pairing Scheme]
Suppose $P$ is any $N \times N$ matrix and $\sigma(P)$ is its corrosponding ordered spectrum. A pairing scheme for the matrix $P$ is simply a subset of $\N_N \times \N_N$ - a subset of pairs of numbers from $\N_N = \{1, \dots, N\}$. In other words, it is a subset of pair indices for $N$ objects - in our case, eigenvalues. We denote a pairing scheme as some set $\Pi = \{(\alpha, \beta) \mid \alpha, \beta \in \N_N\} \subseteq \N_N \times \N_N$. To take the matrix's corrosponding eigenvalue pairs with respect to $\Pi$, we simply take the set of eigenvalue pairs with the corrosponding indices, $\tilde{\sigma}(P \mid \Pi) = \{(\lambda_{\alpha},\lambda_{\beta}) \mid ({\alpha},{\beta}) \in \Pi\}$.
\end{definition}

The reader might find this definition slightly obscure, and rightfully so. The definition is a mere formality, as we will usually only consider a few specific pairing schema. Seeing the explicit examples will hopefully make things more clear. With all the technical details aside, we can just say that a pairing scheme tells us which subset of eigenvalue pairs to consider. If one visualizes an array with $N$ objects on two axes, we are simply choosing a subset of that plane. In fact, consider the following.

\begin{remark}[Spectrum Subset] If we denote $\mathbb{S} = \sigma(P)$ as the $\textbf{ordered}$ spectrum of some matrix $P$, then for any valid pairing scheme $\Pi$, we will find that $\tilde{\sigma}(P \mid \Pi) \subseteq \mathbb{S}^2$ by definition. Note the emphasis on ordered - otherwise, the indices are meaningless.
\end{remark}

%Consider the following common pairing schema below.

\newpage

\begin{center}
$\textbf{Common Pairing Schema}$
\end{center}

Suppose $P$ in an $N \times N$ square matrix, and $\sigma(P)$ is its ordered spectrum.

\begin{enumerate}
  \item The unique pair combinations schema are two complementary pair schema. By specifying $\textbf{either}$ $i > j$ or $i < j$, we characterize this scheme to entail all unique pair combinations of eigenvalue without repeats. The reason we call them upper and lower pair combinations is an allusion to the indices of the upper and lower triangular matrices. \begin{enumerate}
    \item Let $\Pi_>$ be the lower-pair combinations of ordered eigenvalues. This will be the standard unique pair combination scheme used in lieu of the argument orders of our dispersion metrics (more later). In this pairing scheme, the eigenvalue with the lower rank is always listed first, and the higher rank second.
    $$\sigma(P \mid \Pi_>) = \{\pi_{ij} = (\lambda_i,\lambda_j) \mid i > j\}_{i = 1}^{N-1}$$
    \item For completeness, we will also define the upper-pair scheme. $\Pi_<$ is the set of upper-pair unique combinations of ordered eigenvalues. Wont be used because we want bigger - smaller to make positive definite.
    $$\sigma(P \mid \Pi_<) = \{\pi_{ij} = (\lambda_i,\lambda_j) \mid i < j\}_{i = 1}^{N-1}$$
  \end{enumerate} 
  \textbf{Benefits:} Solves the issue of repeated pairs for symmetric dispersion metrics.  
\item Let $\Pi_C$ be the consecutive pairs of eigenvalues in a spectrum. 
  $$\sigma(P \mid \Pi_C) = \{\pi_{j} = (\lambda_{j + 1},\lambda_j)\}_{j = 1}^{N-1}$$
  \textbf{Benefits:} This pairing scheme gives us the minimal information needed to express important bounds and spacings in terms of its elements. 
  \item Let $\Pi_0$ be all the pairs in a spectrum. For completeness, we define this pairing scheme as a standard.
  $$\sigma(P \mid \Pi_0) = \{\pi_{ij} = (\lambda_{i},\lambda_j) \mid i,j \in \N_N\}$$
% \item Let $\Pi_C$ be the consecutive pairs of eigenvalues in a spectrum. This pairing scheme gives us the minimal information needed to express important bounds and spacings in terms of its elements.
% $$\Pi_C = \{\pi_{ij} = (\lambda_i,\lambda_j) \mid i = j + 1\}_{i = 1}^{N-1}$$
\end{enumerate}

\begin{remark}[Order Statistics]
Since the pairing scheme assumes the eigenvalues are given in an ordered spectrum, our analysis will be mostly leveraging this fact by using the indices. However, as notated above, some pairing schemes are indexed with $i$ and $j$, and some like the consecutive pairs only with $j$. These indices are interchangable with ``order statistic'', so in the next section, we will devise a synthetic order statistic that uses two indices. 
\end{remark}

So, if a pair $\pi_j$ has only one index $j$, the pair will be a consecutive pair denoted by $\pi_j = (\lambda_{j+1}, \lambda_j)$. Otherwise, $\pi_{ij}$ will be given by $(\lambda_{i}, \lambda_j)$. The ranking difference class will group pairs into equivalence classes given by an integer $\rho = i - j$.

\begin{center}
$\textbf{Special pairs}$
\end{center}

There are a few pairs that are so special, it may be worth highlighting them explicity.

\begin{enumerate}
  \item Let $\pi_1$ be the pair of the two largest eigenvalues in an ordered spectrum. 
  $$\pi_1 = \{(\lambda_2,\lambda_1)\}$$
\end{enumerate}

Another way we can define the pairing scheme is defining an auxilliary object: the eigenvalue (pair) matrix.

\begin{definition}[Eigenvalue Matrix]
Suppose $P$ is an $N \times N$ square matrix. Then, the eigenvalue matrix of $P$, given by $\Lambda(P)$ is the matrix with entries $\pi_{ij} = (\lambda_i, \lambda_j)$. It is assumed that the eigenvalues, $\lambda_i$ come from some $\textbf{ordered}$ spectrum $\sigma(P)$.
\end{definition}

\begin{center}
$\textbf{Common Pairing Schema}$
\end{center}

Suppose $P$ is an $N \times N$ matrix. \newline

\begin{tabular}{ |p{2.5cm}||p{1.75cm}|p{5.5cm}|  }
 \hline
 \multicolumn{3}{|c|}{Table of Pairing Schema} \\
 \hline
 Scheme & Notation & Formula \\
 \hline
 Lower & $\Pi_<$ & $\{(i,j) \mid i < j \for i,j \in \N_N \}$ \\
 Upper  & $\Pi_>$ & $\{(i,j) \mid i > j \for i,j \in \N_N \}$ \\
 Consecutive  & $\Pi_C$ & $\{(i,j) \mid i = j + 1 \for i,j \in \N_N \}$ \\
 All & $\Pi_0$ & $\{(i,j) \mid i,j \in \N_N \}$ \\
 \hline
\end{tabular}

\newpage

\begin{center}
$\textbf{Pairing Scheme Diagrams for a $5 \times 5$ Matrix}$
\end{center}

All Pairs

$$
\begin{bmatrix}
O & O & O & O & O\\
O & O & O & O & O\\
O & O & O & O & O\\
O & O & O & O & O\\
O & O & O & O & O\\
\end{bmatrix}
$$

The Lower Pair Combinations

$$
\begin{bmatrix}
- & - & - & - & -\\
O & - & - & - & -\\
O & O & - & - & -\\
O & O & O & - & -\\
O & O & O & O & -\\
\end{bmatrix}
$$

The Upper Pair Combinations

$$
\begin{bmatrix}
- & O & O & O & O\\
- & - & O & O & O\\
- & - & - & O & O\\
- & - & - & - & O\\
- & - & - & - & -\\
\end{bmatrix}
$$

The Consecutive Pairs 

$$
\begin{bmatrix}
- & - & - & - & -\\
O & - & - & - & -\\
- & O & - & - & -\\
- & - & O & - & -\\
- & - & - & O & -\\
\end{bmatrix}
$$

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Dispersions}


Finally, we are able to motivate the definition of a matrix dispersion! Suppose we have a $\mathcal{D}$-distributed random matrix $P \in \F^{N \times N}$ or a random matrix ensemble $\Ens = \{P_i \sim \mathcal{D} \mid P_i \in \F^{N \times N}\}$. Then we define their dispersion as follows. 

\begin{definition}[Dispersion]
The dispersion of a matrix $P \in \F^{N \times N}$ with respect to a dispersion metric $d: \Cc \times \Cc \to \F$ and pairing scheme $\Pi$, call it $\Delta_d(P, \Pi)$, is defined as follows. Suppose $\sigma(P) := \S$ is the ordered spectrum of $P$ where $\sigma(P) = \{\lambda_1, \dots, \lambda_N\}$. Then, let $\Pi = \{\pi_{ij} = (\lambda_i,\lambda_j) \} \subseteq \S^2$ be a subset of eigenvalue ordered pairs. Then, the dispersion of $P$ with respect to $d$ is simply the set $\Delta_d(P, \Pi)=\{\delta_{ij} = d(\pi_{ij}) \mid \pi_{ij} = (\lambda_i,\lambda_j) \in \Pi\}$.
\end{definition}

As we usually do, we define the dispersion of an ensemble in a similar fashion. 

\begin{definition}[Ensemble Dispersion]
If we have an ensemble $\Ens$, then we can naturally extend the definition of $\Delta_d(\Ens, \Pi)$. To take the dispersion of an ensemble, simply take the union of the dispersions of each of its matrices. In other words, if $\Ens = \{P_i \sim \mathcal{D} \mid P_i \in \F^{N \times N}\}_{i = 1}^K$, then $\Delta_d(\Ens, \Pi) = \bigcup_{i=1}^K \Delta_d(P_i, \Pi)$.
\end{definition}

With our spectral statistics defined, we are prepared to discuss prominent results in Random Matrix Theory alongside our new findings from the simulations. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Order Statistics}
\subsection{Introduction}
With eigenvalue dispersions and eigenvalue orderings well-defined, we may proceed to start talking about their order statistics. 

In addition to these simple order statistics, we introduce a new variant statistic called the $\textbf{ranking difference class}$. Instead of observing a single eigenvalue at a given rank, we will now observe a pair of eigenvalues at a time. To standardize the process, we introduce a new eqivalence class called the \textit{ranking difference}. As suggested, it is precisely the integer difference of the eigenvalue ranks.

\begin{definition}[Ranking Difference]
The ranking difference is a function $\delta: \N \times \N \to \N$ which takes the index of two eigenvalues (from an \textit{ordered} spectrum) and returns their difference. In other words, $\delta : (\lambda_i,\lambda_j) \mapsto (i - j)$.
\end{definition}

With the function $\delta$, we may take the set of unique eigenvalue pairs $(i > j)$ and partition it into equivalence classes. To do so, we define the equivalence relation $\sim_\delta$ which says $(\lambda_a,\lambda_b) \sim_\delta (\lambda_c,\lambda_d) \iff (a -b) = (c-d)$. These equivalence classes then naturally corrospond to pairs a set distance $\rho = i - j$ apart. So, for an $N \times N$ matrix, $\delta$ assumes a range $\rho \in \{ 1,\dots,N-1\}$.

In summary, $\sim_\delta$ takes the set $\{(\lambda_i, \lambda_j) \mid \lambda_i, \lambda_j \in \sigma(P) \text{ and } i > j \}$ and surjectively partitions it onto the equivalence classes $[(\lambda_i, \lambda_j)]_\rho$ for $\rho \in \{ 1,\dots,N-1\}$. For example, if we consider $\rho = 1$, then we are considering all pairs of eigenvalue neighbors.

Note that the sizes of each equivalence class are $\textbf{never equal}$. With this partition in mind, we can consider various statistics conditioning on the value of $\rho$. Conditioning on $\rho$ will be especially useful in the cases where we are considering matrices like the Hermite-$\beta$ matrices; the eigenvalues of those matrices tend to $\textit{repel}$, so to speak, and we can observe these patterns using $\rho$.

\subsection{Conditional Statistics}

We will considering the conditional statistics $\E(\delta_{ij} \mid \rho)$ and $\Var(\delta_{ij} \mid \rho)$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Analytical Results}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Case Study: Wigmer's Surmise}

Limiting distribution of the eigenvalue spacings of symmetric matrices.

[Plot]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Findings}
