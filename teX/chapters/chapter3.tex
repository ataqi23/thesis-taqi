
\chapter{Dispersions}

%=========================================================================================
\epigraph{Distance doesn't exist, in fact, and neither does time. Vibrations from love or music can be felt everywhere, at all times.}{\textit{Yoko Ono}}
%=========================================================================================

\section{Introduction}

In this section, we define the final spectral statistic studied in this chapter: eigenvalue dispersions.
As the name suggests, these statistics are concerned with the distirbution of the spacings between the eigenvalues.
Interestingly, this is almost as literal as it gets when we use the word ``spectral''.
In physics and chemistry, atomic spectra are essentially differences between energy levels or quanta, so the translation is close.

In any case, we will begin this chapter by first motivating a few definitions and formalisms in this section.
Then, once our setup is ready, we will motivate the definition of a matrix's eigenvalue dispersion and formalize it as a statistic.
To outline the section, we will first define two things: the dispersion metric and the pairing scheme.
In simple terms, we formalize $\textbf{what}$ ``eigenvalue spacings'' are and $\textbf{which}$ eigenvalue pairs' spacings to consider.
So, to begin, we first formally define an eigenvalue pair.

\begin{definition}[Eigenvalue Pair]
  Suppose $P$ is a matrix and $\spec(P)$ is its ordered spectrum. Then, an eigenvalue pair with respect to this ordered spectrum is denoted $\piij$.
  It is defined as the ordered pair $\piij = (\lambda_i, \lambda_j)$.
\end{definition}

%****************************************************************************************%
\minititle{Consecutive Pairs}
%****************************************************************************************%

In the future section where we define pairing schemes, we will introduce new notation of eigenvalue pair with one index is introduced when defining the consecutive pairing scheme $\Pi_C$, and it takes the form $\pit_j$.
This notation is used to denote the consecutive eigenvalue pair for a given matrix.
The consecutive eigenvalue pairs are so special that we denote them with a unique notation for convenience. It also makes the discussion regarding order statistics more intrinisic.

\begin{definition}[Consecutive Pairs]
Suppose $P$ is a matrix and $\spec(P)$ is its ordered spectrum. Then, let $\pit_j$ denote a pair of consecutive eigenvalues, the largest of the two being the $j^{th}$ largest eigenvalue.
So, $\pit_j = (\lambda_{j-1},\lambda_{j})$.
\end{definition}

Since the consecutive pair scheme can be sufficiently indexed by one index ($j$), we will use the convention of omitting the index of the smaller eigenvalue to be more concise and idiomatic.
So, whenever one sees the notation $\pit_j$, think the $j^{th}$ largest eigenvalue and its smaller neighbour.

\begin{example}[The Largest Eigenvalues]
With this notation at hand, we say that $\pit_1$ represents the pair of the two largest eigenvalues in an ordered spectrum.
$$\pit_1 = (\lambda_2,\lambda_1)$$
\end{example}

\noindent Without further ado, we now motivate the dispersion metric.

%=========================================================================================

\subsection{Dispersion Metrics}

Before we may even start to consider studying dispersions of eigenvalues, we must first formalize and make clear what ``metric'' of spacing we are using. To do so, we motivate the dispersion metric.
In simple words, a dispersion metric is a function that takes in a pair of eigenvalues and returns a positive real number that represents some notion of dispersion or spacing.

\begin{definition}[Dispersion Metric]
A dispersion metric $\delta: \Cc \times \Cc \to \R^+$ is defined as a function from the space of pairs of complex numbers to the positive reals.
In simple terms, it is a way of measuring ``space'' between two complex numbers - our eigenvalues.
\end{definition}

%PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP
%\newpage
%PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP

In the scope of this thesis, we consider the following dispersion metrics below.

\begin{enumerate}
\item The standard norm: $\d_{n}(z,z') = |z' - z|$
\item The $\beta$-norm: $\d_\beta(z,z') = |z' - z|^\beta$
\item The difference of absolutes: $\d_{\text{abs}}(z,z') = |z'| - |z|$
\end{enumerate}

\begin{remark}[Symmetric Metrics]
Note that the standard norm and the $\b$-norm are $\textit{symmetric}$ operations, compared to the difference of absolutes metric, which is not.
For a metric to be symmetric means that switching the order of arguments with respect to that metric will have no effect on the function's output.
So, a metric $\d^*$ is symmetric if it satisfies:
$$\d^*(\piij) = \d^*(\pi_{ji})$$
\end{remark}

While we have defined disperison metrics to be functions from $\Cc^2$, there is one special case where we can make an exception so that the domain of $\delta$ is not $\R^+$.

\begin{remark}[Identity Difference Heuristic]
Suppose we take the arithmetic difference of two complex numbers. Then, the range of $\delta$ is $\Cc$.
For this reason, we won't consider the arithmetic difference a formal dispersion metric, but we will honor it as a dispersion huerestic.
As such, we will denote this as the ``identity difference'' huerestic and call it $\delta_{\text{id}}$. So, we define $\delta_{\text{id}}: (z, z') \mapsto z' - z$
\end{remark}

\minititle{Summary Table of Dispersion Metrics}
For every dispersion metric, assume the functions' order of arguments is $\d(z, z')$. \newline
%\hfill \newline
\begin{center}
\dispersiontable %\hfill
\end{center}
\vspace{1em}

\noindent *Note that the identity difference is a heurestic, and not a formal metric.

%=========================================================================================
%\newpage
\subsection{Pairing Schema}

The next thing we need to motivate before talking about eigenvalue dispersions are pairing schema.
In simple terms, pairing schema are templates (for some $N \in \N$) for which eigenvalue pairs to pick.
There are many subtle reasons why this is important, which will be covered in detail later.
Without further ado, we motivate the pairing schema.

Before we may begin talking about pairing scheme, we define an auxilliary object, the spectral pairs of a matrix, which we denote $\spair$.
Since we are now talking about eigenvalue pairs, it is helpful to define this object before proceeding.

\begin{definition}[Spectral Pairs]
Suppose $P$ is an $N \times N$ random matrix. Then, taking the spectral pair of $P$, denoted $\spair(P)$ is equivalent to taking the Cartesian product of its ordered spectrum $\sigma(P)$.
That is, $\spair(P) = \sigma(P) \times \sigma(P) = \{\piij = (\lambda_i, \lambda_j) \mid i,j \in \N_N\}$.
\end{definition}

Now, to select eigenvalue pairs, we motivate the pairing scheme - this is what tells us which indices to select.

\begin{definition}[Pairing Scheme]
Suppose $P$ is any $N \times N$ matrix and $\spair(P)$ are its spectral pairs.
A pairing scheme for the matrix $P$ is a subset of $\N_N \times \N_N$ - a subset of pairs of numbers from $\N_N = \{1, \dots, N\}$.
In other words, it is a subset of pair indices for $N$ objects - in our case, eigenvalues.
We denote a pairing scheme as a set $\Pi = \{(\alpha, \beta) \mid \alpha, \beta \in \N_N\} \subseteq \N_N \times \N_N$.
To take a matrix's spectral pairs with respect to $\Pi$, we simply take the set of eigenvalue pairs with the matching indices,
$\spair(P \mid \Pi) = \{(\lambda_{\alpha},\lambda_{\beta}) \mid ({\alpha},{\beta}) \in \Pi\}$.
\end{definition}

The reader might find this definition slightly obscure, and rightfully so.
The definition is a mere formality, as we will usually only consider a few specific pairing schema.
Seeing the explicit examples will hopefully make things more clear.
With all the technical details aside, we can just say that a pairing scheme tells us which subset of eigenvalue pairs to consider.
If one visualizes an array with $N$ objects on two axes, we are simply choosing a subset of that plane.
In fact, consider the following.

\begin{remark}[Proper Subset] If $\spair(P)$ is the spectral pairs of the matrix $P$, then for any pairing scheme $\Pi$, we will find that $\spair(P \mid \Pi) \subseteq \spair(P)$.
By definition, taking a spectral pair with respect to some pairing scheme constricts which pairs to select - meaning it is a proper subset of the general spectral pairs.
\end{remark}

%****************************************************************************************%
%\newpage
\minititle{Common Pairing Schema}
%****************************************************************************************%

\noindent Suppose $P$ in an $N \times N$ square matrix, and $\spair(P)$ are its spectral pairs. For every pairing scheme, assume $i,j \in \N_N$.

\begin{enumerate}
  \item Let $\Pi_C$ be the consecutive pairs of eigenvalues in a spectrum.
    $$\spair(P \mid \Pi_C) = \{\pit_{j} = (\lambda_{j + 1},\lambda_j)\}_{j = 1}^{N-1}$$
    \textbf{Benefits:} This pairing scheme gives us the minimal information needed to express important bounds and spacings in terms of its elements.
  \item The unique pair combinations schema are two complementary pair schema. By specifying $\textbf{either}$ $i > j$ or $i < j$,
    we characterize this scheme to entail all unique pair combinations of eigenvalues without repeats.
    The reason we call them upper and lower pair combinations is an allusion to the indices of the upper and lower triangular matrices. \begin{enumerate}
    \item Let $\Pi_>$ be the lower-pair combinations of ordered eigenvalues.
      This will be the standard unique pair combination scheme used in lieu of the argument orders of our dispersion metrics (more later).
      In this pairing scheme, the eigenvalue with the lower rank is always listed first, and the higher rank second.
    $$\spair(P \mid \Pi_>) = \{\pi_{ij} = (\lambda_i,\lambda_j) \mid i > j\}_{i = 1}^{N-1}$$
    \item For completeness, we will also define the upper-pair scheme. $\Pi_<$ is the set of upper-pair unique combinations of ordered eigenvalues.
      Wont be used because we want bigger - smaller to make positive definite.
    $$\spair(P \mid \Pi_<) = \{\pi_{ij} = (\lambda_i,\lambda_j) \mid i < j\}_{i = 1}^{N-1}$$
    \end{enumerate}
    \textbf{Benefits:} Solves the issue of repeated pairs for symmetric dispersion metrics.
  \item Let $\Pi_0$ be all the pairs in a spectrum. For completeness, we define this pairing scheme as the implicit pairing scheme for the spectral pairs of a matrix.
    $$\spair(P \mid \Pi_0) = \spair(P) = \{\pi_{ij} = (\lambda_{i},\lambda_j) \mid i,j \in \N_N\}$$
\end{enumerate}
%****************************************************************************************%

%PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP
%\newpage
%PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP

\noindent Consider some examples of generating pairing schemes in R. These methods are
not exported in the $\RMAT$ package.

\begin{code}[Consecutive Pairing Scheme]
  We generate the pairing scheme (indices) for what would be a $5 \times 5$ matrix.
\end{code}
\begin{lstlisting}[language=R]
# Helper function in the source code
pair_indices <- .consecutive_pairs(N = 5)
# Outputs the following
pair_indices
...
     i j
[1,] 2 1
[2,] 3 2
[3,] 4 3
[4,] 5 4
\end{lstlisting}

\begin{code}[Lower Pairing Scheme]
  We generate the pairing scheme (indices) for what would be a $4 \times 4$ matrix.
\end{code}
\begin{lstlisting}[language=R]
# Helper function in the source code
pair_indices <- .unique_pairs_lower(N = 4)
# Outputs the following
pair_indices
...
     i j
[1,] 2 1
[2,] 3 1
[3,] 3 2
[4,] 4 1
[5,] 4 2
[6,] 4 3
\end{lstlisting}

Think of the pairing schemes as a conditional argument when considering dispersions. You ask yourself: which pairs do I want to look at?
There will be a discussion in \textbf{Section 3.2} about why we have to formalize the notions of dispersion metrics and pairing schemes when we eventually talk about dispersions.

Additionally, making the pairing schemes independent from the matrices (taking only their dimension, $N \in \N$) was a difficult decision.
It may cloud understanding initally but the payoff comes if you consider the implementation of the functions in \RMAT.
As a whole, you should not be bothered by the details of formalizing the pairing schemes but you should definitely understand what they do.
In fact, it's totally fair that you do. The \RMAT \; implementation abstracts this all away in the dispersion function.
The user would input a string denoting which pairing scheme and the function would handle it.
Similarly, you should do the same!

%****************************************************************************************%
%****************************************************************************************%

%****************************************************************************************%
\newpage
\minititle{Summary Table of Pairing Schema}
%****************************************************************************************%
Suppose $P$ is an $N \times N$ matrix. Then, its spectral pairs $\spair(P)$ may take on the following pairing schemes. Assume that $i,j \in \N_N$. \newline
%\hfill \newline
\begin{center}
\pairingschemetableNOIJ %\hfill
\end{center}
\vspace{1em}

\noindent Another way we can define the pairing scheme is defining an auxilliary object: the eigenvalue (pair) matrix.

%****************************************************************************************%
\medskip
\minititle{An Alternative Represetation: Eigenvalue Matrix}
%****************************************************************************************%
\begin{definition}[Eigenvalue Matrix]
Suppose $P$ is an $N \times N$ square matrix and $\spair(P)$ are its spectral pairs.
Then, the eigenvalue matrix of $P$, given by $\Lambda(P)$ is the matrix with entries $\pi_{ij} = (\lambda_i, \lambda_j)$ for $\piij \in \spair(P)$.
Again, it is given that the eigenvalues $\lambda_i$ come from some $\textbf{ordered}$ spectrum $\sigma(P)$ as per the definition of spectral pairs.
\end{definition}

With the matrix analogy, describing the pairing schemes can be much simpler.
For instance, the $\textbf{lower pairs}$ are given by the indices of the entries in the lower triangle of the matrix.
Analogously, the $\textbf{upper pairs}$ by those of the upper triangle of the matrix.
The $\textbf{consecutive pairs}$, given our default lower pair scheme, are given by the lower main off-diagonal band of the matrix.

\begin{example}[Eigenvalue Matrix for a $5 \times 5$ Matrix]
Suppose we have a $5 \times 5$ matrix $P$ and its corresponding spectral pairs $\spair(P)$. Then, its eigenvalue matrix has the following strucutre:
\end{example}
$$
\begin{bmatrix}
-      & \pi_{12} & \pi_{13}   & \pi_{14} & \pi_{15} \\
\pit_{1} & -      & \pi_{23}   & \pi_{24} & \pi_{25} \\
\pi_{31} & \pit_{2} & -        & \pi_{34} & \pi_{35} \\
\pi_{41} & \pi_{42} & \pit_{3} & -        & \pi_{45} \\
\pi_{51} & \pi_{52} & \pi_{53} & \pit_{4} & -        \\
\end{bmatrix}
$$
\newline

\begin{remark}[Main Diagonal]
Note that in the matrix example above, we omit the entries in the main diagonal. We do so because those pairs given by $\{\pi_{ii} \mid i \in \N_N\}$
are redundant to include. We are only interested comparing eigenvalues with other eigenvalues. This is because every the dispersion of eigenvalue with itself is zero,
meaning $\forall \d : \d(\pi_{ii}) = 0$. So, as a convention, we omit these pairs.
\end{remark}

%=========================================================================================
\newpage
\subsection{Dispersions}

Now, with dispersion metrics and pairing schemes defined, we are finally able to motivate the definition of a matrix dispersion for both singleton matrices and their ensemble counterparts.
In simple words, we define a dispersion of a matrix as a function that takes in a matrix along with a pairing scheme and dispersion metric.
Then, it outputs the mapping of that dispersion metric onto the subset of spectral pairs specified by the pairing scheme.
To formalize this, consider the following definition of a matrix dispersion:

\begin{definition}[Dispersion]
Suppose $P$ is an $N \times N$ matrix, and $\spair(P)$ are its spectral pairs.
The dispersion of $P$ with respect to the pairing scheme $\Pi$ and dispersion metric $\delta_M$ is denoted by $\disp_M(P \mid \Pi)$ and it is given by the following:
$$\disp_M(P \mid \Pi) = \{\d_M(\piij) \mid \piij \in \spair(P \mid \Pi)\}$$
\end{definition}

Consider the following code example, generating the dispersion of standard normal $5 \times 5$ matrix with respect to the consecutive pairing scheme.

\begin{code}[Consecutive Pair Dispersion of a Standard Normal Matrix]
In our notation, we are simulating the disperison of $P \sim \Normal(0,1) \where P \in \R^{5 \times 5}$.
Specifically, we are simulating $\disp(P \mid \Pi_C)$. In the code implementation, we obtain an array for every dispersion metric $\d$.
\end{code}
\begin{lstlisting}[language=R]
library(RMAT)
P <- RM_norm(N = 5, mean = 0, sd = 1)
disp_P <- dispersion(P, pairs = "consecutive")
# Outputs the following
disp_P
...
i  j  eig_i       eig_j       id_diff    iddiff_norm abs_diff rank_diff
2  1 -0.54-1.35i -0.54+1.35i  0.00+2.71i 2.71         0.00    1
3  2  0.23+1.43i -0.54-1.35i -0.77-2.78i 2.88         0.02    1
4  3  0.23-1.43i  0.23+1.43i  0.00+2.85i 2.85         0.00    1
5  4 -0.87+0.00i  0.23-1.43i  1.09-1.43i 1.80         0.57    1
\end{lstlisting}

Next, we extend the definition of dispersion for an ensemble as we usually do.

\begin{definition}[Ensemble Dispersion]
If we have an ensemble $\Ens$, then we can naturally extend the definition of $\Delta_M(\Ens \mid \Pi)$.
To take the dispersion of an ensemble, simply take the union of the dispersions of each of its matrices.
In other words, if $\Ens = \{P_i \sim \mathcal{D}\}_{i = 1}^K$, then its dispersion is given by:
$$\Delta_M(\Ens \mid \Pi) = \bigcup_{i=1}^K \Delta_M(P_i \mid \Pi)$$
\end{definition}

Consider the following code example, generating the dispersion of a beta ensemble with respect to the consecutive pairing scheme.

\begin{code}[Consecutive Pair Dispersions of a Beta Ensemble]
In our notation, we are simulating the disperison of $\Ens \sim \H(\b = 4) \where P \in \R^{5 \times 5}$.
Specifically, we are simulating $\disp(P \mid \Pi_C)$. In the code implementation, we obtain an array for every dispersion metric $\delta$.
\end{code}
\begin{lstlisting}[language=R]
library(RMAT)
ens <- RME_beta(N = 4, beta = 4, size = 3)
disp_ens <- dispersion(ens, pairs = "consecutive")
# Outputs the following
disp_ens
...
i j eig_i    eig_j    id_diff iddf_norm abs_diff diff_ij
2 1 -3.78+0i 4.00+0i  7.78+0i 7.78      0.22     1
3 2 2.06+0i -3.78+0i -5.84+0i 5.84      1.72     1
4 3 0.19+0i  2.06+0i  1.88+0i 1.88      1.88     1
2 1 3.80+0i -4.00+0i -7.80+0i 7.80      0.20     1
3 2 -1.80+0i 3.80+0i  5.60+0i 5.60      2.00     1
4 3 0.89+0i -1.80+0i -2.69+0i 2.69      0.92     1
2 1 3.51+0i -3.53+0i -7.04+0i 7.04      0.03     1
3 2 1.35+0i  3.51+0i  2.16+0i 2.16      2.16     1
4 3 -0.67+0i 1.35+0i  2.02+0i 2.02      0.68     1
\end{lstlisting}

%=========================================================================================
%=========================================================================================

\section{Dispersion Analysis}

With dispersions well defined, we provide a few guidelines for analyzing a dispersion of a matrix.
We will synthesize all the techniques, notations, and remarks in the previous section to provide a comprehensive manual on how to analyze the dispersion of a matrix.

\subsection{Considerations}

After an extensive amount of setup, possibly too many definitions, the reader should be proud! This is the last of it. (I promise.)
Now, in the future sections, we will utilize our toolkit to analyze and study dispersions. \newline

However, before we may proceed, this section will be a primer prior to starting analyzing dispersions.
Specifically, this section will discuss various subtle caveats and general considerations to take into account before proceeding.
Essentially, we will be justifying the truck-load of definitions that were provided by showing why it is useful to have short-hand for them and use them concisely.
Without further ado, here are some considerations that should be taken into account when studying a dispersions. \newline

Generally speaking, when we consider from a dispersion set, what we care to obtain from it are insights and patterns in the eigenvalue spacings.
The dispersion of a matrix $P$, $\Delta_M(P)$ could tell us quite a few things.
One thing it could tell us is the distribution of the dispersion of eigenvalues given that we uniformly and randomly select a pair of eigenvalues;
this works when $\d$ is a symmetric function. However, if we are considered about spacings, then we must taking into consideration a few facts.
\newline
\medskip

\blocktitle{Linear Combinations} The identity difference metric is not necessarily a ``bad metric''.
In fact, it is one of the few metrics which allow transitivity under composition.
This brings up an important detail.
In the all-pairs dispersion $\Delta_{id}(P \mid \Pi_0)$, there is some redundant information if we are totally concerned with spacings. Namely, consider the following fact:
\begin{align*}
\lambda_i - \lambda_j = (\lambda_i - \lambda_k) - (\lambda_j - \lambda_k)
\end{align*}
In other words, if $\d = \d_{\text{id}}$, then $\d(\pi_{ij}) = \d(\pi_{ik}) - \d(\pi_{jk})$.
We could say that $\lambda_k$ is a pivot in which we could perform right-cancellation.
Because of this, using the all-pairs scheme $\Pi_0$ leads to finding various linear combinations in our dispersions.
\newline
\medskip

\blocktitle{Triangle Inequality} However, as we said previously, only the identity difference metric gives us transitive properties.
On the other hand, the other metrics provide us bounds when we compose them.
They are given by a simple application of the reverse triangle inequality. Namely, consider the following fact, which applies to any numbers $x, y$ in either $\R$ or $\C$.
\begin{align*}
||y| - |x|| \leq |y - x|
\end{align*}
%If we let $\d_{abs}$ be the difference of absolutes metric and $\d_{n}$ be the standard norm, then we could rephrase this as saying
Realize, two eigenvalues are just a pair of complex numbers, so we may rephrase that inequality as saying $|\d_{abs}(\pi_{ij})| \leq \d_{n}(\pi_{ij})$.
However, if choose the appopriate pairing scheme, we can remove the absolute value to obtain a more meaningful upper bound. \newline

Consider the following. The value of $\d_{abs}(\pi_{ij})$ can be interpreted as the difference between the sizes of $\lambda_j$ and $\lambda_i$.
However, if we use the \textbf{lower} pair combinations $\Pi_<$, then we know that $i > j$, making the value $|\lambda_j|$ always greater than $|\lambda_i|$ by construction.
As such, the left-hand value would always be positive, allowing us to drop the absolute value. So, we could say:
\begin{align*}
\d_{abs}(\pi) \leq \d_{n}(\pi) \given \pi \in \spair(P \mid \Pi_>)
\end{align*}
\medskip

\blocktitle{Sufficiency of Consecutive Pairs} Recall from previously that with respect to the identity difference, some elements of the dispersion $\Delta_{id}(P)$
can be expressed as a linear combination of other elements. To avoid this issue of linear combinations, we simply take the dispersion with respect to the consective pairing scheme. In other words,
we consider only consecutive eigenvalues.
By imposing this condition, we get that none of the values are a linear combination of the other in $\disp_{id}(P \mid \Pi_C)$. So, we can say that the consecutive pairs are sufficient in the sense that
they give us almost all the information we need.

%=========================================================================================
\subsection{Order Statistics}
With eigenvalue dispersions and eigenvalue orderings well-defined, we may proceed to start talking about their order statistics.

In \textbf{Section 2.2.3}, we observed simple order statistics regarding the eigenvalues.
This time around, we have two eigenvalues being compared at once, so there needs to be further setup with regards to dispersion order statistics.

However, prior to introducing any new concepts, there is one scenario where using one index is sufficient in terms of discussing dispersion order statistics.

\begin{remark}[Consecutive Pair Dispersions]
When we consider the dispersion with repsect to the consecutive pairs, things are much simpler since our pairs are intrinsically defined by one index ($j$).
This is because we are observing the $j^{th}$ eigenvalue and its smaller neighbour. As such, we will consider dispersion statistics in the form of $\E(\pit_j \mid j)$ and $\Var(\pit_j \mid j)$.
\end{remark}

Otherwise, we synthesize a new order statistic that takes in two indices (from a given pair) called the $\textbf{ranking difference class}$.
Since we are no longer observing a single eigenvalue at a given rank, we will need a way to standardize observing a pair of eigenvalues at a time.
To do so, we introduce a new $\textbf{equivalence relation}$ called the \textbf{ranking difference}. As the name suggests, it is precisely the integer difference of the eigenvalue orders.

\minititle{Ranking Difference}

\begin{definition}[Ranking Difference]
The ranking difference is a function $\rankdiff: \N \times \N \to \N$ which takes the index of two ordered eigenvalues and returns their difference.
In other words, it is the function $\rankdiff: (\lambda_i,\lambda_j) \mapsto (i - j)$.
\end{definition}

With the ranking difference, we may now take the spectral pairs of some random matrix with respect to either the lower $(\Pi_<)$ or upper $(\Pi_>)$ pairing scheme
and partition the eigenvalues into distinct equivalence classes.

\begin{remark}[Equivalence Relation]
Formally speaking, we may define $\sim_\rankdiff$ as an equivalence relation. It satsifies all three properties of reflexivity, symmetry, and transitivity.
More precisely, it is an equivalence class which partitions $\N_N \times \N_N$ into $N - 1$ equivalence classes given by $[r]_\rankdiff$ for $r \in \N_{N-1}$.
We would define the equivalence relation $\sim_\rankdiff$ as follows:
$$(\lambda_a,\lambda_b) \sim_\rankdiff (\lambda_c,\lambda_d) \iff (a -b) = (c-d)$$
So, two eigenvalue pairs would belong to the same class $[r]_\rankdiff$ if their ranking difference is the same. That is,
$$[r]_\rankdiff = \{(\lambda_\alpha, \lambda_\beta) \mid \alpha - \beta = r\}$$
\end{remark}

\begin{remark}[Matrix Analogy]
Using the eigenvalue matrix analogy, $\sim_\rankdiff$ partitions the matrix into diagonal bands.
The diagonal represents $[0]_\rankdiff$, the first off-diagonal represents $[1]_\rankdiff$, and so on...
Notice that this means every equivalence class has a different size.
\end{remark}

\newpage

With this partition in mind, we can consider various statistics conditioning on the value of $r$.
Conditioning on $r$ will be especially useful in the cases where we are considering matrices like the Hermite-$\beta$ matrices;
the eigenvalues of those matrices tend to $\textit{repel}$, so to speak, and we can observe these patterns using $r$. \newline

\noindent Consider the following example.

\begin{example}[Ranking Differences for a $4 \times 4$ Matrix]
Suppose $P$ is a $4 \times 4$ random matrix. Then, if we take the spectral pairs with respect to the lower pairs $\spair(P \mid \Pi_<)$ and partition it by $\rankdiff$,
we get the following classes:
\begin{enumerate}
  \item $[1]_\rho = \{\pit_1, \pit_2, \pit_3\}$
  \item $[2]_\rho = \{\pi_{31}, \pi_{42}\}$
  \item $[3]_\rho = \{\pi_{41}\}$
\end{enumerate}
\end{example}

\minititle{Dispersion: Matrix Plot}

%FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF
\plotwrapperNC{h}{0.6}{../graphics/chap3/3-2_pairtable}
%FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF

% %=========================================================================================
% \newpage
% \subsection{Conditional Order Statistics}
%
% We will considering the conditional statistics $\E(\rankdiff_{ij} \mid \rho)$ and $\Var(\rankdiff_{ij} \mid \rho)$.
%
% [Plots]

%=========================================================================================
%=========================================================================================
\newpage
\section{Wigner's Surmise}

%=========================================================================================

Wigner's surmise is a result found by Eugene Wigner regarding the limiting distribution of eigenvalue spacings of for symmetric matrices.
To start talking about this, we must talk about normalized spacings, which are the precise items considered in the distribution.
Before, we can talk about the normalized spacing, we define the mean spacing.

\begin{definition}[Mean Spacing]
Suppose $P$ is an $N \times N$ symmetric matrix, and $\sigma(P)$ are its real, sign-ordered eigenvalues.
Then, the mean (eigenvalue) spacing, denoted $\langle s \rangle$ is the average distance between two consecutive eigenvalues. That is,
$$\langle s \rangle = \E[\disp_\d(P \mid \Pi_C)] = \E[\d(\pit_{j})]_{j = 1}^{N - 1}$$
\end{definition}

\noindent So, with the mean spacing defined, we now define the normalized spacing between a pair of consecutive eigenvalues below.

\begin{definition}[Normalized Spacing]
Suppose $P$ is an $N \times N$ symmetric matrix, and $\sigma(P)$ are its real, sign-ordered eigenvalues.
Then, the normalized spacing of the $j^{th}$ pair of eigenvalues, denoted $s_j$ is given by the following formula.
$$s_j = \frac{(\lambda_j - \lambda_{j+1})}{\meanspacing} = \frac{\d(\pit_{j})}{\langle s \rangle}$$
\end{definition}

Finally, we define the Wigner dispersion, which we may reconstruct using our notation. This way, we can formalize Wigner's Surmise as
an observation of the Wigner dispersion for symmetric matrices.

\begin{definition}[Wigner Dispersion]
Suppose $P$ is an $N \times N$ symmetric matrix, and $\sigma(P)$ are its real, sign-ordered eigenvalues.
Then, the Wigner dispersion denoted $\disp_{Wigner}(P)$ is given by the set of normalized conseuctive eigenvalues of $P$. That is,
$$ \disp_{W}(P) = \left\{ \frac{\d_n(\pi)}{\meanspacing} \mid \pi \in \spair(P \mid \Pi_C) \right\} $$
\end{definition}

The extension for ensembles is trivial; it inherits the same notation for matrices and the definition is extended similar to how we
did so for the spectrum and dispersion of an ensemble. That being said, consider the following simulation of Wigner's surmise.

\newpage

%=========================================================================================

\subsection{Symmetric Normal Matrices}

We simulate the Wigner dispersion distribution (Wigner's surmise) for several $\Ens \sim \Normal(0, \sigma^2)^\dagger$ matrices over $\R$.

%FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF
\plotwrapper{h}{0.6}{../graphics/chap3/3-3_wigner_norm}{Wigner's Surmise for Symmetric Normal Matrices}
%FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF

\minititle{Plot Takeaways}

It seems as though the distribution is independent of the variance of the distribution. So, how do we really vary the wigner dispersion distribution?
The answer is $\b$-ensembles.

%=========================================================================================
\newpage
\subsection{Beta Ensembles}
%=========================================================================================

The analytical results for the Hermite $\beta$-ensembles for $\beta = 1, 2, 4$, denoted by $w_\beta$, are stated below.
These are the asymptotic densities for the normalized spacings between consecutive eigenvalues for each of the matrices for $\b = 1,2,4$.

\begin{align*}
  w_1(s) &= \frac{\pi}{2} \exp(-\frac{\pi}{4}s^2) \\
  w_2(s) &= \frac{32}{\pi^2} s^2 \exp(-\frac{4}{\pi}s^2) \\
  w_4(s) &= \frac{2^{18}}{3^{6}\pi^3} s^4 \exp(-\frac{64}{9\pi}s^2) \\
\end{align*}

Additionally, their simulated densities are given in the plot below.

%FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF
\plotwrapper{h}{0.6}{../graphics/chap3/3-3_wigner_beta}{Wigner's Surmise for standard Beta Ensembles}
%FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF

\minititle{Plot Takeaways}

Moments, variance, mean

%=========================================================================================%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%=========================================================================================%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \newpage
% \section{Checkpoint}
%
% Here's what we've done so far.
%
% %=========================================================================================%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \minititle{Chapter 1}
%
%
% \begin{enumerate}
%   \item Formalized the following items
%     \begin{enumerate}
%       \item Random Matrices using $\D$-distributions.
%       \item Random Matrix Ensembles
%     \end{enumerate}
%   \item Some item
% \end{enumerate}
%
% %=========================================================================================%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \minititle{Chapter 2}
%
% \begin{enumerate}
%     \item Formalized the following items
%       \begin{enumerate}
%         \item Spectrum of a Random Matrix (Ensemble)
%       \end{enumerate}
%     \item Some item
% \end{enumerate}
%
% %=========================================================================================%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \minititle{Chapter 3}
%
% \begin{enumerate}
%     \item Formalized the following items
%       \begin{enumerate}
%         \item Spectral Pairs: the set of eigenvalue pair combinations
%         \item Pair schemes: which eigenvalue pairs to choose
%         \item Dispersion metrics: what distance metric to use on the pairs
%         \item Dispersion of a Random Matrix (Ensemble)
%       \end{enumerate}
%     \item Some item
% \end{enumerate}
%
% %=========================================================================================%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \minititle{Segway to Chapter 4}
%
% Here's how we'll segway to Chapter 4, which doesn't flow without this segway.
%=========================================================================================%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%=========================================================================================%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\section{Findings}
