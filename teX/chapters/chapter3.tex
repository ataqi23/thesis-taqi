
\chapter{Dispersions}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

In this section, we define the final spectral statistic studied in this chapter: eigenvalue dispersions. As the name suggests, these statistics are concerned with the distirbution of the spacings between the eigenvalues. 

Oddly enough, this is almost as literal as it gets when we use the word "spectral". In physics and chemistry, atomic spectra are essentially differences between energy levels or quanta, so the translation is close. 

In any case, we motivate a few definitions and formalisms in this section, then motivate the definition of a matrix's eigenvalue dispersion. To start off, we define an object useful for pairing our eigenvalues together, the pairing scheme.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Pairing Schema}

Next up, we introduce a new notation for a pairing scheme denoted $\Pi$. What are pairing schemes and why do they matter? Recall that our goal is to study the spacings between eigenvalues. If we are studying spacing, then a priori, we are concerned with pairs of eigenvalues! Spacing, after all, is a binary relationship. 

So, with the definitions of spectra well motiviated, a natural definition of pairing schema follows. Essentially, $\Pi$ is just a subset of the Cartesian product of a spectrum with itself. In other words, if we denote $\S := \sigma(P)$, then we say that a pairing scheme is simply a subset $\Pi \subseteq \S^2$. Here are some pairing schema that we will consider:

\begin{enumerate}
  \item The unique pair combinations are two related pair schema. By specifying $i > j$ or $i < j$, we are able to obtain all unique pairs of unique indices. The reason we call them upper and lower pair combinations is in reference to the indices of the upper and lower triangular matrices. \begin{enumerate}
    \item Let $\Pi_>$ be the set of unique (lower-pair) combinations of ordered eigenvalues. This will be the standard ordered pair scheme used in lieu of our dispersion metric argument orders (more later). In this pairing scheme, the eigenvalue with the lower rank is always listed first, and the higher rank second.
    $$\Pi_> = \{\pi_{ij} = (\lambda_i,\lambda_j) \mid i > j\}_{i = 1}^{N-1}$$
    \item For completeness, we will also define $\Pi_<$. This is the set of (upper-pair) unique combinations of ordered eigenvalues. 
    $$\Pi_< = \{\pi_{ij} = (\lambda_i,\lambda_j) \mid i < j\}_{i = 1}^{N-1}$$
  \end{enumerate}
\item Let $\Pi_1$ be the largest pair of eigenvalues of a spectrum. Nice and simple.
$$\Pi_1 = \{(\lambda_2,\lambda_1)\}$$
\item Let $\Pi_C$ be the consecutive pairs of eigenvalues in a spectrum. This pairing scheme gives us the minimal information needed to express important bounds and spacings in terms of its elements.
$$\Pi_C = \{\pi_{j} = (\lambda_{j + 1},\lambda_j)\}_{j = 1}^{N-1}$$
% \item Let $\Pi_C$ be the consecutive pairs of eigenvalues in a spectrum. This pairing scheme gives us the minimal information needed to express important bounds and spacings in terms of its elements.
% $$\Pi_C = \{\pi_{ij} = (\lambda_i,\lambda_j) \mid i = j + 1\}_{i = 1}^{N-1}$$
\end{enumerate}

[Plot showing difference when using different graphs]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Dispersion Metrics}

When we define the dispersion of a matrix, we will see that there is a free argument $d$ called the dispersion metric. This function $d$ is a general function whose domain is always two eigenvalues. In set notation, this is the set $\Cc \times \Cc$ - a pair of two complex numbers. Its range will often be the positive reals $\R^+$; this is because the dispersion metric often will be substitutable with distance metric. Sometimes, the range will be $\Cc$. So, the dispersion metric will take the following form:
\begin{align*}
d: \Cc \times \Cc \to \{\R^+, \Cc\}
\end{align*}

Consider the following dispersion metrics below. Out of those 4 dispersion metrics, only the first one has a range of $\Cc$. The rest have a range of $\R^+$. Additionally, the second and third metrics are $\textit{symmetric}$ operations while the rest are not. The $\beta$-norm is only a symmetric operation when $\beta$ is even.

\begin{enumerate}
\item The identity difference: $d_{id}(z,z') = z' - z$
\item The standard norm: $d_{n}(z,z') = |z' - z|$
\item The $\beta$-norm: $d_\beta(z,z') = |z' - z|^\beta$
\item The difference of absolutes: $d_{ad}(z,z') = |z'| - |z|$
\end{enumerate}

Finally, we are able to motivate the definition of a matrix dispersion! Suppose we have a $\mathcal{D}$-distributed random matrix $P \in \F^{N \times N}$ or a random matrix ensemble $\Ens = \{P_i \sim \mathcal{D} \mid P_i \in \F^{N \times N}\}$. Then we define their dispersion as follows. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Spectral Dispersions}

\begin{definition}[Dispersion]
The dispersion of a matrix $P \in \F^{N \times N}$ with respect to a dispersion metric $d: \Cc \times \Cc \to \F$ and pairing scheme $\Pi$, call it $\Delta_d(P, \Pi)$, is defined as follows. Suppose $\sigma(P) := \S$ is the ordered spectrum of $P$ where $\sigma(P) = \{\lambda_1, \dots, \lambda_N\}$. Then, let $\Pi = \{\pi_{ij} = (\lambda_i,\lambda_j) \} \subseteq \S^2$ be a subset of eigenvalue ordered pairs. Then, the dispersion of $P$ with respect to $d$ is simply the set $\Delta_d(P, \Pi)=\{\delta_{ij} = d(\pi_{ij}) \mid \pi_{ij} = (\lambda_i,\lambda_j) \in \Pi\}$.
\end{definition}

As we usually do, we define the dispersion of an ensemble in a similar fashion. 

\begin{definition}[Ensemble Dispersion]
If we have an ensemble $\Ens$, then we can naturally extend the definition of $\Delta_d(\Ens, \Pi)$. To take the dispersion of an ensemble, simply take the union of the dispersions of each of its matrices. In other words, if $\Ens = \{P_i \sim \mathcal{D} \mid P_i \in \F^{N \times N}\}_{i = 1}^K$, then $\Delta_d(\Ens, \Pi) = \bigcup_{i=1}^K \Delta_d(P_i, \Pi)$.
\end{definition}

With our spectral statistics defined, we are prepared to discuss prominent results in Random Matrix Theory alongside our new findings from the simulations. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Order Statistics}
\subsection{Introduction}
With eigenvalue dispersions and eigenvalue orderings well-defined, we may proceed to start talking about their order statistics. 

In addition to these simple order statistics, we introduce a new variant statistic called the $\textbf{ranking difference class}$. Instead of observing a single eigenvalue at a given rank, we will now observe a pair of eigenvalues at a time. To standardize the process, we introduce a new eqivalence class called the \textit{ranking difference}. As suggested, it is precisely the integer difference of the eigenvalue ranks.

\begin{definition}[Ranking Difference]
The ranking difference is a function $\delta: \N \times \N \to \N$ which takes the index of two eigenvalues (from an \textit{ordered} spectrum) and returns their difference. In other words, $\delta : (\lambda_i,\lambda_j) \mapsto (i - j)$.
\end{definition}

With the function $\delta$, we may take the set of unique eigenvalue pairs $(i > j)$ and partition it into equivalence classes. To do so, we define the equivalence relation $\sim_\delta$ which says $(\lambda_a,\lambda_b) \sim_\delta (\lambda_c,\lambda_d) \iff (a -b) = (c-d)$. These equivalence classes then naturally corrospond to pairs a set distance $\rho = i - j$ apart. So, for an $N \times N$ matrix, $\delta$ assumes a range $\rho \in \{ 1,\dots,N-1\}$.

In summary, $\sim_\delta$ takes the set $\{(\lambda_i, \lambda_j) \mid \lambda_i, \lambda_j \in \sigma(P) \text{ and } i > j \}$ and surjectively partitions it onto the equivalence classes $[(\lambda_i, \lambda_j)]_\rho$ for $\rho \in \{ 1,\dots,N-1\}$. For example, if we consider $\rho = 1$, then we are considering all pairs of eigenvalue neighbors.

Note that the sizes of each equivalence class are $\textbf{never equal}$. With this partition in mind, we can consider various statistics conditioning on the value of $\rho$. Conditioning on $\rho$ will be especially useful in the cases where we are considering matrices like the Hermite-$\beta$ matrices; the eigenvalues of those matrices tend to $\textit{repel}$, so to speak, and we can observe these patterns using $\rho$.

\subsection{Conditional Statistics}

We will considering the conditional statistics $\E(\delta_{ij} \mid \rho)$ and $\Var(\delta_{ij} \mid \rho)$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Analytical Results}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Case Study: Wigmer's Surmise}

Limiting distribution of the eigenvalue spacings of symmetric matrices.

[Plot]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Findings}
