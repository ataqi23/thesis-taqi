
\chapter{Spectra}

%=========================================================================================
\epigraph{Life is like a box of crayons.}{\textit{Unknown}}
%=========================================================================================

\section{Introduction}

In the context of this thesis, \textit{spectral statistics} will be an umbrella term for random matrix statistics that somehow involve that matrix's eigenvalues and eigenvectors.
That being said, if we fix a \textit{random matrix}, we can study its features by studying its eigenvalues - fundemental numbers that tell us a lot about the matrix.
They are quite important for many reasons.
For instance in statistical physics, many processes are represented by operators or matrices, and as such, their behaviours could be partially determined by the eigenvalues of their corresponding matrices.
The study of eigenvalues and eigenvectors primarily falls in the scope of linear algebra, but their utility is far-reaching. So, what are \textit{eigenvalues} exactly?

%=========================================================================================

\subsection{The Quintessential Spectral Statistic: the Eigenvalue}
Given any standard square matrix $N \times N$ matrix $P$ over some field $\F$,
its \textit{eigenvalues} are simply the roots of the characteristic polynomial, $\text{char}_P{(\lambda)}$ = $\det(P - \lambda I)$.
The characteristic polynomial is just like any other polynomial. What is important to know is that $\text{char}_P$ will have a degree of $N$ (i.e. equal to the dimension of the matrix).
By the Fundamental Theorem of Algebra, this implies that there is always as many complex eigenvalues $\lambda \in \Cc$ as the dimension of the matrix.

That being said, when our random matrix has a specified distribution (say, standard normal), we can see patterns in the eigenvalue distributions.
So, an eigenvalue is a \textbf{spectral statistic} of a random matrix! To talk about a matrix's eigenvalues in a more formal and concise manner, we motivate what is the \text{eigenvalue spectrum}.

%\newpage
\bigskip
\bigskip
\begin{definition}[Spectrum]
Suppose $P \in \F^{N \times N}$ is a square matrix of size $N$ over $\F$. Then, the (eigenvalue) spectrum of $P$ is defined as the multiset of its eigenvalues and it is denoted
$$\sigma(P) = \{\lambda_i \in \Cc \mid \text{char}_P(\lambda_i) = 0 \}_{i=1}^N$$
Note that it is important to specify that a spectrum is a multiset and not just a set; eigenvalues could be repeated due to algebraic multiplicity and we opt to always have $N$ eigenvalues.
\end{definition}

\medskip
For example, consider the following code example from the $\RMAT$ package.

\begin{code}[Spectrum of a Standard Normal Matrix]
Let $P \sim \Normal(0,1)$ be a $5 \times 5$ standard normal random matrix. We can generate the spectrum of $P$, $\sigma(P)$ as follows:
\end{code}

\begin{lstlisting}[language=R]
library(RMAT)
P <- RM_norm(N = 5, mean = 0, sd = 1)
spectrum_P <- spectrum(P)
# Outputs the following
spectrum_P
...
        Re      Im   Norm Order
 1 -0.5434  1.3539 1.4589     1
 2 -0.5434 -1.3539 1.4589     2
 3  0.2255  1.4250 1.4427     3
 4  0.2255 -1.4250 1.4427     4
 5 -0.8678  0.0000 0.8678     5
\end{lstlisting}

Notice, in the table above, we obtain the spectrum of the matrix $P$ as returned by the spectrum wrapper function in $\RMAT$.
In this tidy\footnote{In reference to the tidyverse principles of tidy data frames.} table, each row represents one eigenvalue and the output is comprised of four columns.
The first two columns represent the real and imaginary components of the eigenvalue, respectively. The third column denotes its norm. Lastly, the fourth column denotes the eigenvalue's
relative order within the matrix's spectrum; the order is with respect to the default order scheme the function chooses, which is by norm. Order schemes will be discussed in more depth in \textbf{Section 2.2.1}.

\newpage

\minititle{Eigenvalues as Statistics: A Formalization}

While our definition for spectrum is nice and clean, there are a few caveats that we must take care of.
First, how are spectra considered statistics? Simply put, we know that a statistic is some function of random data.
However, before we can even call them so, there needs to be some formalization as to justify considering eigenvalues statistics.
In the upcoming dialogue, we will reminicse on the corresponding formalization dialogue in \textbf{Section 1.1.3} when we formalized random matrices.
Before beginning, the reader is encouraged to review what a statistic is in \textbf{Appendix A.2.2}.

Without further ado, recall that when we defined and motivated the $\D$-distribution framework of simulating random matrices, there was always one common factor - a representation using vectors of random variables.
Using this framework, the formalization of eigenvalue spectra as statistics is not too difficult.

\begin{formalization}
Suppose $P \sim \D$ is an $N \times N$ random matrix.
Then, $P$ has a representation as a sequence of $K$ random variables (for some $K \in \N$).
Denote it as the vector $\vec{X} = \seqo[K]{X}$.
Then, the spectrum of the matrix $P$ is simply a function of the vector $\vec{X}$.
We can denote this $\sigma(\vec{X})$, where the operator $\sigma$ is overloaded to mean the spectrum of a matrix $\textbf{with respect to the vector representation}$.
%The actual process for $\sigma$ is not necessary to explicitly write out, since characterizing it will be sufficient for now.
In that case, $\sigma$ is a function that would parse the random variables into the appopriate matrix form by index hacking.
Then, using the matrix, the eigenvalues are computed by finding the roots of the characteristic polynomial (or some other algorithm).
Sparing the details, this characterization of $\sigma$ is sufficient to show that the eigenvalues are in one way or another a function of random variables -- or in other words, a statistic.
To summarize this, consider the flow chart below.
\end{formalization}

Note how in the formalization we said that an $N \times N$ matrix $P$ has a representation as a vector of $K$ random variables rather than $N^2$ variables.
Recall that some non-homogenous explicit $\D$-distributions do not initialize every entry of the matrix. Only our homogenous explicit and implicit $\D$-distributions do.
That being said, to summarize, here is how we formalize the spectrum as a statistic. Suppose we sample $P \sim \D$. Then, we take its spectrum formally using $\sigma$ as such:\hfill
$$ X_1,X_2,\dots,X_K \xra{\text{make array}} (\vec{X} \rightsquigarrow P) \xra{\det(P - \lambda I)} char_{P}(\lambda) \xra{\text{solve for roots}} \sigma(P) $$

\begin{remark}[Computation]
While eigenvalues are theoretically defined as the roots of the characteristic polynomial, they are not computed using them in practice.
This is intractable as $N$ grows; for an $1000 \times 1000$ matrix, we would need to solve for one thousand roots.
Needless to say, that is unviable.
To simulate our random spectra, the $\RMAT$ package relies on the \codeword{eigen()} function in base R.
The function relies on techniques such as the $QR$ algorithm and power iteration. [\cite{qr}]
However, this is not our subject matter, and this does not impact our formalization at all.
Whether the eigenvalues are computed by solving for roots or by the QR algorithm, they will always remain a function of random variables --- a statistic.
\end{remark}


%=========================================================================================
\newpage
\subsection{Interlude: Ensembles}
While the spectrum of a matrix provides a good summary of the matrix in and of itself, a matrix is only considered a single point/observation in random matrix theory.
Additionally, simulating large matrices and computing their eigenvalues becomes harder and more computationally expensive as $N \to \infty$.
As such, to obtain more eigenvalue statistics efficiently, another dimension is introduced by motiving the \textit{spectrum of a random matrix ensemble}.
If we have an ensemble $\Ens$, then we can naturally extend the definition of $\sigma(\Ens)$.

\begin{definition}[Ensemble Spectrum]
Let $\Ens \sim \D$ be an ensemble of matrices $P_i \in \F^{n \times n}$. To take the spectrum of $\Ens$, simply take the union of the spectra of each of its matrices.
In other words, if $\Ens = \{P_i \sim \mathcal{D}\}_{i = 1}^K$, then we denote the spectrum of the ensemble
$$\sigma(\Ens) = \bigcup_{i=1}^K \sigma(P_i)$$
\end{definition}

\medskip
For example, consider the following code example from the $\RMAT$ package.
\begin{code}[Spectrum of a Standard Normal Matrix Ensemble]
Let $\Ens \sim \Normal(0,1)$ be an ensemble of $3 \times 3$ standard normal random matrices of size $3$. We can generate the spectrum of $\Ens$, $\sigma(\Ens)$ as follows:
\end{code}

\begin{lstlisting}[language=R]
library(RMAT)
ens <- RME_norm(N = 3, mean = 0, sd = 1, size = 3)
spectrum_ens <- spectrum(ens)
# Outputs the following
spectrum_ens
...
\end{lstlisting}

\begin{lstlisting}[language=R]
        Re      Im   Norm Order
 1  1.7581  0.0000 1.7581     1
 2 -0.2614  1.0012 1.0347     2
 3 -0.2614 -1.0012 1.0347     3
 4  1.2327  0.4227 1.3032     1
 5  1.2327 -0.4227 1.3032     2
 6 -0.8504  0.0000 0.8504     3
 7 -0.5296  1.0508 1.1767     1
 8 -0.5296 -1.0508 1.1767     2
 9  0.7357  0.0000 0.7357     3
\end{lstlisting}

Notice, in the array above, we have a table similar to that of a spectrum of a random matrix (like in the code example above).
However, we now have multiple values at a given order since we are taking the union of all the spectra.
For this reason, ensembles can tell us more about the bounds of the eigenvalues by observing the largest and smallest eigenvalues.

\newpage

A common theme in this thesis will be that singleton matrices do not provide insightful information on their own.
Rather, it is the collective behavior of a $\D$-distributed ensemble that tells us about how $\D$ impacts our spectral statistics.
So in a way, ensemble statistics are the engine of this research.

%FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF
\plotwrapper{h}{0.125}{../graphics/chap2/2-1-2_comparison}{Spectrum of a Matrix versus an Ensemble}
%FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF

From the plot above, we can see that the spectrum of the ensemble is much more populated.
In both plots, we are simulating the spectrum of the same object: a standard normal $50 \times 50$ matrix.
For this reason, the Order legends on the right-hand side are equivalent across the plots.
However, since we are only considering the spectrum of a singleton matrix on the left-hand side, it means that every Order corresponds to exactly one point.
On the right-hand side, each Order is populated with $K$ points (where $K$ is the size of the ensemble).
As such, from just taking numerous iterations of the matrix, we are able to ``complete the picture'' and see that the eigenvalues seem to be uniformly distributed on a complex disk!

%=========================================================================================
%=========================================================================================
% \newpage
\section{Spectrum Analysis}

%=========================================================================================

\subsection{Ordered Spectra}

When we motivate the idea of matrix dispersion in the next section, we will consider order statistics of that matrix's eigenvalues in tandem with its dispersion.
However, to do so presupposes that we have a sense of what \textit{ordered} eigenvalues means.
Take a matrix $P$ and its \textit{unordered} spectrum $\sigma(P) = \{\lambda_j\}$.
It is paramount to know what ordering scheme $\sigma(P)$ is using, because otherwise, the eigenvalue indices are meaningless!
So, to eliminate confusion, we add an index to $\sigma$ that indicates how the spectrum is ordered.
Often, the ordering context will be clear and the indexing will be omitted.

\minititle{Order Schemes: How to Order Eigenvalues}

% \noindent Consider the two following $\textit{ordering schema}$: \\

In this thesis, we will discuss three order schemes. However, when it comes to practice, we will most often use the two primary order schemes listed below. \\

In the relevant literature, researchers often use the standard ordering on $\R$ to define an ordered spectrum.
We denote this as the ordering by the $\textbf{sign scheme}$.
However, because total-ordering is only well-defined on $\R$, we can only use this scheme when none of our eigenvalues lives in $\C$.
So, we write the $\textit{sign-ordered spectrum}$ as follows:
\begin{align*}
\sigma_S(P) = \{\lambda_j : \lambda_1 \geq \lambda_2 \geq \dots \geq \lambda_N\}_{j = 1}^N
\end{align*}
Alternatively, we could sort the spectrum by the norm of its entries; denote this as ordering using the \textbf{norm scheme}.
We are forced to use this scheme if some of our eigenvalues live in $\C$, due to the reasons mentioned above.
By taking their norms, each eigenvalue is mapped to a real value. From which, we could then sort them using the regular ordering on $\R$.
Essentially, we are simply using the sign ordering scheme on the norms of the eigenvalues.
Without further ado, we write the $\textit{norm-ordered spectrum}$ as follows:
\begin{align*}
\sigma_N(P) = \{\lambda_j : |\lambda_1| \geq |\lambda_2| \geq \dots \geq |\lambda_N|\}_{j = 1}^N
\end{align*}
Note that when we take the norms of the eigenvalues, we essentially ignore ``rotational'' features of the eigenvalues.
Signs of eigenvalues indicate reflection or rotation, so when we take the norm, we essentially become more concerned with scaling. \\

That being said, consider the following plots, showing the difference in using the sign and norm ordering schemes for the same spectrum.
\trim
%FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF
\plotwrapper{h}{0.40}{../graphics/chap2/2-2-1_orderscheme}
{Spectrum of an Ensemble Using Two Different Ordering Schemes}
%FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF

%=========================================================================================
\newpage
\minititle{Singular Values}

Our last ordering scheme is moreso a different method of defining the spectrum of a matrix than one of sorting the eigenvalues.
However, for simplicity, we will call it an ordering scheme.
Suppose $P$ is a random matrix.
Then, we can take its singular values as such.
\begin{definition}[Singular Values]
The singular values of a matrix $P$ are given by the square root of the eigenvalues of the corresponding product of that matrix and its transpose.
That is, $$\sigma_+(P) = \sqrt{\sigma(P \cdot P^T)}$$
\end{definition}
Because of the way they are defined, singular values are always non-negative, even if the eigenvalues are negative.
As a result, unlike the eigenvalues, singular values in a sense ``ignore'' rotational features of the matrix. Instead, they only tell us about the ``scaling'' features of the matrix.

There are a few reasons why singular values are important.
One enormous advantage of using singular values is that it opens the door for studying spectral statistics for \textbf{non-square} matrices.
This is because any matrix $Q \in \F^{n \times m}$ multiplied by its transpose $Q^T \in \F^{m \times n}$ yields a square matrix $S = Q Q^T \in \F^{n \times n}$.

Additionally, it is worth noting that because of the way singular values are defined,
taking the singular values of a symmetric matrix is equivalent to taking its spectrum with respect to the norm ordering scheme.
This follows because if a matrix is symmetric, then its singular values are simply the norm of the eigenvalues.

%\newpage
%****************************************************************************************%
\minititle{Summary Table of Spectrum Schema}
\spectrumschemetable
%****************************************************************************************%

%=========================================================================================
\newpage
\subsection{Order Statistics}

%=========================================================================================

With eigenvalue ordering unambiguous and well-defined, we may proceed to start talking about their order statistics.
In short, given a random sample of fixed size, order statistics are random variables defined as the value of an element conditioning on its rank within the sample. (see \textbf{Appendix A.2.2})

In general, order statistics are quite useful and tell us a lot about how the eigenvalues distribute given a distribution.
They tell us how the eigenvalues space themselves and give us useful upper and lower bounds.
For example, the maximum of a sample is an order statistic concerned with the highest ranked element.
In our case, this could correspond the largest eigenvalue of a spectrum. After all, a spectrum is a random sample of fixed size, so this statistic is well-defined.

\begin{remark}[Indices]
It is a common convention when notating ordering statistics to say that $X_i > X_j$ when $i > j$. However, in this thesis, we will oppose this convention.
This is because we are often interested in notating the largest eigenvalue $\l_1$ rather than $\l_N$ where $N$ is the dimension of the matrix.
So, to make the larger eigenvalues intrinsic (independent of $N$), we use the opposite convention and say that $X_i < X_j$ when $i > j$
\end{remark}

That being said, then we can notate the value $\l_1$ the largest eigenvalue in the spectrum, for instance.
Given an $N \times N$ matrix $P$, the eigenvalue $\l_N$ represents the smallest eigenvalue.

\begin{warning}[Order Schema \& Indices]
Notice how when we are discuss summary statistics below, we make sure to say value or size (and generalize by saying quantity) because the $i^{th}$ rank has a different intrepretation under different order schema.
In the norm-ordered scheme, $\l_1$ means the largest eigenvalue whereas in the sign-ordered scheme, $\l_1$ means the ``most positive'' eigenvalue.
\end{warning}


% \begin{example}[The Largest Eigenvalue]
% Suppose we have seek the largest eigenvalue distribution for a ensemble distribution $\D$, we would simulate an ensemble $\Ens$ and observe $\lambda_1$ for each of its matrices.
% Then, we can set the distribution of the largest eigenvalue for $\D$ by observing the distribution of $\lambda_1$.
% \end{example}

%\minititle{Conditional Statistics}

All that being said, one framework of studying order statistics will be conditioning on their values. So, we will consider the following summary statistics that condition on the order statistics. \newline

%\vspace{1em}

\blocktitle{Expectation} $\E(\lambda_{i})$ One useful summary statistic to consider when analyzing a spectrum is the expected norm or value (which we will just call quantity hereinafter) of the eigenvalue at the $i^{th}$ rank.
When considered at the ensemble level, the mean quantity order statistic can tell us a lot about the bounds on the eigenvalue sizes or values for a given $\D$-distribution.
For instance, given the norm-ordered scheme the expected extreme values (assuming $N \times N$ matrices), given by $\E(\l_1)$ and $\E(\l_N)$ respectively can tell us the expected range our spectrum might take. \newline

\medskip

\blocktitle{Variance} $\Var(\lambda_{i})$ Similarly, the variance of the eigenvalue quantity at a given order $i$ can tell us a lot about an ensemble. For example, we may observe that variances are heteroskedastic (not equal in every level) across all the levels $i$. This insight can tell us how a $\D$-distribution disperses it eigenvalues or perhaps how the eigenvalues ``repel'' each other.

\medskip


%=========================================================================================
%=========================================================================================
\newpage
\subsection{Case Study: Perron-Frobenius Theorem}

\begin{definition}[Spectral Radius]
  Let be $P$ be any matrix and $\spec(P)$ be its ordered spectrum. Then, the spectral radius of $P$ is defined as $\rho(P) = ||\sup \spec(P)||$ is the norm of its largest eigenvalue.
\end{definition}

\minititle{Detour: The Perron-Frobenius Theorem}

Using the toolkit we have now accquired, we can now discuss an elegant, visual representation of the Perron-Frobenius theorem (see \textbf{Section A.1.2}).
To put it shortly, the Perron-Frobenius theorem, applied to stochastic matrices is a result that guarantees the existence of a stationary distribution to an ergodic Markov Chain (see \textbf{Section A.3}).
That being said, the following facts give us a heuristic demonstration of the Perron-Frobenius theorem.

\begin{theorem}[Perron-Frobenius Theorem] \hfill
  \vspace{-1em}
  \\ \\ Consider the two following facts:
  \begin{enumerate}
    \item The largest eigenvalue of stochastic matrices is 1.
    \item When multiplied by $P^K$, any point $v$ asymptotically enters the eigenspace of the matrix's largest eigenvalue as $K \to \infty$. That is, as $K$ grows, $v P^K$ approaches eigenvector of $P$ of $\lambda_1$.
  \end{enumerate}
So, there is an eigenvector of the largest eigenvalue -- for an irreducible Markov Chain, this is a stationary distribution.
\end{theorem}

\begin{note}
  Note that this is only part of the results of the theorem. The theorem is more general and has a wider scope and applies to matrices in a more general fashion.
  We only demonstrate this because we have a case with a constant largest eigenvalue and an eigenvector with a unique interpretation (a stationary distribution).
\end{note}

\newpage

Again, those two statements were not formally proven in this thesis. However, there are large amounts of computational evidence for both of these results.

Consider the first statement. Below we have a stochastic ensemble spectrum.
Notice how the largest eigenvalue is 1, living far from the island of \textbf{complex} eigenvalues.

%FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF
\plotwrapper{h}{0.15}{../graphics/chap2/2-4_stoch_spec}{Spectrum of a Stochastic Matrix ensemble}
%FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF

This shows that the spectral radius of stochastic matrices is $1$! What is left is the other fact.
For the second statement, again, there is plenty of computational evidence to back this up.
The reader may refer to \textbf{Appendix D} for an empirical demonstation of fact 2.
Because the discussion of eigenvectors is too far afield from the current topic, so we will not include it in the current section.

From the simulations, we find that there is overwhelming evidence that most matrices, if not all, satisfy the second statement.
The ``almost all'' part here is an artifact of random matrices, since it is unlikely they will have duplicate eigenvalues, or eigenvalues corresponding to pure rotations (with no scaling).
These are the types of eigenvalues that lead to the second statement not holding. \newline

\blocktitle{Conclusions} So, to conclude, we can say that the computational evidence for these two facts supports and provides an alternative method of empirically demonstrating that the Perron-Frobenius theorem is true.

%=========================================================================================
%=========================================================================================

\newpage
\section{Symmetric and Hermitian Matrices}

\subsection{Introduction}
A very important class of matrices in linear algebra is that of symmetric or hermitian matrices (see \textbf{Appendix A.1.1}). Simply put, those are matrices which are equal to their conjugate transpose.

\begin{remark}[Symmetric versus Hermitian]
Since real numbers are their own conjugate transpose, every symmetric matrix is hermitian. However, we will still delineate the two terms to avoid confusion and indicate what field $\F$ we are working with.
\end{remark}

In any case, one critical result in linear algebra that will be extensively wielded in this thesis is the fact that if a matrix is symmetric or hermitian, then it has real eigenvalues [\cite{horn}]. In other words:
\begin{align*}
P = \overline{P^{T}} \implies \sigma(P) = \{\lambda_i \mid \lambda_i \in \R\}
\end{align*}
Having a complete set of real eigenvalues yields many great properties. For instance, if all eigenvalues are real, we have the option of observing either the sign-ordered spectrum or the norm-ordered spectrum.
This way, we can preserve negative signs and we would not lose the rotational aspect of the eigenvalue when we study its statistics. That is just one reason out of many more why having real eigenvalues is quite nice.

%=========================================================================================
\minititle{Example: Stochastic Matrices}

One very pleasing example to look at is stochastic matrices.
As seen previously in \textbf{Section 2.2.2}, stochastic matrices tend to have two components: a complex disk of eigenvalues about the origin and isolated point that is the largest eigenvalue.

Below we have a \textbf{symmetric} stochastic ensemble spectrum. Notice how the largest eigenvalue is 1, living far from the island of \textbf{real} eigenvalues.
This can be compared to the spectrum of non-symmetric stochastic matrices in the previous figure.
The patterns are similar, but now, imposing symmetry on the matrices forces the eigenvalues to ``collapse'' onto the real line!

%FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF
\plotwrapper{h}{0.6}{../graphics/chap2/2-4_symmstoch_spec}{Spectrum of a Symmetric Stochastic Matrix ensemble}
%FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF

%=========================================================================================
\newpage
\subsection{Wigner's Semicircle Distribution}

The eigenvalues of hermitian matrices obey Wigner's Semicircle distribution.
Since hermitian matrices have real eigenvalues, then we can be more precise and generally say that the real component of the eigenvalues follow the semicircle distribution.
The distribution is named after physicist Eugene Wigner.

\begin{definition}[Semicircle Distribtion]
If a random variable $X$ is semicircle distributed with radius $R \in \R^+$, then we say $X \sim \text{SC}(R)$. $X$ has the following probability density function:
$$\Prb(X = x) = \frac{2}{\pi R^2} \sqrt{R^2 - x^2} \for x \in [-R, R]$$
\end{definition}

As it turns out, the dimension of the matrix ($N$) impacts the radius of the eigenvalue distribution [\cite{tao}].

\begin{remark}[Radius and Matrix Dimension]
The dimension of the matrix determines the radius of the eigenvalues.
Namely, if a hermitian matrix $P$ is $N \times N$, then its eigenvalues are approximately semicircle distributed with radius $R = 2\sqrt{N}$; the approximation improves as $N$ gets larger.
That is, $P^{\dagger}$ has a spectrum $\sigma({P}) \sim \text{SC}(R = 2\sqrt{N})$ as $N \to \infty$.
\end{remark}


%FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF
\plotwrapper{h}{0.58}{../graphics/chap2/2-3-2_semicircle}{Eigenvalues of a Symmetric Matrix displaying the Semicircle Distribution}
%FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF

\newpage
\subsection{Spectra}
%=========================================================================================

In this section, we will be carefully analyzing an ensemble of symmetric matrices to showcase
the special properties of symmetric and hermitian matrices.
Namely, we will be considering an ensemble $\Ens \sim \Normal(0,1)^\dagger$ of $15 \times 15$ matrices over $\R$.

%=========================================================================================
%\newpage
%=========================================================================================
\minititle{Sign-Ordered}

Consider the plot of the eigenvalues in our sign-ordered spectrum of our ensemble $\Ens$ below. To understand why our eigenvalues assume this shape of distribution, we need to recall that symmetric matrices (approximately) obey the Semicircle distribution. What we are observing is the ``echo'' of that being the underlying distribution.

What is more interesting, on the other hand, is the peaked variances we observe at the boundaries in the variance plot. Since this is the sign-ordered scheme, the boundaries correspond to the largest eigenvalues. In fact, if we consider symmetry about the origin, we could deduce a very clear trend that as the eigenvalue gets larger (closer to the semicircle boundary), the more variance we expect to observe in the quantities there. As such, we now switch to the \textit{norm-ordering scheme} for a different perspective.

\plotwrapperNC{h}{0.4}{../graphics/chap2_order/sign_Re}%{Largest eigenvalues distribution}
\trim
\plotwrapperNC{h}{0.4}{../graphics/chap2_order/sign_Re_var}%{Largest eigenvalues distribution}

\newpage
%=========================================================================================
\minititle{Norm-Ordered}

Consider the plot of the \textbf{same} eigenvalues in our norm-ordered spectrum of our ensemble $\Ens$ below.
Now, consider the norm component of our norm-ordered spectrum of our ensemble $\Ens$ below. One of the very first things to note about the distribution of the eigenvalues is that when conditioning on its order, we see a very clear trend in the variance of the eigenvalue. Namely, we notice that as $i$ grows, $\Var(\l_i) \to 0$. In simple words, this means that the smallest eigenvalues have less ``freedom'' in their distribution. This is opposed to the largest eigenvalue, which has the highest variance or the most ``freedom''.

\plotwrapperNC{h}{0.45}{../graphics/chap2_order/norm_Re}%{Largest eigenvalues distribution}
\trim
\plotwrapperNC{h}{0.45}{../graphics/chap2_order/norm_Re_var}%{Largest eigenvalues distribution}

\newpage

Lastly, consider the distribution of the \textbf{eigenvalue norms}. As mentioned previously, the variance seems to increase closer to the boundaries. Since we are now using the norm-ordering scheme, the fact that the distribution is symmetric means we are observing twice as many ``occupants'' in a statistical observation. Imagine this as ``folding the semicircle in half''. This explains why the variance is ``increasing'' more rapidly as our eigenvalues shrink.

That being said, the variance plot shows that the variance of the eigenvalue norms seems to have an interesting, non-linear shape. Namely, we observe a rapidly decreasing variance initally, a levelling off, and then a slight dip to the minimum variance. We could trace this back as an artifact of the c.d.f of the semicircle distribution. Since the p.d.f. is the derivative of the c.d.f., we know that the flattest part of the semicircle occurs closer to the origin. As such, we observe the most ``stability'' there.

\plotwrapperNC{h}{0.45}{../graphics/chap2_order/norm_Norm}%{Largest eigenvalues distribution}
\trim
\plotwrapperNC{h}{0.45}{../graphics/chap2_order/norm_Norm_var}%{Largest eigenvalues distribution}

%=========================================================================================
%=========================================================================================
\newpage
\section{A Survey of Spectra}

In this section, we will briefly survey the spectra for a variety of $\D$-distributions and characterize their properties.

%=========================================================================================
\subsection{Uniform Ensembles}
Here we have an ensemble of $\Ens \sim \Unif(0,1)$ matrices. We see a resemblance to
stochastic matrices.

%FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF
\vspace{-1em}
\plotwrapperNC{h}{0.155}{../graphics/chap2/2-4_unif01_spec}
\vspace{-2em}
%{Spectrum of a Unif(0,1) ensemble}
%FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF

%\minititle{Plot takeaways}
The first thing that stands out about the spectrum of the $\Unif(0,1)$ ensemble spectrum is that it very much resembles the spectrum of the stochastic matrix ensemble!
While surprising at first, it starts to make sense once we consider what the Stochastic $\D$-distribution fundamentally reperesents. Namely, it represents a matrix with normalized, uniformly random weights. The algorithm for generating the Stochastic matrices itself samples from the $\Unif(0,1)$ distribution. Additionally, the general shape of the spectrum of the uniform ensemble is similar to that of the stochastic matrix ensemble. This is in the sense that they both have a two ``islands'' of eigenvalues, one complex disk, and one ``island'' of real eigenvalues. \newline

However, despite these similiarties, there is a notable difference between the two $\D$-distributions. This lies in the distribution of the largest eigenvalues which live in the aforementioned ``real island''. For the uniform ensemble, the largest eigenvalues have \textbf{non-zero variance}, we can see them being scattered around roughly the same place. On the other hand, stochastic matrices have a constant largest eigenvalue of one, meaning the largest eigenvalue has \textbf{zero variance}.

\newpage

%=========================================================================================
\subsection{Erdos p-Ensembles}

For this section, we will simulate an Erdos-Renyi ensemble for various values of $p \in [0,1]$ and find the second largest pair of eigenvalues.
That is, we will simulate the distribution of $\l_2,\l_3 \in \sigma(\Ens)$ where $\Ens \sim \text{ER}(p)$ for many values of $p$ in hopes of finding a trend.
The reason we are doing this is to find the radius of the complex disk portion of the Stochastic matrix ensemble but for matrices with parameterized sparsity.

\begin{remark}[Conjugate Pairs]
  The reader might ask, if we are interested in the radius of the complex disk, why is the distribution of $\l_2$ not sufficient? Well, the issue lies in the fact that eigenvalues quite often come in conjugate pairs. For this reason, R must arbitrarily select which eigenvalue in a conjugate pair to select, and it selects the values with a positive imaginary component. So, to generate the outer rim of the complex disk, we will need to find the \textbf{second largest pair} of eigenvalues to capture any conjugate pairs.
\end{remark}

%FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF
\plotwrapperNC{h}{0.6}{../graphics/chap2/2-4_erdos_lam2}%{Second Largest Pair of Eigenvalues of an Erdos-p ensemble}
%FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF

At first glance, we can see a resounding pattern in the plot. It seems like the value of $p$ is inversely related to the radius of the complex disk. So, we conclude that the more connected a graph is, the smaller is the magnitude of the second largest eigenvalue. On the ensemble level, this means that the more connected an ensemble of graphs is, the smaller will be the radius of the complex disk. Additionally, it is worth noting that the values of $p$ close to $0$ have \textbf{higher variance} roughly speaking. Also, the maximum radius of the eigenvalues is less than $1$, which makes sense since the largest eigenvalue must be $1$ for an arbitrary stochastic matrix. As a result, this also demonstrates that the second largest eigenvalue is bound by the \textbf{unit complex disk}.

%=========================================================================================
%\newpage
\subsection{Normal Ensembles}

% Below, we simulate Normal matrices tend to have eigenvalues distributed about the complex disk.
%
% %FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF
% \plotwrapper{h}{0.125}{../graphics/chap2/2-4_normal_spec}{Spectrum of a Standard Normal Matrix ensemble}
% %FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF

%\newpage

We have observed in Figure 2.1 in \textbf{Section 2.1.2} that a standard normal matrix ensemble tends to have a spectrum that is uniformly distributed about a complex disk. This happens to be the case for both real-valued and complex-valued standard normal matrices. Similarly, the result regarding symmetric and hermitian matrices in \textbf{Section 2.3.1} also applies to complex-valued standard normal matrix ensembles. Consider the plot below.

%FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF
\plotwrapper{h}{0.12}{../graphics/chap2/2-4_cplxherm_normal_spec}{Spectrum of a Complex Hermitian Standard Normal Matrix ensemble}
%FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF

As we can see, despite having purely complex entries, this ensemble of Hermitian standard normal matrices over $\C$ has a spectrum of real eigenvalues!
However, we do observee a slight imaginary component.

\begin{remark}[Floating Point Errors]
The \codeword{eigen()} function used to complete eigenvalues in $\RMAT$ uses a numerical algorithm as opposed to solving for the roots of the characteristic polynomial.
For this reason, floating point errors and algorithmic systemic error will yield small, but negligible imaginary components.
\end{remark}
