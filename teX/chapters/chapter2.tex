
\chapter{Spectra}

%=========================================================================================
\epigraph{Life is like a box of crayons.}{\textit{Unknown}}
%=========================================================================================

\section{Introduction}

In the context of this thesis, \textit{spectral statistics} will be an umbrella term for random matrix statistics that somehow involve that matrix's eigenvalues and eigenvectors. That being said, if we fix a \textit{random matrix}, we can study its features by studying its eigenvalues - fundemental numbers that tell us a lot about the matrix. They are quite important for many reasons. For instance in statistical physics, many processes are represented by operators or matrices, and as such, their behaviours could be partially determined by the eigenvalues of their corresponding matrices. The study of eigenvalues and eigenvectors primarily falls in the scope of Linear Algebra, but their utility is far-reaching. So, what are \textit{eigenvalues} exactly?

%=========================================================================================

\subsection{The Quintessential Spectral Statistic: the Eigenvalue}
Given any standard square matrix $P \in \F^{N \times N}$, its \textit{eigenvalues} are simply the roots of the characteristic polynomial $\text{char}_P{(\lambda)}$ = $\det(P - \lambda I)$. By the Fundamental Theorem of Algebra, we know that there is always have as many complex eigenvalues $\lambda \in \Cc$ as the dimension of the matrix.

That being said, when our random matrix has a specified distribution (say, standard normal), we can see patterns in the eigenvalue distributions. So, an eigenvalue is a \textbf{spectral statistic} of a random matrix! To talk about a matrix's eigenvalues in a more formal and concise manner, we motivate what is the \text{eigenvalue spectrum}.

%\newpage

\begin{definition}[Spectrum]
Suppose $P \in \F^{N \times N}$ is a square matrix of size $N$ over $\F$. Then, the (eigenvalue) spectrum of $P$ is defined as the multiset of its eigenvalues and it is denoted
$$\sigma(P) = \{\lambda_i \in \Cc \mid \text{char}_P(\lambda_i) = 0 \}_{i=1}^N$$
Note that it is important to specify that a spectrum is a multiset and not just a set; eigenvalues could be repeated due to algebraic multiplicity and we opt to always have $N$ eigenvalues.
\end{definition}

\medskip
\noindent For example, consider the following code example from the RMAT package.
\begin{code}[Spectrum of a Standard Normal Matrix]
Let $P \sim \Normal(0,1)$ be a $5 \times 5$ standard normal random matrix. We can generate the spectrum of $P$, $\sigma(P)$ as follows:
\end{code}

\begin{lstlisting}[language=R]
library(RMAT)
P <- RM_norm(N = 5, mean = 0, sd = 1)
spectrum_P <- spectrum(P)
# Outputs the following
spectrum_P
...
      Real    Imag   Norm Order
 1 -0.5434  1.3539 1.4589     1
 2 -0.5434 -1.3539 1.4589     2
 3  0.2255  1.4250 1.4427     3
 4  0.2255 -1.4250 1.4427     4
 5 -0.8678  0.0000 0.8678     5
\end{lstlisting}

\minititle{[Explain what each column represents]}

Since this is the first time the output of the spectrum appears, it may be worth briefly specifying what each column represents (i.e. Re denotes real, Im is imaginary, norm is complex norm and order is ranking from largest norm to smallest

While our definition for spectrum is nice and clean, there are a few caveats that we must take care of. First, how are spectra statistics? So we can even call them statistics, so there needs to be some formalization as to why we consider eigenvalues statistics. In this dialogue, we will call back on the same sort of formalization dialogue in the Random Matrices section. Before beginning, the reader is encouraged to review what a statistic is in the review appendix (A.x).

Recall that when we defined and motivated the $\D$-distribution framework of simulating random matrices,we always had one thing - a vector representation of random variables. Using this framework, the formalization is trivial.

\begin{formalization}
Suppose $P \sim \D$ is an $N \times N$ random matrix.
Then, $P$ has a representation as a sequence of $K$ random variables (for some $K \in \N$).
Denote it as the vector $\vec{X} = \seqo[K]{X}$.
Then, the spectrum of the matrix $P$ is simply a function of the vector $\vec{X}$.
We can denote this $\sigma(\vec{X})$, where the operator $\sigma$ is overloaded to mean the spectrum of a matrix $\textbf{with respect to the vector representation}$.
The actual process for $\sigma$ is not necessary to explicitly write out, since characterizing it will be sufficient for now.
Essentially, $\sigma$ is a function that would parse the random variables into the array form by index hacking.
Then, it must compute the determinant of the matrix $P - \lambda I$, and solve for its roots.
To summarize this, consider the flow chart below.
\end{formalization}

To summarize, here is how we formalize the spectrum as a statistic. Suppose we sample $P \sim \D$. Then, we take its spectrum formally using $\sigma$ as such:\hfill
$$ X_1,X_2,\dots,X_K \xra{\text{Make array}} (\vec{X} \rightsquigarrow P) \xra{\det(P - \lambda I)} char_{P}(\lambda) \xra{\text{Solve for roots}} \sigma(P) $$

\begin{remark}[Formalization]
Note how in the formalization we said that an $N \times N$ matrix $P$ has a representation as a vector of $K$ random variables rather than $N^2$ variables. Recall that some non-homogenous explicit $\D$-distributions
do not initialize every entry of the matrix. Only our homogenous explicit and implicit $\D$-distributions do.
\end{remark}
%=========================================================================================

\subsection{Interlude: Ensembles}
While the spectrum of a matrix provides a good summary of the matrix, a matrix is only considered a single point/observation in random matrix theory. Additionally, simulating large matrices and computing their eigenvalues becomes harder and more computationally expensive as $N \to \infty$. As such, to obtain more eigenvalue statistics efficiently, another dimension is introduced by motiving the \textit{spectrum of a random matrix ensemble}. If we have an ensemble $\Ens$, then we can naturally extend the definition of $\sigma(\Ens)$.

\begin{definition}[Ensemble Spectrum]
Let $\Ens \sim \D$ be an ensemble of matrices $P_i \in \F^{n \times n}$. To take the spectrum of $\Ens$, simply take the union of the spectra of each of its matrices. In other words, if $\Ens = \{P_i \sim \mathcal{D}\}_{i = 1}^K$, then we denote the spectrum of the ensemble
$$\sigma(\Ens) = \bigcup_{i=1}^K \sigma(P_i)$$
\end{definition}

\medskip
\noindent For example, consider the following code example from the RMAT package.
\begin{code}[Spectrum of a Standard Normal Matrix Ensemble]
Let $\Ens \sim \Normal(0,1)$ be an ensemble of $3 \times 3$ standard normal random matrices of size $3$. We can generate the spectrum of $\Ens$, $\sigma(\Ens)$ as follows:
\end{code}

\begin{lstlisting}[language=R]
library(RMAT)
ens <- RME_norm(N = 3, mean = 0, sd = 1, size = 3)
spectrum_ens <- spectrum(ens)
# Outputs the following
spectrum_ens
...
      Real    Imag   Norm Order
 1  1.7581  0.0000 1.7581     1
 2 -0.2614  1.0012 1.0347     2
 3 -0.2614 -1.0012 1.0347     3
 4  1.2327  0.4227 1.3032     1
 5  1.2327 -0.4227 1.3032     2
 6 -0.8504  0.0000 0.8504     3
 7 -0.5296  1.0508 1.1767     1
 8 -0.5296 -1.0508 1.1767     2
 9  0.7357  0.0000 0.7357     3
\end{lstlisting}

%\newpage

A common theme in this thesis will be that singleton matrices do not provide insightful information on their own. Rather, it is the collective behavior of a $\D$-distributed ensemble that tells us about how $\D$ impacts our spectral statistics. So in a way, ensemble statistics are the engine of this research.

%FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF
\FIGUREspectrumcomparison{h}{0.65}
%FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF

\minititle{[Plot takeaways]}

Include brief discussion (~1 paragraph) about what the reader should take away from this graph in particular

%=========================================================================================
%=========================================================================================
%\newpage
\section{Spectrum Analysis}

%=========================================================================================

\subsection{Ordered Spectra}

When we motivate the idea of matrix dispersion in the next section, we will consider order statistics of that matrix's eigenvalues in tandem with its dispersion. However, to do so presupposes that we have a sense of what \textit{ordered} eigenvalues means. Take a matrix $P$ and its \textit{unordered} spectrum $\sigma(P) = \{\lambda_j\}$. It is paramount to know what ordering scheme $\sigma(P)$ is using, because otherwise, the eigenvalue indices are meaningless! So, to eliminate confusion, we add an index to $\sigma$ that indicates how the spectrum is ordered. Often, the ordering context will be clear and the indexing will be omitted.

\minititle{Order Schemes: How to Order Eigenvalues}

\noindent Consider the two following $\textit{ordering schema}$:

Standard definitions of an ordered spectrum follows the standard ordering in the reals; denote this as the ordering by the $\textbf{sign scheme}$. Note that because total-ordering is only well-defined on the reals, we can only use this scheme when on a spectrum with real entries. So, we write the $\textit{sign-ordered spectrum}$ as follows:
\begin{align*}
\sigma_S(P) = \{\lambda_j : \lambda_1 \geq \lambda_2 \geq \dots \geq \lambda_N\}_{j = 1}^N
\end{align*}
Alternatively, we can motivate a different scheme that properly handles complex eigenvalues. We could sort the spectrum by the norm of its entries; denote this as ordering using the \textit{norm scheme}. This way, all the eigenvalues are mapped to a real value, in which we could use the sign-scheme of ordering. Without further ado, we write the $\textit{norm-ordered spectrum}$ as follows:
\begin{align*}
\sigma_N(P) = \{\lambda_j : |\lambda_1| \geq |\lambda_2| \geq \dots \geq |\lambda_N|\}_{j = 1}^N
\end{align*}
Note that when we take the norms of the eigenvalues, we essentially ignore "rotational" features of the eigenvalues. Signs of eigenvalues indicate reflection or rotation, so when we take the norm, we essentially become more concerned with scaling.

For example, consider the following plots, showing the difference in using the sign and norm ordering schemes for the same spectrum.

%FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF
\FIGUREorderscheme{h}{0.55}
%FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF

%=========================================================================================
%\newpage
\minititle{Singular Values}

An aternative to using the norm ordering scheme is using the singular values of the matrix. If a matrix is symmetric, the singular values are simply the norm of the eigenvalues. We ignore rotational features and focus solely on scale when we do so.

Suppose $P$ is a random matrix. Then, we can take it singular values as such.

\begin{definition}[Singular Values]
The singular values of a matrix $P$ are given by the square root of the eigenvalues of the corresponding product of that matrix and its transpose. That is, $\sigma_+(P) = \sqrt{\sigma(P \cdot P^T)}$.
\end{definition}


%\newpage
%****************************************************************************************%
\minititle{Summary Table of Spectrum Schema}
\spectrumschemetable
%****************************************************************************************%

%=========================================================================================

\minititle{Order Statistics}

With eigenvalue ordering unambiguous and well-defined, we may proceed to start talking about their order statistics. In short, given a random sample of fixed size, order statistics are random variables defined as the value of an element conditioning on its rank within the sample. (See A.x)

In general, order statistics are quite useful and tell us a lot about how the eigenvalues distribute given a distribution. They tell us how the eigenvalues space themselves and give us useful upper and lower bounds.

For example, the maximum of a sample is an order statistic concerned with the highest ranked element. In our case, this could correspond the largest eigenvalue of a spectrum. After all, a spectrum is a random sample of fixed size, so this statistic is well-defined.

\begin{example}[The Largest Eigenvalue]
Suppose we have seek the largest eigenvalue distribution for a ensemble distribution $\D$, we would simulate an ensemble $\Ens$ and observe $\lambda_1$ for each of its matrices. Then, we can set the distribution of the largest eigenvalue for $\D$ by observing the distribution of $\lambda_1$.
\end{example}

So, we will consider the conditional order statistics $\E(\lambda_{i} \mid i)$ and $\Var(\lambda_{i} \mid i)$.

%=========================================================================================
%=========================================================================================
%\newpage
\subsection{Case Study: Perron-Frobenius Theorem}

\begin{definition}[Spectral Radius]
  Let be $P$ be any matrix and $\spec(P)$ be its ordered spectrum. Then, the spectral radius of $P$ is defined as $\rho(P) = ||\sup \spec(P)||$ is the norm of its largest eigenvalue.
\end{definition}

\minititle{Detour: The Perron-Frobenius Theorem}

Using the toolkit we have now accquired, we can now discuss an elegant, visual representation of the Perron-Frobenius theorem (see \textbf{Section A.1.2}).
To put it shortly, the Perron-Frobenius theorem, applied to stochastic matrices is a result that guarantees the existence of a stationary distribution to an ergodic Markov Chain (see \textbf{Section A.3}).
That being said, the following facts give us a heuristic demonstration of the Perron-Frobenius theorem.

\begin{theorem}[Perron-Frobenius Theorem] \hfill
  \vspace{-1em}
  \\ \\ Consider the two following facts:
  \begin{enumerate}
    \item The largest eigenvalue of stochastic matrices is 1.
    \item When multiplied by $P^K$, any point $v$ asymptotically enters the eigenspace of the matrix's largest eigenvalue as $K \to \infty$. That is, as $K$ grows, $v P^K$ approaches eigenvector of $P$ of $\lambda_1$.
  \end{enumerate}
So, there is an eigenvector of the largest eigenvalue --- for a Markov Chain, this is a stationary distribution.
\end{theorem}

\begin{note}
  Note that this is only part of the results of the theorem. The theorem is more general and has a wider scope and applies to matrices in a more general fashion.
  We only demonstrate this because we have a case with a constant largest eigenvalue and an eigenvector with a unique interpretation (a stationary distribution).
\end{note}

Again, those two statements were not formally proven in this thesis. However, there are large amounts of computational evidence for both of these results. Consider the first statement.

Below we have a stochastic ensemble spectrum. Notice how the largest eigenvalue is 1, living far from the island of \textbf{complex} eigenvalues.

%FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF
\FIGUREstochspectrum{h}{0.65}
%FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF

This shows that the spectral radius of stochastic matrices is $1$! What is left is the other fact.
For the second statement, again, there is plenty of computational evidence to back this up.
%This is the reason why \textbf{Appendix D} is included. This is a side project that started in this thesis, simulating eigenvectors computationally.
The reader may refer to \textbf{Appendix D} for an empirical demonstation of fact 2. Because the discussion of eigenvectors is too far afield from the current topic, so we will not include it in the current section.

From the simulations, we find that there is overwhelming evidence that most matrices, if not all, satisfy the second statement.
The "almost all" part here is an artifact of random matrices, since it is unlikely they will have duplicate eigenvalues, or eigenvalues corresponding to pure rotations (with no scaling).
These are the types of eigenvalues that lead to the second statement not holding. \newline

\blocktitle{Conclusions} So, to conclude, we can say that the computational evidence for these two facts support and provide an alternative method of empirically demonstrating that the Perron-Frobenius theorem is true.

%=========================================================================================
%=========================================================================================
\newpage
\section{Symmetric and Hermitian Matrices}

%\subsection{Introduction}
A very important class of matrices in Linear Algebra is that of Symmetric or Hermitian matrices (See A.x). Simply put, those are matrices which are equal to their conjugate transpose.

\begin{remark}[Symmetric versus Hermitian]
Since real numbers are their own conjugate transpose, every Symmetric matrix is Hermitian. However, we will still delineate the two terms to avoid confusion and indicate what field $\F$ we are working with.
\end{remark}

In any case, one critical result in Linear Algebra that will be extensively wielded in this thesis is the fact that if a matrix is Symmetric or Hermitian, then it has real eigenvalues. In other words:
\begin{align*}
P = \overline{P^{T}} \implies \sigma(P) = \{\lambda_i \mid \lambda_i \in \R\}
\end{align*}
Having a complete set of real eigenvalues yields many great properties. For instance, if all eigenvalues are real, we have the option of observing either the sign-ordered spectrum or the norm-ordered spectrum. This way, we can preserve negative signs and we would not lose the rotational aspect of the eigenvalue when we study its statistics. That is just one reason out of many more why having real eigenvalues is quite nice.

%=========================================================================================
\minititle{Example: Stochastic Matrices}

One very pleasing example to look at is stochastic matrices. As seen previously in \textbf{Subsection 2.2.2}, stochastic matrices tend to have two components: a complex disk of eigenvalues about the origin and isolated point that is the largest eigenvalue.

Below we have a \textbf{symmetric} stochastic ensemble spectrum. Notice how the largest eigenvalue is 1, living far from the island of \textbf{real} eigenvalues. This can be compared to the spectrum of non-Symmetric stochastic matrices in the previous figure. The patterns are similar, but now, imposing symmetry on the matrices forces the eigenvalues to ``collapse'' onto the real line!

%FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF
\FIGUREsymmstochspectrum{h}{0.65}
%FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF

%=========================================================================================
\newpage
\subsection{Wigner's Semicircle Distribution}

The eigenvalues of Hermitian matrices obey Wigner's Semicircle distribution. Since Hermitian matrices have real eigenvalues, then we can be more precise and generally say that the real component of the eigenvalues follow the semicircle distribution. The distribution is named after physicist Eugene Wigner.

\begin{definition}[Semicircle Distribtion]
If a random variable $X$ is semicircle distributed with radius $R \in \R^+$, then we say $X \sim \text{SC}(R)$. $X$ has the following probability density function:
$$\Prb(X = x) = \frac{2}{\pi R^2} \sqrt{R^2 - x^2} \for x \in [-R, R]$$
\end{definition}

\begin{remark}[Radius and Matrix Dimension]
The dimension of the matrix determines the radius of the eigenvalues. Namely, if a Hermitian matrix $P$ is $N \times N$, then its eigenvalues are approximately semicircle distributed with radius $R = 2\sqrt{N}$; the approximation improves as $N$ gets larger. That is, $P^{\dagger}$ has a spectrum $\sigma({P}) \sim \text{SC}(R = 2\sqrt{N})$ as $N \to \infty$.
\end{remark}

%FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF
\FIGUREsemicircle{h}{0.6}
%FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF

%=========================================================================================
%=========================================================================================
\newpage
\section{Findings}
%=========================================================================================
\subsection{Uniform Ensembles}
Here we have an ensemble of $\Ens \sim \Unif(0,1)$ matrices. We see a resemblance to
stochastic matrices.

%FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF
\FIGUREunifspectrum{h}{0.65}
%FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF

\minititle{Plot takeaways}
Largest eigenvalue is varied; compared with stochastic!

\newpage

%=========================================================================================
\subsection{Erdos p-Ensembles}

Here we have the second largest eigenvalue of the Erdos-Renyi ensemble. We parameterize the eigenvalue by $p$ and we observe a trend below.

%FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF
\FIGUREerdosLAMTWO{h}{0.65}
%FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF

\minititle{Plot takeaways}

Why upper half plane only?

%=========================================================================================
\newpage
\subsection{Normal Matrices}

Normal matrices tend to have eigenvalues distributed about the complex disk.

%FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF
\FIGUREnormalspectrum{h}{0.5}
%FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF

%\newpage

Hermitian Complex Normal matrices tend to have eigenvalues distributed about the complex disk.

\begin{remark}[Floating Point Errors]
Because the model involves finding coefficients of polynomials with rounded entries (by computing the determinant), floating point errors and algorithmic systemic error will yield small, but negligible imaginary components.
\end{remark}

%FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF
\FIGUREnormalCPLXHERMspectrum{h}{0.65}
%FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF
