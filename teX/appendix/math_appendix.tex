\chapter{Math Review}

%\minititle{Preamble}
%\noindent Add a short paragraph. Cite sources. Indicate how much the reader needs to skim. Think about the audience of this paper. Guide them to the resources as we are listing the definitions accordingly. Mention the importance and rank each definition (1,2,3 stars).
\noindent In the first section of this appendix, we list a few items in linear algebra worth reviewing to make sure there are no gaps in knowledge necessary to understand concepts in this thesis.
For a helpful reference, consult [\cite{horn}].
Additionally, there is another subsection including a proof that was written in the research process of this thesis. Albeit this proof has no direct relation to the primary topic, spectral statistics, it is loosely relevant and displays the tools and techniques relevant to the material.

%=========================================================================================
\section{Linear Algebra}
\subsection{Matrices}
\begin{definition}[Eigenvalue]
Suppose $P \in \F^{n \times n}$ is a square matrix. Then, the eigenvalues of the matrix $P$ are precisely the roots of the characteristic polynomial of $P$, given by $\text{char}_P(\lambda) = \det(P - \lambda I)$. The polynomial $\text{char}_P(\lambda)$ has degree $n$. So by the Fundamental Theorem of Algebra, $P$ has a multiset of $n$ eigenvalues.
\end{definition}

% \begin{definition}[Eigenvector]
% \end{definition}

\begin{definition}[Inverse Matrix]
Suppose there is a matrix $P \in \F^{m\times n}$. Then, $P^{-1}$ is its inverse matrix iff multiplying it by $P$ returns the identity matrix. That is, the inverse matrix must satisfy:
$$ P^{-1} \text{ is the inverse of } P \iff P^{-1} P = I$$
\end{definition}

\begin{definition}[Transpose Matrix]
Suppose there is a matrix $P = (p_{ij}) \in \F^{m\times n}$. Then, its transpose matrix, $P^{T} = (t_{ij}) = (p_{ji}) \in \F^{n\times m}$ matrix whose columns are the rows of the original matrix.
\end{definition}

\begin{definition}[Conjugate Transpose Matrix]
Suppose there is a matrix $P = (p_{ij}) \in \C^{m\times n}$. Then, its conjugate transpose matrix, $P^{\dagger} = (t_{ij}) = (\overline{p_{ji}}) \in \F^{n\times m}$ is a matrix whose columns are the rows of the original matrix.
\end{definition}

\begin{definition}[Symmetric Matrix]
A matrix $P$ is symmetric iff it is equal to its transpose:
$$P \text{ is Symmetric } \iff P = P^{T}$$
\end{definition}

\begin{definition}[Hermitian Matrix]
A matrix $P$ is Hermitian iff it is equal to its conjugate transpose:
$$P \text{ is Hermitian } \iff P = P^{\dagger}$$
\end{definition}

\begin{definition}[Orthogonal Matrix]
A matrix $P$ is called orthogonal iff its transpose is its inverse:
$$P \text{ is Orthogonal } \iff P^{T} = P^{-1}$$
\end{definition}

\begin{definition}[Unitary Matrix]
A matrix $P$ is called unitary iff its conjugate transpose is its inverse:
$$ P \text{ is Unitary } \iff P^{\dagger} = P^{-1}. $$
\end{definition}

%=========================================================================================

\subsection{Other}

\noindent For $\beta$-ensembles, we mention the invariance criterion, which is defined in lieu of the following groups.

\begin{theorem}[Orthogonal Group]
The set of {\em all} orthogonal matrices in $\F^{n\times n}$ is a matrix group. It is called the {orthogonal group}.
\end{theorem}

\begin{theorem}[Unitary Group]
The set of {\em all} unitary matrices in $\C^{n\times n}$ is a matrix group. It is called the {unitary group}.
\end{theorem}

As mentioned in \textbf{Section 2.3}, here is the statement of the Perron-Frobenius Theorem.
Specifically, we consider only the application to ergodic Markov Chains by means of limiting scope to stochastic/transition matrices.
For a reference, see [\cite{levin}].

\begin{theorem}[Perron-Frobenius Theorem]
The Perronâ€“Frobenius theorem asserts that a real square matrix with positive entries has a unique largest real eigenvalue and that the corresponding eigenvector can be chosen to have strictly positive components.
\end{theorem}

%=========================================================================================

\newpage

\subsection{Proof: Real Symmetric Matrices have Real Eigenvectors}

\minititle{How does this relate to the rest of the thesis?}

Eigenvectors have a wide range of applications in various fields. As such, a result \textbf{guaranteeing} the existence of real-valued eigenvectors is a strong, non-trivial one. For instance, the stationary distribution of a Markov chain is an eigenvector of its transition matrix of eigenvalue 1. In other scenarios, right-eigenvectors represent the conditional expectation of a transition matrix, and so forth. Altogether, this proof showcases techniques and tools used in this related domains of study. \newline

%\vspace{1em}

\blocktitle{Notation} For notational convenience, for any $N \in \N$, let $\N_{N} = \{1,\dots,N\}$.

\minititle{The Proof}

In this section, we will prove that for any $M \times M$ real symmetric matrix, $S_M \in \R^{M \times M}$, there exists for some eigenvalue $\lambda$, a corresponding \textbf{real} eigenvector $\vec{v} \in \R^M$. Prior to starting the main proof, we begin by proving a auxiliary lemma. \newline

\blocktitle{Lemma} Suppose we have a $M \times M$ real symmetric matrix with some eigenvalue $\lambda$. If there we have a corresponding eigenvector $v \in \Cc^M$, then every entry of $v$, say $v_i$ is equal to a \textbf{real} linear combination of the other entries $v_j \mid j \neq i$. So, we will show that:
$$\forall i \in \N_{M}: v_i =  {\sum_{j \neq i} c_j v_j} \quad (c_j \in \R)$$

\lemmaproof \newline

\noindent Now, leveraging the result of this lemma, we will prove the main theorem.

\begin{theorem}[Taqi] Suppose we have a $M \times M$ real symmetric matrix, $S_M$. Then, we will show that there exists for some eigenvalue $\lambda$, a corresponding \textbf{real} eigenvector $\vec{v} \in \R^M$.
\end{theorem}

\taqiproof

%=========================================================================================
%=========================================================================================
\newpage
\section{Probability Theory}

Please refer to [\cite{blitz}] as a resource; the definitions were sourced from there. For the definitions and theorems covered in Section A.1, consider checking out Chapter 3: Random Variables and their distributions. For Order Statistics, consider Chapter 8, Section 6: Order Statistics.

\subsection{Random Variables}

\begin{definition}[Random Variable]
A random variable $X: \Omega \to \R$ is a function from some sample space $\Omega = \{s_i\}_{i=1}^n$ to the real numbers $\R$. The sample space is taken to be any set of events such that the probability function corresponding to the random variable, $p_X$ exhausts over all the events in $\Omega$. In other words, we expect $\int_\Omega p_X(s) = 1$.
\end{definition}

%=========================================================================================
\minititle{PDFs}

\begin{definition}[Probability Density Function]
For a continuous r.v. $X$ with CDF $F$, the probability density function (PDF) of $X$ is the derivative $f$ of the CDF, given by $f(x) = F'(x)$. The support of $X$, and of its distribution, is the set of all $x$ where $f(x) > 0$.
\end{definition}

\begin{theorem}[Characterizing the PDF]
A probability density function is characterized by a few properties that are necessary for it to be valid. They are as follows:
  \begin{enumerate}
    \item Non-negativity: the PDF must be a non-negative valued function everywhere.
      $$ f(x) \geq 0  $$
    \item Integrates to 1: the PDF must integrate to 1 when integrated over its entire support.
    $$ \int_{-\infty}^{\infty} f(x) dx = 1 $$
    \end{enumerate}
\end{theorem}

\begin{example}[Normal PDF]
For example, consider the probability density function for a r.v. $X \sim \Normal(\mu, \sigma)$.
$$\P(X = x) = \frac{1}{\sigma \sqrt{2 \pi}} \exp (-\frac{1}{2} \left(\frac{x - \mu}{\sigma}\right)^2 )$$
\end{example}

%=========================================================================================
\minititle{CDFs}

\begin{definition}[Cumulative Distribution Function]
The cumulative distribution function (CDF) of an r.v. $X$ is the function $F_X$ given by $F_X(x) = \P (X \leq x)$. When there is no risk of ambiguity, we sometimes drop the subscript and just write $F$ (or some other letter) for a CDF.
\end{definition}

\begin{theorem}[Characterizing the CDF]
A cumulative distribution function is characterized by a few properties that are necessary for it to be valid. They are as follows:
  \begin{enumerate}
    \item Monotonic function: the CDF must always be a monotonically increasing function. That is:
      $$ x_0 \leq x_1 \implies F(x_0) \leq F(x_1) $$
    \item Right-continuous: The CDF is continuous except possibly for having some jumps. Wherever there is a jump, the CDF is continuous from the right. That is, for any $a$, we have:
    $$ F(a) = \lim_{x \to a^+} F(x) $$
    \item Converges to 0 and 1 in its limits: since the CDF represents a cumulative probability, its limits must reflect that aspect of probability spaces. So, the CDF must satisfy:
    $$ \lim_{x \to -\infty} F(x) = 0 \and \lim_{x \to \infty} F(x) = 1 $$
  \end{enumerate}
\end{theorem}

%=========================================================================================
\minititle{Independence}

\begin{definition}[Independence]
Random variables X and Y are said to be independent if $\forall x,y \in \R$:
$$ \P(X \leq x, Y \leq y) = \P(X \leq x) \cdot \P(Y \leq y)$$
If the variables are discrete, then this is equivalent to the condition:
$$ \P(X = x, Y = y) = \P(X = x) \cdot \P(Y = y) $$
\end{definition}

\begin{definition}[i.i.d]
A vector of random variables $\vec{X} = (X_i)_{i = 1}^N$ is said to be i.i.d (independent and identically distributed) is each of its entries $X_i$ are exactly so.
\end{definition}

%=========================================================================================
%=========================================================================================
\subsection{Statistics}

\begin{definition}[Statistic]
A statistic is formally defined as a function of a vector of random variables. So, for instance $f$ is a statistic of a vector of random variables $\bar{X}$. Its observed value is given by $f(\vec{X})$.
\end{definition}

\begin{example}[Mean Statistic]
For instance, the mean of a random sample $\vec{X}$ is a statistic. Formally, we would define the statistic $f : \vec{X} \to \frac{\sum_i x_i}{N}$. So the mean of the random sample is defined as $\bar{x} = f(\vec{X})$.
\end{example}

\minititle{Order Statistics}

\begin{definition}[Order Statistic]
Suppose $\vec{X} = \{X_i\}_{i = 1}^N$ is an ordered vector of random variables. Then, the $i^{th}$ order statistic of $X$ is given by $X_i$.
\end{definition}

\begin{example}[Order Statistic]
Suppose $X = (20,7,2,1)$. The smallest (fourth) order statistic is $1$. The second order statistic is $7$. The largest (first) order statistic is $20$.
\end{example}

\begin{theorem}[Order Statistics of Uniform i.i.d Variables]
Let $\seq[n]{U}$ be i.i.d. $\Unif(0, 1)$. Then, the $j^{th}$ order statistic $U_{(j)}$, is distributed as $U_{(j)} \sim \Beta(j, n-j+1)$.
\end{theorem}

%=========================================================================================
%=========================================================================================
\newpage
\section{Markov Chains}

%\minititle{How does this relate to the rest of the thesis?}

In this section, we will define and discuss Markov chains, which are a very useful construct with countless applications.
In short, a Markov chain is a stochastic model that describes a sequence of possible events in which the probability of each event is independent of every other state \textbf{except} the state directly preceding it.
Markov chains are embedded in this thesis through our study of stochastic/transition matrices. Additionally, they also show up in \textbf{Appendix D} when we talk about the mixing time of Markov chains.
There are many types of Markov chains, but we will stick to a version that is ubiquitous and canonical.

So within the scope of this thesis, we only care about discrete, time-homogeneous Markov chains.
Specifically, we will be considering Markov chains that represent a random walk on a fixed graph.
This is a simple Markov chain with two characterizing sets of parameters: the number of vertices, and the weights between each vertex.
In this representation, each vertex represents a state and each edge represents a transition probability.
This way, we can obtain an $N \times N$ transition matrix for any graph with $N$ given that we know every transition probability.
This will be formalized further, but the idea is that each random stochastic matrix will represent a graph with random weights.

Please refer to [\cite{blitz}], Chapter 11, for more information on Markov chains.

\begin{definition}[Markov Chain] Say a set of random variables $X_i$ each take a value in a set, called the state space, $S_M = \{1,2,\dots,M\}$. Then, a sequence of such random variables $X_0,X_1,\dots,X_n$ is called a Markov Chain if the following conditions are satisfied:
\end{definition}

\fancyblocktitle{Closed Support} $\forall X_i:$ $X_i$ has support and range $S_M = \{1,2,...,M\}$. \\

\fancyblocktitle{Markov Property} The transition probability from state $i \to j$, given by $\Prb(X_{n+1} = j \mid X_n = i)$ is conditionally independent from all past events in the sequence $X_{n-1} = i',X_{n-2} = i'', \dots,X_0 = i^{(n-1)}$, excluding the present/last event in the sequence. In other words, given the present, the past and the future are conditionally independent. So, $\forall i,j \in S_M$, we observe:
$$ \Prb(X_{n+1} = j \mid X_n = i) = \Prb(X_{n+1} = j \mid X_n = i, X_{n-1} = i', \dots,X_0 = i^{(n-1)})$$

%=========================================================================================
\newpage
\minititle{Transition Matrices}

\begin{definition}[Transition Matrix]
Take a Markov Chain with states $\{\oneto[M]\}$.
Letting $q_{ij} = \P(X_{n+1} = j \mid X_n = i)$ be the transition probability from $i \to j$, then the matrix $Q=(q_{ij})$ is the $\textit{transition matrix}$ of the chain.
That being said, transition matrices must satisfy a few conditions such as those mentioned below.
\end{definition}



\fancyblocktitle{Nonnegativity} $Q$ is a non-negative matrix. So every entry $q_{ij} \in \R^+$ is non-negative.
This follows because probabilities are necessarily non-negative values. \\

\fancyblocktitle{Stochasticity} For this transition matrix to be valid, its rows have to be stochastic, meaning their entries sum to 1. This may be understood as applying the law of total probability to the event of transitioning from any given state $i \in S_M$. In other words, the chain has to go somewhere with probability 1.
$$\forall i \in \N_M: \sum_{j \in \N_M} q_{ij} = 1$$
Note, it is \textbf{not} necessary that the converse holds. The columns of our transition matrix need not sum to 1 for it to be a valid transition matrix.


\begin{definition}[$n$-step Transition Probability] The $n$-step transition probability of $i \to j$ is the probability of being at $j$ exactly $n$ steps after being at $i$. We denote this value $q_{ij}^{(n)}:$
\end{definition}

$$ q_{ij}^{(n)}: \Prb(X_n = j \mid X_0 = i)$$
Realize:
$$q_{ij}^{(2)} = \sum_{k \in S_M} q_{ik}\cdot q_{kj}$$
Because by definition, a Markov Chain is closed under a support/range of $S_M$ so the event $i \to j$ may have taken any intermediate step $k \in S_M$. Realize by notational equivalence, $Q^2 = (q_{ij}^{(2)})$. Inducting over $n$, we then obtain that:

$$q_{ij}^{(n)} \text{ is the } (i,j) \text{ entry of } Q^n$$

\begin{definition}[Marginal Distribution of $X_n$]
Let $\vec{\pi} = (\pi_1,\pi_2,\dots,\pi_M)$ represent a vector of probabilities such that for every state $i$, $\pi_i = \Prb(X_0 = i)$ represents the initial probability of being at state $i$.
Then, the marginal distribution of $X_n$ (the distribution of the chain at time $n$) is a vector where the $j^{th}$ component is $\P(X_n = j)$ for $j \in S_M$.
The marginal distribution of $X_n$ is precisely given the vector $\vec{\pi} Q^n \in [0,1]^{M}$.
So, we call $\vec{\pi}$ an initial state distribution, and $\vec{\pi} Q^n$ the state of the distribution at time $n$.
\end{definition}

%=========================================================================================
\newpage
\minititle{Classification of states}

Having defined Markov chains and their relationship to transition matrices, we now discuss a few important properties of Markov chains through the process of classifying the behavior of some of its states. \\

\fancyblocktitle{Recurrent State} A state $i \in S_M$ is said to be $\textbf{recurrent}$ if starting from $i$, the probability is 1 that the chain will $\textit{eventually}$ return to $i$. If the chain is not recurrent, it is $\textbf{transient}$, meaning that if it starts at $i$, there is a non-zero probability that it never returns to $i$.

That being said, here is a caveat to take into consideration. As we let $n \to \infty$, our Markov chain will guarantee that all transient states will be left forever, no matter how small the probability is.
This can be proven by letting the probability be some $\epsilon$, then realizing that by the support of $\text{Geom}(\epsilon)$ is always some finite value.
Then, by the equivalence between the \textit{Markov property} and independent Geometric trials, we are guaranteed the existence of some finite value such that there is a success of never returning to $i$.

\begin{definition}[Reducibility] A Markov chain is said to be $\textbf{irreducible}$ if for any $i,j \in S_M$, it is possible to go from $i \to j$ in a finite number of steps with positive probability.
In other words:
$$\forall i,j \in S_M: \exists n \in \mathbb{N} : q_{ij}^{(n)} > 0$$
\end{definition}

From our quantifier formulation of \textit{irreducible} Markov chains,
we can equivalently say that a chain is  \textit{irreducible} if there is an integer $n \in \mathbb{N}$ such that the $(i,j)$ entry of $Q^n$ is positive for any $i,j$.
That being said, we define a Markov chain to be $\textbf{reducible}$ if it is not $\textbf{irreducible}$.
Using our quantifier formulation, it means that it suffices to find \textit{transient} states so that:
$$\exists i,j \in S_M: \nexists n \in \mathbb{N} : q_{ij}^{(n)} > 0$$

Lastly, another state feature that is important to our discussion is periodicity.
In simple words, the period of a state is defined as an upper bound on the time needed to return to a state.
First, we define the period of a given state.

\begin{definition}[Period]
A state $i$ has period $k$ if any return to state $i$ must occur in $k$ steps.
Formally, the period of state $i$, denoted $\phi_i$ is defined as follows:
$$\phi(i) = \{\gcd{n} \mid p_{ii}^n > 0\}$$
\end{definition}

That being said, if every state of the chain has a period of $1$, then we call it \textbf{aperiodic}.
Having defined irreducibility and aperiodicity, we are finally able to define our primary objects of interest: ergodic Markov chains.

%=========================================================================================
\newpage
\minititle{Ergodicity}

Lastly, we come to our discussion of ergodicity. Ergodicity is a term used in statistical physics to describe stochastic systems which are thoroughly ``mixed''.
In an ergodic system, every state is expected to be visited uniformly and randomly. Roughly speaking, we could also say ergodicity is akin to a state of equilibrium.
In fact, the origin of the word comes from ``ergodic theory'' developed by Boltzmann, who was interested in statistical mechanics.
That being said, we define an ergodic Markov chain as follows.

\begin{definition}[Ergodic Markov Chain]
An ergodic Markov chain is a Markov chain whose states are irreducible and aperiodic.
Ergodic Markov chains guarantee the existence of a unique steady-state vector $\vec{\pi}$ called the stationary distribution, which represent a left eigenvector of the corresponding transition matrix.
Additionally, if we denote $\pi_i$ as the steady-state probability of state $i$ and $\eta(i,t)$ as the number of visits to state $i$ in the time $t$, then:
$$\lim_{t \to \infty} \frac{\eta(i,t)}{t} = \pi(i)$$
In other words, $\pi(i)$ represents the average time the Markov chain spends at state $i$ in the long term, which converges as a result of erodicity or ``thorough mixing''.

\end{definition}

Informally, irreducibility ensures that there is a sequence of transitions of non-zero probability from any state to any other.
Aperiodicity, on the other hand, ensures that the states are not partitioned into sets such that all state transitions occur cyclically from one set to another.
That being said, this wraps up our discussion of Markov chains for this section.
This subject is revisited in the mixing time simulation chapter in \textbf{Appendix D}, where we discuss the time it takes a matrix to send an arbitrary probability vector to the stationary distribution.


%=========================================================================================
%=========================================================================================
