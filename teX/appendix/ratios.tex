\chapter{Mixing Time Simulations}
\section{Introduction}
In this chapter, we'll talk about ratio-mixing time simulations. Essentially, these simulations are a method of approximating the distribution of a random transition matrix's mixing time. There will be a fun exploration of the Erdos-Renyi matrix ensembles and we will computationally show that the parameterized ensemble has a mixing time inversely proportional to graph sparsity.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Mixing Time Simulations}
With the Erdos-Renyi graph defined, we may now motivate the simulation of random walks on them. First, however, we need to generate their corrosponding transition matrices. An algorithm for this is outlined below.

Suppose we have simulated a transition matrix for an Erdos-Renyi graph called $Q$. Now, fixing some initial probability distribution $\vec{x} \in \R^M$, we may consider the evolution sequence of a random walk on this Erdos-Renyi graph by taking its evolution sequence $\mathcal{S}(Q, x)$.

\begin{definition}[Random Batches]
Let $\F$ be a field, and fix some $M \in \N$. Let $\B_\lambda \subset \F^M$ be a uniformly random batch of points in the $M$-hypercube of length $\lambda$. That is, 
$$\B_\lambda = \{\vec{x} \mid x_i \sim \text{Unif}(-\lambda, \lambda) \text{ for } i = \oneto[M]\}$$

$\textbf{Note:}$ If $\F = \mathbb{C}$, then take $\vec{x} \in \B_\lambda$ to mean $\vec{x} = a + bi \text{ where } a,b \sim \text{Unif}(-\lambda,\lambda)$.
\end{definition}


\begin{definition}[Evolution Sequence]
An evolution sequence of a vector $\vec{\pi}$ and a transition matrix $Q$ is defined as the sequence $\mathcal{S}(Q,\pi) = ( \pi'_n )_{n=1}^N$ where $ \pi'_n  = \pi Q^n$
\end{definition}

\begin{definition}[Finite Evolution Sequences]
Suppose we sample a random point from $\B_\lambda$, emulating a random point $\vec{v} \in \F^M$. Additionally, let $Q \in \F^{M \times M}$ be a transition matrix over $\F$. Fixing a maximum power ('time') $T \in \N$, define the evolution sequence of $\vec{v}$ as follows:
$$\Seq(v, Q, T) = (\alpha_n)_{n=1}^T \text{ where } \alpha_k = {v}Q^k$$

If we do not impose a finiteness constraint on the sequence, we consider powers for $n \in \N$ or $t = \infty$
\end{definition}


\begin{definition}[Consecutive Ratio Sequences]

Accordingly, define the consecutive ratio sequence (CST) of $\vec{v}$ as follows:

$$\Rseq(v, Q, T) = (r_n)_{n=2}^T \text{ where } (r_n)_j = \frac{(\alpha_n)_j}{(\alpha_{n-1})_j} \text{ for } j = \oneto[M]$$

In other words, the consecutive ratio sequence of $v$ can be obtained by performing $\textbf{component-wise division}$ on consecutive elements of the evolution sequence of $v$.
\end{definition}

\begin{definition}[Near Convergence]

Because these sequences may never truly converge to eigenvectors of the matrix, we formalize a notion of "near convergence". As a prelimenary, we first define $\varepsilon$-equivalence. Let $\F$ be a field, and fix $\varepsilon \in \R^+$. Suppose we have vectors $v, v' \in \F^M$. Then, $v \sim_\epsilon v'$ if $||v - v'|| < \epsilon$ where $|| \cdot ||$ is the norm on $\F$.
\end{definition}

Let $\varepsilon \in \R^+$, and suppose we have an evolution sequence $(a[\vec{v}])_n$. Then, $a_n$ $\varepsilon$-converges at $N \in \N$ if:
$$\forall n \geq N \mid  a_N \sim_\epsilon a_n$$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Erdos-Renyi Ensemble Simulations}

\begin{definition}[Erdos-Renyi Graph]
An Erdos-Renyi graph is a graph $G = (V,E)$ with a set of vertices $V = \oneto{M}$ and edges $E = \mathds{1}_{i,j \in V} \sim \Bern(p_{ij})$. It is homogenous if $p_{ij} = p$ is fixed for all $i, j$.
\end{definition}

Essentially, an Erdos-Renyi graph is a graph whose 'connectedness' is parameterized by a probability $p$ (assuming it's homogenous, which this document will unless otherwise noted). As $p \to 0$, we say that graph becomes more sparse; analogously, as $p \to 1$ the graph becomes more connected.\newline
\indent Recall from probability theory that a sum of i.i.d Bernoulli random variables is a Binomial variable. As such, we may alternatively say that the degree of each vertex $v$ is distributed as $deg(v) \sim Bin(M,p)$. This is helpful to know because the process of simulating graphs becomes much simpler.

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Questions}

\begin{enumerate}
  \item How are the entries of the CRS distributed? Are they normal, and if so, what is its mean?
  \item Are the entries of the CRS i.i.d as $t \to \infty$?
  \item For an Erdos-Renyi matrix, is the mixing time $t$ dependent on the parameter $p$?
  \item What impact does the running time parameter $T$ have on $\sigma$ (the variance of the distribution of the CRS entries)? 
\end{enumerate}

\subsection{Cauchy Distributed Ratios}

It seems to be the case that the $\textbf{log-transformed}$ entries of the CRS are Cauchy distributed about $\log{\lambda_1}$ where $\lambda_1 = \max(\sigma(Q))$, the largest eigenvalue of $Q$. That is,

$$r_i \sim \text{Cauchy}(\ln\lambda_1) \text{ for } i = \oneto[M]$$