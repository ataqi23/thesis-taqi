\chapter{Mixing Time Simulations}
\section{Introduction}

This section goes hand-in-hand with \textbf{Appendix A.3} on Markov Chains, so please read that section first.
In this supplementary chapter, we will be overviewing the topic of simulating mixing time distributions for Markov chains.
While this topic is not directly related to spectral statistics,
mixing time simulations involve numerically approximating how many powers of a stochastic matrix are needed to obtain a stationary distribution.
As such, we are simulating the time it takes to approach an eigenvector of the transition matrix.
% Stationary distributions are eigenvectors with eigenvalue one, which we found to be the spectral radius of any transition matrix.
% By randomizing the weights of the graphs these stochastic matrices, we return to the world of using matrix ensembles to describe overall behavior.
% In this case, we will cover mixing time simulations for $\D = Stochastic$ and $\D = \text{ER}(p)$ matrix ensembles.
% Specifically, we will talk about the applications of these simulations for $p$-Erdos ensembles and demonstrate that mixing time is positively related to sparsity.
%

That being said, we will start to define the mixing time of a Markov chain by using the framework of transition matrices.
However, before we do so, we must formalize probability vectors, which are the vectors with which we will iterate our transition matrices.

% For a fixed point, The mixing time of a Markov chain is defined as how long a given Markov chain takes to reach a stationary distribution.
% This is captured by the minimum power needed such that the fixed point multiplied by the transition matrix to that power becomes an eigenvector.
% Precisely, it is an eigenvector of eigenvalue one.

\begin{definition}[Probability Vector]
An $N$-dimensional probability vector, denoted $\pivec \in [0,1]^N$ is a vector of probabilities such that the entries of $\pivec$ sum to one. That is,
$$\pivec = (\pi_j)_{j = 1}^N \where \sum_{j = 1}^N \pi_j = 1$$
In other words, a probability vector is a stochastic row, so we use the same algorithm to randomly generate one as documented in \textbf{Algorithm B.1.1}.
To notate a randomly sampled probability vector of size $N$, write $\pivec \sim \Phi(N)$.

\end{definition}


Now, given a probability vector $\pivec$, we are interested in observing the expected time spent at each vertex in the graph after $n$ steps.
See the definition of the $n$-step probability in \textbf{Appendix A.3}. From that section, we know that this is equivalent to taking powers of the matrix.
In other words, we are interested in observing the advancing probability vector $\pivec^{(n)} = \pivec Q^n$.

In other words, to observe how the Markov chain advances an arbitrary probability vector $\pivec$, we would seek the following sequence of $N$-dimensional probability vectors:
$$\pivec \ra \pivecn{1} \ra \pivec^{(2)} \ra \dots \ra \pivec^{(\maxtime)}$$
As such, it is worthwhile to define the power sequence of a probability vector w.r.t. a given transition matrix, which we do below.

\begin{definition}[Power Sequences]
Suppose that $\pivec \in [0,1]^N$ is a probability vector and $Q$ is an $N \times N$ transition matrix.
The power sequence of $\pivec$ w.r.t. $Q$ is defined as the sequence $\Seq(\pivec \mid Q)$ where:
$$\Seq(\pivec \mid Q) = \langle \pivecn{n} \rangle \where \powervec_n = \pivec Q^n \for {n \in \N}$$
In practice, we usually take finite sequences since infinite sequences are computationally intractable.
To notate a sequence that goes from $n = 0, \dots, \maxtime$, we denote this $\Seq^{(\maxtime)}(\pivec \mid Q)$.
\end{definition}


% Now, to estimate the eigenvectors, we use the method of taking the entry-wise ratios.

% $$(\pivec Q^\infty) Q = (\pivec Q^\infty)$$
%
% $$\pi^* Q = \pi^*$$
%
% $$v Q = \l v \where \l = 1$$
%
% $$ \vvec Q = \l \vvec$$
%
% $$ \vvec Q Q^{N - 1} = \l \vvec Q^{N - 1}$$
%
% $$ \vvec Q^n = \l \vvec Q^{N - 1}$$
%
% $$ \powervec_n = \l \powervec_{n-1}$$
%
% $$ \frac{\powervec_n}{\powervec_{n-1}} = \l$$
%
% $$ \rvecn{n} = \l$$
%
% $$ \rvecn{n} = (\l, \l, \dots, \l)$$


%\newpage

Now, what we are interested in is observing when a point becomes a stationary distribution, or in other words, an eigenvector where $\lambda = 1$.
In other words, we seek to find the power $\tau$ where $\pivecn{\tau}$ satisfies:
$$ \pivecn{\tau}Q = \pivecn{\tau} $$
As we said, this is akin to solving the eigenvector equation for a particular eigenvalue. So, what we seek is a vector $\pivec'$:
$$ \pivec'Q = \l \pivec' $$

\minititle{Power Sequence and Ratio Approximations}

\noindent Suppose we are generally trying to solve the eigenvector equation:
$$\pivec Q = \l \pivec$$
If we multiply both sides by $Q^{n-1}$, we get the equation:
$$\pivec Q^n = \l \pivec Q^{n - 1}$$
Notice, we could use our power sequence to simplify this and get:
$$\pivecn{n} = \l \pivecn{n-1}$$
If we represent lambda as a vector of constants by saying $\vec{\l} = (\l, \l, \dots, \l)$, we get:
$$\pivecn{n} = \lambdasvec \pivecn{n - 1}$$
This means that we could express $\pivecn{n}$ as the vector $\pivecn{n - 1}$ where every entry is scaled by $\l$. This means that for every entry $j$,
$$\pivecn{n}_j = \lambda \pivecn{n - 1}_j$$
If we interpret division on vectors to mean component-wise division of the vector entries, we could rewrite:
$$\frac{\pivecn{n}}{\pivecn{n-1}} = \vec{\l}$$
 There isn't a precise notion for this.
As such, the quantity $\frac{\pivecn{n}}{\pivecn{n-1}}$ is a special quantity which we will denote $\rvecn{n}$.
It is a vector of ratios, and for this reason, we will call the following the consecutive ratio sequence.

\begin{definition}[Consecutive Ratio Sequences]
Now, suppose we have a probability vector $\pivec$ and its corresponding (finite) power sequence $\Seq^{(\maxtime)}(\pivec \mid Q)$ w.r.t. some transition matrix $Q$.
Accordingly, define the consecutive ratio sequence (CRS) of $\pivec$ as follows:
$$\Rseq(\pivec \mid Q) = \langle \rvecn{n} \rangle_{n=2}^\maxtime \text{ where } \rvecn{n}_j = \frac{\pin{n}_j}{\pin{n-1}_j} \text{ for } j = \oneto[N]$$
In other words, the consecutive ratio sequence of $v$ can be obtained by performing $\textbf{component-wise division}$ on two consecutive elements of the power sequence of $\pivec$.
\end{definition}


% So, if we observe that $\forall j \in \N_N : \rvecn{n}_j = \lambda$ or $\vec{\rho} = (\l,\l,\dots,\l) = \vec{\l}$ then we have that $\pivecn{n}$ is an eigenvector!
% In other words, we want $\rvecn{n} \to (\l, \l, \dots, \l)$.

Finally, we can formalize the notion of numerically obtaining eigenvectors using the power and consecutive ratio sequences by the following fact:
$$ \pivecn{n} \; \text{is an eigenvector} \iff \rvecn{n} = \lambdasvec$$

As such, stationary distributions are reached when we obtain an eigenvector of eigenvalue one. That is,
$$ \pivecn{n} \; \text{is a stationary distribution} \iff \rvecn{n} = (1,1,\dots,1)$$
We can interpret this as saying that for a vector to be a stationary distribution, multiplying it by the transition matrix should not alter any of the entries.

\section{Convergence}

%Finally, because these sequences may never truly converge to the eigenvectors of the matrix, we formalize a notion of ``near convergence''.
Because these ratios never actually converge to the eigenvectors for a finite power $n \in \N$, we must define a notion of ``near convergence''.
So as a prelimenary, we first must define $\varepsilon$-equivalence, where we consider two vectors to be equivalent when they are close enough to each other.

\begin{definition}[$\epsilon$-Equivalence]
Let $\F$ be a field, and fix $\varepsilon \in \R^+$. Suppose we have vectors $v, v' \in \F^N$.
Then, $v, v'$ are $\epsilon$-equivalent if and only if each of their components have a difference of at most $\ep$.
In other words,
$$v \sim_\ep v' \iff \forall i \in \N_N: |v_i - v'_i| < \ep$$
\end{definition}


% \begin{remark}[Norm Equivalence]
% Another possible route to define equivalence could be by using the norm. Then, $v \sim_\epsilon v'$ if $||v - v'|| < \epsilon$ where $|| \cdot ||$ is the norm on $\F$.
% However, this would imply that more work is needed to see how the CRS converges towards an eigenvector.
% The entry-wise ratio convergence towards the vector $(\l, \l, \dots, \l)$ is cleaner and sufficient.
% Namely, if $v$ is an eigenvector and $v' = v Q$, we would have the following bound:
% $$|v' - v| = (\lambda - 1) |v|$$
% \end{remark}


This makes defining ``near convergence'' quite simple.

\begin{definition}[$\epsilon$-Convergence]
Let $\varepsilon \in \R^+$, and suppose we have a \textbf{finite} evolution sequence $\Seq^{(T)}(\pivec \mid Q)$.
Then, $\pivec$ $\varepsilon$-converges w.r.t $Q$ within $\Seq^{(T)}(\pivec \mid Q)$ if and only if:
$$ \exists \tau \in \N \mid \tau \leq T \text{\; such that \;} \forall k \geq \tau \mid \pivecn{\tau} \sim_\epsilon \pivecn{k}$$
\end{definition}

Finally, we are able to define mixing time.
This definition is inspired in part by Mark Levin's book ``Markov Chains and Mixing Times''.
Please see \cite{levin} as a resource.

\begin{definition}[Mixing Time]
Suppose we have a Markov chain represented by the transition matrix $Q$, and suppose that $\pivec$ is an probability vector.
Then, the mixing time of $\pivec$ with respect to $Q$ is the positive integer $\tau$ such that $\pivec Q^\tau = \pivecn{\tau} \approx \pivecn{\tau + 1}$.
In other words, a vector $\pivec$ is mixed at $\tau$ when $\pivecn{\tau}$ becomes a stationary distribution or eigenvector of $Q$.
To be concrete, we say that $\pivec$ is $\ep$-mixed w.r.t. $Q$ with a mixing time of $\tau$ given the following:
$$\tau = \min\left\{ N \in \N \mid \forall n \geq N : \pivecn{n} \sim_{\ep} \pivecn{N} \right\} $$
%$$ \pivecn{\tau} \approx \pivec$$
\end{definition}


% Without further ado, we are finally able to define the mixing time of a Markov chain for a fixed point (i.p.d).

%\newpage

\section{Simulation}

Finally, we are ready to simulate the mixing time distribution of a random matrix or random matrix ensemble.

\begin{definition}[Monte-Carlo Stochastic Batch]
A Monte-Carlo stochastic batch of dimension $N$ and size $K$ is a set of $K$ random probability vectors of size $B$. That is,
$$\B = \{\pivec_i \sim \Phi(N) \}_{i = 1}^{B}$$
\end{definition}



\begin{definition}[Mixing Time Distribution]
Suppose we have an ensemble of stochastic matrices $\Ens$ and that we have a batch of probability vectors $\B$.
Then, the mixing time distribution of $\Ens$ is defined as the following:
$$\{\tau(P, \pivec) \mid P \in \Ens, \pivec \in \B \}$$
\end{definition}

% \newpage

\section{Generalizations}

Generally, we could do this for any matrix, not just stochastic matrices! We have a notion of mixing even for non-stochastic matrices.

\begin{definition}[Random Batches]
Let $\F$ be a field, and fix some $M \in \N$. Let $\B_\lambda \subset \F^M$ be a uniformly random batch of points in the $M$-hypercube of length $\lambda$. That is,
$$\B_\lambda = \{\vec{x} \mid x_i \sim \text{Unif}(-\lambda, \lambda) \text{ for } i = \oneto[M]\}$$
\end{definition}

For stochastic matrices, we usually opt to take random batches where the points are stochastic rows.
Additionally, if If $\F = \mathbb{C}$, then take $\vec{x} \in \B_\lambda$ to mean $\vec{x} = a + bi \text{ where } a,b \sim \Unif(-\lambda,\lambda)$.

\minititle{A Generalization of Mixing}

We could generalize the notion of mixing to any vector $\vvec$ and any matrix $M$.
To do so, we rewrite the approximate eigenvector statement as follows:
$$\vvecn{n} \text{\; is an eigenvector of \;} Q \iff \rvecn{n} = (\l_1, \l_1, \dots, \l_1)$$

\textbf{So, we find extensive computational evidence that for any matrix P and vector v, iterating P and multiplying it by v will eventually return an eigenvector of the largest eigenvalue.}

% \newpage

\minititle{Open Questions}

Generally speaking, the ratio sequence has a distribution of terms that is Cauchy-like (a ratio distribution).
We get extreme values because we are trying to obtain the orthoganlization to a given entry. So, here are some open questions:

\begin{enumerate}
  \item How are the entries of the consecutive ratio sequence distributed?
  \item Are there any cases when the power sequences do not converge to an eigenvector of $\l_1$?
\end{enumerate}
